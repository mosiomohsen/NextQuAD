{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ddb01b8",
   "metadata": {
    "papermill": {
     "duration": 0.03461,
     "end_time": "2022-06-28T20:51:31.667420",
     "exception": false,
     "start_time": "2022-06-28T20:51:31.632810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f974d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:31.745464Z",
     "iopub.status.busy": "2022-06-28T20:51:31.744785Z",
     "iopub.status.idle": "2022-06-28T20:51:39.947776Z",
     "shell.execute_reply": "2022-06-28T20:51:39.948208Z",
     "shell.execute_reply.started": "2022-06-27T21:07:28.482573Z"
    },
    "id": "E4l6PirHET3x",
    "outputId": "eeaea823-bdbc-4bef-a518-e51736877d6e",
    "papermill": {
     "duration": 8.248439,
     "end_time": "2022-06-28T20:51:39.948364",
     "exception": false,
     "start_time": "2022-06-28T20:51:31.699925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex AMP Installed :: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus =1# multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f94056",
   "metadata": {
    "papermill": {
     "duration": 0.030392,
     "end_time": "2022-06-28T20:51:40.077076",
     "exception": false,
     "start_time": "2022-06-28T20:51:40.046684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26eed7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:40.144629Z",
     "iopub.status.busy": "2022-06-28T20:51:40.143838Z",
     "iopub.status.idle": "2022-06-28T20:51:40.146183Z",
     "shell.execute_reply": "2022-06-28T20:51:40.145776Z",
     "shell.execute_reply.started": "2022-06-27T21:07:38.640071Z"
    },
    "id": "LUx5XplNET3y",
    "papermill": {
     "duration": 0.038904,
     "end_time": "2022-06-28T20:51:40.146285",
     "exception": false,
     "start_time": "2022-06-28T20:51:40.107381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    model_type = 'xlm_roberta'\n",
    "    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n",
    "    config_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "    gradient_accumulation_steps = 2\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "    max_seq_length = 310\n",
    "    doc_stride = 256\n",
    "\n",
    "    # train\n",
    "    epochs = 1\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_type = 'AdamW'\n",
    "    learning_rate = 1.5e-5\n",
    "    weight_decay = 1e-2\n",
    "    epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # scheduler\n",
    "    decay_name = 'linear-warmup'\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 10\n",
    "    \n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c44c99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:40.210519Z",
     "iopub.status.busy": "2022-06-28T20:51:40.210040Z",
     "iopub.status.idle": "2022-06-28T20:51:41.125875Z",
     "shell.execute_reply": "2022-06-28T20:51:41.126294Z",
     "shell.execute_reply.started": "2022-06-27T21:07:38.66125Z"
    },
    "papermill": {
     "duration": 0.950184,
     "end_time": "2022-06-28T20:51:41.126448",
     "exception": false,
     "start_time": "2022-06-28T20:51:40.176264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>تاریخ مفهومی انتزاعی است که دست کم دو معنا از ...</td>\n",
       "      <td>از مفهوم انتزاعی تاریخ کدام معنا استخراج می شود؟</td>\n",
       "      <td>ناظر به وقایع گذشته معطوف به مطالعه و بررسی وقایع</td>\n",
       "      <td>66.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تاریخ مفهومی انتزاعی است که دست کم دو معنا از ...</td>\n",
       "      <td>به چه چیزی تاریخ گفته می شود؟</td>\n",
       "      <td>علم تاریخ و موضوع آن</td>\n",
       "      <td>132.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تاریخ 1 مجموعه حوادث فرهنگی طبیعی اجتماعی اقتص...</td>\n",
       "      <td>رویدادها در تعریف تاریخ شامل چه اموری می شود؟</td>\n",
       "      <td>کردارها و دستاوردهای مادی و معنوی گفته ها اندی...</td>\n",
       "      <td>192.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تاریخ 1 مجموعه حوادث فرهنگی طبیعی اجتماعی اقتص...</td>\n",
       "      <td>تاریخ 2 چگونه معرفتی است؟</td>\n",
       "      <td>ناظر به وقایع جزیی و درک پدیده ها</td>\n",
       "      <td>277.0</td>\n",
       "      <td>310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تاریخ 1 مجموعه حوادث فرهنگی طبیعی اجتماعی اقتص...</td>\n",
       "      <td>معرفت شکل گرفته در ذهن تاریخ نگار از چه نوعیست؟</td>\n",
       "      <td>معرفت درجه یک</td>\n",
       "      <td>391.0</td>\n",
       "      <td>404.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  تاریخ مفهومی انتزاعی است که دست کم دو معنا از ...   \n",
       "1  تاریخ مفهومی انتزاعی است که دست کم دو معنا از ...   \n",
       "2  تاریخ 1 مجموعه حوادث فرهنگی طبیعی اجتماعی اقتص...   \n",
       "3  تاریخ 1 مجموعه حوادث فرهنگی طبیعی اجتماعی اقتص...   \n",
       "4  تاریخ 1 مجموعه حوادث فرهنگی طبیعی اجتماعی اقتص...   \n",
       "\n",
       "                                           question  \\\n",
       "0  از مفهوم انتزاعی تاریخ کدام معنا استخراج می شود؟   \n",
       "1                     به چه چیزی تاریخ گفته می شود؟   \n",
       "2     رویدادها در تعریف تاریخ شامل چه اموری می شود؟   \n",
       "3                         تاریخ 2 چگونه معرفتی است؟   \n",
       "4   معرفت شکل گرفته در ذهن تاریخ نگار از چه نوعیست؟   \n",
       "\n",
       "                                              answer  start    end  \n",
       "0  ناظر به وقایع گذشته معطوف به مطالعه و بررسی وقایع   66.0  115.0  \n",
       "1                               علم تاریخ و موضوع آن  132.0  152.0  \n",
       "2  کردارها و دستاوردهای مادی و معنوی گفته ها اندی...  192.0  248.0  \n",
       "3                  ناظر به وقایع جزیی و درک پدیده ها  277.0  310.0  \n",
       "4                                      معرفت درجه یک  391.0  404.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(r\"../input/qa-farsi-final/qa_train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3a50a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:41.197849Z",
     "iopub.status.busy": "2022-06-28T20:51:41.197301Z",
     "iopub.status.idle": "2022-06-28T20:51:41.200995Z",
     "shell.execute_reply": "2022-06-28T20:51:41.200150Z",
     "shell.execute_reply.started": "2022-06-27T21:09:50.990184Z"
    },
    "papermill": {
     "duration": 0.04265,
     "end_time": "2022-06-28T20:51:41.201105",
     "exception": false,
     "start_time": "2022-06-28T20:51:41.158455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = arToPersianChar(sentence)\n",
    "    sentence = arToPersianNumb(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def arToPersianNumb(number):\n",
    "    dic = {\n",
    "        '١': '۱',\n",
    "        '٢': '۲',\n",
    "        '٣': '۳',\n",
    "        '٤': '۴',\n",
    "        '٥': '۵',\n",
    "        '٦': '۶',\n",
    "        '٧': '۷',\n",
    "        '٨': '۸',\n",
    "        '٩': '۹',\n",
    "        '٠': '۰',\n",
    "    }\n",
    "    return multiple_replace(dic, number)\n",
    "\n",
    "\n",
    "def arToPersianChar(userInput):\n",
    "    dic = {\n",
    "        'ك': 'ک',\n",
    "        'دِ': 'د',\n",
    "        'بِ': 'ب',\n",
    "        'زِ': 'ز',\n",
    "        'ذِ': 'ذ',\n",
    "        'شِ': 'ش',\n",
    "        'سِ': 'س',\n",
    "        'ى': 'ی',\n",
    "        'ي': 'ی'\n",
    "    }\n",
    "    return multiple_replace(dic, userInput)\n",
    "\n",
    "\n",
    "def multiple_replace(dic, text):\n",
    "    pattern = \"|\".join(map(re.escape, dic.keys()))\n",
    "    return re.sub(pattern, lambda m: dic[m.group()], str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9622f10b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:41.321812Z",
     "iopub.status.busy": "2022-06-28T20:51:41.311538Z",
     "iopub.status.idle": "2022-06-28T20:51:43.908079Z",
     "shell.execute_reply": "2022-06-28T20:51:43.907512Z",
     "shell.execute_reply.started": "2022-06-27T21:09:51.392717Z"
    },
    "papermill": {
     "duration": 2.675456,
     "end_time": "2022-06-28T20:51:43.908208",
     "exception": false,
     "start_time": "2022-06-28T20:51:41.232752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['context'] = train['context'].apply(clean_sentence)\n",
    "train['question'] = train['question'].apply(clean_sentence)\n",
    "train['answer'] = train['answer'].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc80d7",
   "metadata": {
    "papermill": {
     "duration": 0.030652,
     "end_time": "2022-06-28T20:51:44.042496",
     "exception": false,
     "start_time": "2022-06-28T20:51:44.011844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3863abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:44.115017Z",
     "iopub.status.busy": "2022-06-28T20:51:44.114168Z",
     "iopub.status.idle": "2022-06-28T20:51:44.949785Z",
     "shell.execute_reply": "2022-06-28T20:51:44.949221Z",
     "shell.execute_reply.started": "2022-05-24T07:54:26.956934Z"
    },
    "id": "X_eRZQrzET3z",
    "papermill": {
     "duration": 0.875899,
     "end_time": "2022-06-28T20:51:44.949914",
     "exception": false,
     "start_time": "2022-06-28T20:51:44.074015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['language'] = \"fa\"\n",
    "train = train.rename(columns={\"questions\" : \"question\"})\n",
    "\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['context'])):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "\n",
    "train = create_folds(train, num_splits=8)\n",
    "\n",
    "def convert_answers(row):\n",
    "    if row[1] == '':\n",
    "        return {'answer_start': [], 'answer': []}\n",
    "    return {'answer_start': [row[0]], 'answer': [row[1]]}\n",
    "\n",
    "train['answers'] = train[['start', 'answer']].apply(convert_answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d337f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:45.023724Z",
     "iopub.status.busy": "2022-06-28T20:51:45.023039Z",
     "iopub.status.idle": "2022-06-28T20:51:45.026139Z",
     "shell.execute_reply": "2022-06-28T20:51:45.026506Z",
     "shell.execute_reply.started": "2022-05-24T07:54:28.271265Z"
    },
    "papermill": {
     "duration": 0.044275,
     "end_time": "2022-06-28T20:51:45.026658",
     "exception": false,
     "start_time": "2022-06-28T20:51:44.982383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3467\n",
       "0    3467\n",
       "5    3467\n",
       "6    3467\n",
       "2    3467\n",
       "3    3467\n",
       "7    3467\n",
       "4    3467\n",
       "Name: kfold, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.kfold.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a6eb3",
   "metadata": {
    "papermill": {
     "duration": 0.030722,
     "end_time": "2022-06-28T20:51:45.224733",
     "exception": false,
     "start_time": "2022-06-28T20:51:45.194011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Covert Examples to Features (Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de01b799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:45.300843Z",
     "iopub.status.busy": "2022-06-28T20:51:45.299034Z",
     "iopub.status.idle": "2022-06-28T20:51:45.301427Z",
     "shell.execute_reply": "2022-06-28T20:51:45.301859Z",
     "shell.execute_reply.started": "2022-05-24T07:55:33.914115Z"
    },
    "id": "dxbZdct1ET3z",
    "papermill": {
     "duration": 0.046521,
     "end_time": "2022-06-28T20:51:45.301980",
     "exception": false,
     "start_time": "2022-06-28T20:51:45.255459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(args, example, tokenizer):\n",
    "    \n",
    "#     example = example[0]\n",
    "    example[\"question\"] = example[\"question\"].lstrip()\n",
    "    tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    max_length=args.max_seq_length,\n",
    "    stride=args.doc_stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\")\n",
    "        \n",
    "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        \n",
    "        feature = {}\n",
    "\n",
    "        input_ids = tokenized_example[\"input_ids\"][i]\n",
    "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
    "\n",
    "        feature['input_ids'] = input_ids\n",
    "        feature['attention_mask'] = attention_mask\n",
    "        feature['offset_mapping'] = offsets\n",
    "\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_example.sequence_ids(i)\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example[\"answers\"]\n",
    "        \n",
    "        if len(answers[\"answer\"]) == 0:\n",
    "            feature[\"start_position\"] = cls_index\n",
    "            feature[\"end_position\"] = cls_index\n",
    "        else:\n",
    "            \n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"answer\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "                \n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "                \n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                feature[\"start_position\"] = cls_index\n",
    "                feature[\"end_position\"] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                feature[\"start_position\"] = token_start_index - 1\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                feature[\"end_position\"] = token_end_index + 1\n",
    "                        \n",
    "        features.append(feature)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e495bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:45.439478Z",
     "iopub.status.busy": "2022-06-28T20:51:45.438817Z",
     "iopub.status.idle": "2022-06-28T20:51:52.869939Z",
     "shell.execute_reply": "2022-06-28T20:51:52.869396Z",
     "shell.execute_reply.started": "2022-05-24T07:55:37.652162Z"
    },
    "papermill": {
     "duration": 7.465935,
     "end_time": "2022-06-28T20:51:52.870084",
     "exception": false,
     "start_time": "2022-06-28T20:51:45.404149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91323decdd44207aeccadc9811ad863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a349412b305484db1a996b2c4c6d12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ff4e512659456992123351b802030e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67225f6f1fe7469c9e9a40954288b4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Config()\n",
    "config = AutoConfig.from_pretrained(args.config_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d44a5f",
   "metadata": {
    "papermill": {
     "duration": 0.032543,
     "end_time": "2022-06-28T20:51:53.152279",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.119736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec126ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:53.225747Z",
     "iopub.status.busy": "2022-06-28T20:51:53.225088Z",
     "iopub.status.idle": "2022-06-28T20:51:53.227602Z",
     "shell.execute_reply": "2022-06-28T20:51:53.227192Z",
     "shell.execute_reply.started": "2022-05-24T07:55:46.334975Z"
    },
    "id": "6TuzHdjmET30",
    "papermill": {
     "duration": 0.04307,
     "end_time": "2022-06-28T20:51:53.227729",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.184659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, features, mode='train'):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.features = features\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item):   \n",
    "        feature = self.features[item]\n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':feature['offset_mapping'],\n",
    "                'sequence_ids':feature['sequence_ids'],\n",
    "                'id':feature['example_id'],\n",
    "                'context': feature['context'],\n",
    "                'question': feature['question']\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd0e2b",
   "metadata": {
    "papermill": {
     "duration": 0.032712,
     "end_time": "2022-06-28T20:51:53.295815",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.263103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02f7804b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:53.370282Z",
     "iopub.status.busy": "2022-06-28T20:51:53.369501Z",
     "iopub.status.idle": "2022-06-28T20:51:53.371477Z",
     "shell.execute_reply": "2022-06-28T20:51:53.371922Z",
     "shell.execute_reply.started": "2022-05-24T07:55:47.513511Z"
    },
    "id": "9OxhKqxcET31",
    "papermill": {
     "duration": 0.04358,
     "end_time": "2022-06-28T20:51:53.372102",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.328522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self._init_weights(self.qa_outputs)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        # token_type_ids=None\n",
    "    ):\n",
    "        outputs = self.xlm_roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        # sequence_output = self.dropout(sequence_output)\n",
    "        qa_logits = self.qa_outputs(sequence_output)\n",
    "        \n",
    "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "    \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86907394",
   "metadata": {
    "papermill": {
     "duration": 0.033219,
     "end_time": "2022-06-28T20:51:53.438817",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.405598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5e16b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:53.508355Z",
     "iopub.status.busy": "2022-06-28T20:51:53.507548Z",
     "iopub.status.idle": "2022-06-28T20:51:53.509554Z",
     "shell.execute_reply": "2022-06-28T20:51:53.509965Z",
     "shell.execute_reply.started": "2022-05-24T07:55:50.09266Z"
    },
    "id": "SxuNrJqqET32",
    "papermill": {
     "duration": 0.038907,
     "end_time": "2022-06-28T20:51:53.510085",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.471178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds = preds\n",
    "    start_labels, end_labels = labels\n",
    "    \n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
    "    total_loss = (start_loss + end_loss) / 2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bbd4c",
   "metadata": {
    "papermill": {
     "duration": 0.03282,
     "end_time": "2022-06-28T20:51:53.575398",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.542578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Grouped Layerwise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "528e8c26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:53.655730Z",
     "iopub.status.busy": "2022-06-28T20:51:53.654915Z",
     "iopub.status.idle": "2022-06-28T20:51:53.657266Z",
     "shell.execute_reply": "2022-06-28T20:51:53.656847Z",
     "shell.execute_reply.started": "2022-05-24T07:55:53.125007Z"
    },
    "id": "vf6HVcu2ET34",
    "papermill": {
     "duration": 0.048873,
     "end_time": "2022-06-28T20:51:53.657372",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.608499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(args, model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc5abe",
   "metadata": {
    "papermill": {
     "duration": 0.032833,
     "end_time": "2022-06-28T20:51:53.722778",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.689945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Metric Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b4f718f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:53.794164Z",
     "iopub.status.busy": "2022-06-28T20:51:53.793274Z",
     "iopub.status.idle": "2022-06-28T20:51:53.795818Z",
     "shell.execute_reply": "2022-06-28T20:51:53.795363Z",
     "shell.execute_reply.started": "2022-05-24T07:55:55.397876Z"
    },
    "id": "bkFB-iMcET34",
    "papermill": {
     "duration": 0.040617,
     "end_time": "2022-06-28T20:51:53.795921",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.755304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b60ffb",
   "metadata": {
    "papermill": {
     "duration": 0.032263,
     "end_time": "2022-06-28T20:51:53.860205",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.827942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7725d7b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:53.984110Z",
     "iopub.status.busy": "2022-06-28T20:51:53.983094Z",
     "iopub.status.idle": "2022-06-28T20:51:53.985513Z",
     "shell.execute_reply": "2022-06-28T20:51:53.984878Z",
     "shell.execute_reply.started": "2022-05-24T07:55:56.612673Z"
    },
    "papermill": {
     "duration": 0.067922,
     "end_time": "2022-06-28T20:51:53.985685",
     "exception": false,
     "start_time": "2022-06-28T20:51:53.917763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    model = Model(args.model_name_or_path, config=config)\n",
    "    return config, tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a43a62c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:54.101039Z",
     "iopub.status.busy": "2022-06-28T20:51:54.099071Z",
     "iopub.status.idle": "2022-06-28T20:51:54.114633Z",
     "shell.execute_reply": "2022-06-28T20:51:54.115557Z",
     "shell.execute_reply.started": "2022-05-24T07:55:59.36107Z"
    },
    "id": "spFRutV0ET34",
    "papermill": {
     "duration": 0.078786,
     "end_time": "2022-06-28T20:51:54.115777",
     "exception": false,
     "start_time": "2022-06-28T20:51:54.036991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    model = Model(args.model_name_or_path, config=config)\n",
    "    return config, tokenizer, model\n",
    "\n",
    "def make_optimizer(args, model):\n",
    "    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    if args.optimizer_type == \"AdamW\":\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.epsilon,\n",
    "            correct_bias=True\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "def make_scheduler(\n",
    "    args, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if args.decay_name == \"cosine-warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    args, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    train_features, valid_features = [[] for _ in range(2)]\n",
    "\n",
    "    for i, row in train_set.iterrows():\n",
    "        train_features += prepare_train_features(args, row, tokenizer)\n",
    "    for i, row in valid_set.iterrows():\n",
    "        valid_features += prepare_train_features(args, row, tokenizer)\n",
    "\n",
    "    train_dataset = DatasetRetriever(train_features)\n",
    "    valid_dataset = DatasetRetriever(valid_features)\n",
    "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler,\n",
    "#         num_workers=1,#optimal_num_of_loader_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.eval_batch_size, \n",
    "        sampler=valid_sampler,\n",
    "#         num_workers=1,#optimal_num_of_loader_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515a723",
   "metadata": {
    "papermill": {
     "duration": 0.06917,
     "end_time": "2022-06-28T20:51:54.241564",
     "exception": false,
     "start_time": "2022-06-28T20:51:54.172394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af2b58f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:54.421598Z",
     "iopub.status.busy": "2022-06-28T20:51:54.420780Z",
     "iopub.status.idle": "2022-06-28T20:51:54.443437Z",
     "shell.execute_reply": "2022-06-28T20:51:54.445125Z",
     "shell.execute_reply.started": "2022-05-24T07:56:03.266457Z"
    },
    "id": "iFLvh1VQET35",
    "papermill": {
     "duration": 0.112433,
     "end_time": "2022-06-28T20:51:54.445285",
     "exception": false,
     "start_time": "2022-06-28T20:51:54.332852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train(\n",
    "        self, args, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        fix_all_seeds(args.seed)\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "\n",
    "            outputs_start, outputs_end = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            \n",
    "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            count += input_ids.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
    "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67e608",
   "metadata": {
    "papermill": {
     "duration": 0.110268,
     "end_time": "2022-06-28T20:51:54.657688",
     "exception": false,
     "start_time": "2022-06-28T20:51:54.547420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74edc7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:54.888358Z",
     "iopub.status.busy": "2022-06-28T20:51:54.887315Z",
     "iopub.status.idle": "2022-06-28T20:51:54.933372Z",
     "shell.execute_reply": "2022-06-28T20:51:54.934400Z",
     "shell.execute_reply.started": "2022-05-24T07:56:04.739565Z"
    },
    "id": "1a8kG2UYET36",
    "papermill": {
     "duration": 0.168189,
     "end_time": "2022-06-28T20:51:54.934613",
     "exception": false,
     "start_time": "2022-06-28T20:51:54.766424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "\n",
    "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "            \n",
    "            with torch.no_grad():            \n",
    "                outputs_start, outputs_end = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                \n",
    "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "                \n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c1458",
   "metadata": {
    "papermill": {
     "duration": 0.108391,
     "end_time": "2022-06-28T20:51:55.151290",
     "exception": false,
     "start_time": "2022-06-28T20:51:55.042899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3c7a2d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:55.305203Z",
     "iopub.status.busy": "2022-06-28T20:51:55.304457Z",
     "iopub.status.idle": "2022-06-28T20:51:55.306591Z",
     "shell.execute_reply": "2022-06-28T20:51:55.307052Z",
     "shell.execute_reply.started": "2022-05-24T07:56:06.049628Z"
    },
    "id": "v-gUDyq2ET37",
    "papermill": {
     "duration": 0.067371,
     "end_time": "2022-06-28T20:51:55.307178",
     "exception": false,
     "start_time": "2022-06-28T20:51:55.239807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_training(args, data, fold):\n",
    "    fix_all_seeds(args.seed)\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # model\n",
    "    model_config, tokenizer, model = make_model(args)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "\n",
    "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = make_optimizer(args, model)\n",
    "\n",
    "    # scheduler\n",
    "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
    "    if args.warmup_ratio > 0:\n",
    "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
    "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # mixed precision training with NVIDIA Apex\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        model, model_config, tokenizer, optimizer, scheduler, \n",
    "        train_dataloader, valid_dataloader, result_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4540a22",
   "metadata": {
    "papermill": {
     "duration": 0.032668,
     "end_time": "2022-06-28T20:51:55.372774",
     "exception": false,
     "start_time": "2022-06-28T20:51:55.340106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecc278ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:55.451679Z",
     "iopub.status.busy": "2022-06-28T20:51:55.450842Z",
     "iopub.status.idle": "2022-06-28T20:51:55.452628Z",
     "shell.execute_reply": "2022-06-28T20:51:55.453121Z",
     "shell.execute_reply.started": "2022-05-24T07:56:07.338569Z"
    },
    "id": "39ei5Bm5ET37",
    "papermill": {
     "duration": 0.04703,
     "end_time": "2022-06-28T20:51:55.453240",
     "exception": false,
     "start_time": "2022-06-28T20:51:55.406210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(data, fold):\n",
    "    args = Config()\n",
    "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
    "        valid_dataloader, result_dict = init_training(args, data, fold)\n",
    "    \n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
    "    evaluator = Evaluator(model)\n",
    "\n",
    "    train_time_list = []\n",
    "    valid_time_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "\n",
    "        # Train\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            args, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "        \n",
    "        # Evaluate\n",
    "        torch.cuda.synchronize()\n",
    "        tic3 = time.time()\n",
    "        result_dict = evaluator.evaluate(\n",
    "            valid_dataloader, epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic4 = time.time() \n",
    "        valid_time_list.append(tic4 - tic3)\n",
    "            \n",
    "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
    "            \n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
    "            \n",
    "        print()\n",
    "\n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    \n",
    "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
    "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del trainer, evaluator\n",
    "    del model, model_config, tokenizer\n",
    "    del optimizer, scheduler\n",
    "    del train_dataloader, valid_dataloader, result_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aef9c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T20:51:55.522901Z",
     "iopub.status.busy": "2022-06-28T20:51:55.522298Z",
     "iopub.status.idle": "2022-06-29T04:30:27.109914Z",
     "shell.execute_reply": "2022-06-29T04:30:27.109153Z",
     "shell.execute_reply.started": "2022-05-24T07:56:11.99205Z"
    },
    "papermill": {
     "duration": 27511.635175,
     "end_time": "2022-06-29T04:30:27.120966",
     "exception": false,
     "start_time": "2022-06-28T20:51:55.485791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "FOLD: 0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d276a19012a74f7baa9d274f4fae03ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27712, Num examples Valid=4010\n",
      "Total Training Steps: 3464, Total Warmup Steps: 346\n",
      "Epoch: 00 [    4/27712 (  0%)], Train Loss: 2.74942\n",
      "Epoch: 00 [   44/27712 (  0%)], Train Loss: 2.75038\n",
      "Epoch: 00 [   84/27712 (  0%)], Train Loss: 2.74278\n",
      "Epoch: 00 [  124/27712 (  0%)], Train Loss: 2.72111\n",
      "Epoch: 00 [  164/27712 (  1%)], Train Loss: 2.70307\n",
      "Epoch: 00 [  204/27712 (  1%)], Train Loss: 2.67185\n",
      "Epoch: 00 [  244/27712 (  1%)], Train Loss: 2.62834\n",
      "Epoch: 00 [  284/27712 (  1%)], Train Loss: 2.58518\n",
      "Epoch: 00 [  324/27712 (  1%)], Train Loss: 2.53550\n",
      "Epoch: 00 [  364/27712 (  1%)], Train Loss: 2.47337\n",
      "Epoch: 00 [  404/27712 (  1%)], Train Loss: 2.39788\n",
      "Epoch: 00 [  444/27712 (  2%)], Train Loss: 2.33247\n",
      "Epoch: 00 [  484/27712 (  2%)], Train Loss: 2.24378\n",
      "Epoch: 00 [  524/27712 (  2%)], Train Loss: 2.15825\n",
      "Epoch: 00 [  564/27712 (  2%)], Train Loss: 2.06871\n",
      "Epoch: 00 [  604/27712 (  2%)], Train Loss: 1.98638\n",
      "Epoch: 00 [  644/27712 (  2%)], Train Loss: 1.91199\n",
      "Epoch: 00 [  684/27712 (  2%)], Train Loss: 1.85325\n",
      "Epoch: 00 [  724/27712 (  3%)], Train Loss: 1.78691\n",
      "Epoch: 00 [  764/27712 (  3%)], Train Loss: 1.72126\n",
      "Epoch: 00 [  804/27712 (  3%)], Train Loss: 1.66789\n",
      "Epoch: 00 [  844/27712 (  3%)], Train Loss: 1.61414\n",
      "Epoch: 00 [  884/27712 (  3%)], Train Loss: 1.56950\n",
      "Epoch: 00 [  924/27712 (  3%)], Train Loss: 1.51916\n",
      "Epoch: 00 [  964/27712 (  3%)], Train Loss: 1.48658\n",
      "Epoch: 00 [ 1004/27712 (  4%)], Train Loss: 1.44847\n",
      "Epoch: 00 [ 1044/27712 (  4%)], Train Loss: 1.41573\n",
      "Epoch: 00 [ 1084/27712 (  4%)], Train Loss: 1.39274\n",
      "Epoch: 00 [ 1124/27712 (  4%)], Train Loss: 1.36070\n",
      "Epoch: 00 [ 1164/27712 (  4%)], Train Loss: 1.33494\n",
      "Epoch: 00 [ 1204/27712 (  4%)], Train Loss: 1.30343\n",
      "Epoch: 00 [ 1244/27712 (  4%)], Train Loss: 1.28460\n",
      "Epoch: 00 [ 1284/27712 (  5%)], Train Loss: 1.25532\n",
      "Epoch: 00 [ 1324/27712 (  5%)], Train Loss: 1.23508\n",
      "Epoch: 00 [ 1364/27712 (  5%)], Train Loss: 1.20981\n",
      "Epoch: 00 [ 1404/27712 (  5%)], Train Loss: 1.19374\n",
      "Epoch: 00 [ 1444/27712 (  5%)], Train Loss: 1.17264\n",
      "Epoch: 00 [ 1484/27712 (  5%)], Train Loss: 1.15294\n",
      "Epoch: 00 [ 1524/27712 (  5%)], Train Loss: 1.13797\n",
      "Epoch: 00 [ 1564/27712 (  6%)], Train Loss: 1.11666\n",
      "Epoch: 00 [ 1604/27712 (  6%)], Train Loss: 1.10008\n",
      "Epoch: 00 [ 1644/27712 (  6%)], Train Loss: 1.08394\n",
      "Epoch: 00 [ 1684/27712 (  6%)], Train Loss: 1.07140\n",
      "Epoch: 00 [ 1724/27712 (  6%)], Train Loss: 1.06093\n",
      "Epoch: 00 [ 1764/27712 (  6%)], Train Loss: 1.04655\n",
      "Epoch: 00 [ 1804/27712 (  7%)], Train Loss: 1.03031\n",
      "Epoch: 00 [ 1844/27712 (  7%)], Train Loss: 1.01977\n",
      "Epoch: 00 [ 1884/27712 (  7%)], Train Loss: 1.01026\n",
      "Epoch: 00 [ 1924/27712 (  7%)], Train Loss: 0.99975\n",
      "Epoch: 00 [ 1964/27712 (  7%)], Train Loss: 0.98844\n",
      "Epoch: 00 [ 2004/27712 (  7%)], Train Loss: 0.97750\n",
      "Epoch: 00 [ 2044/27712 (  7%)], Train Loss: 0.97082\n",
      "Epoch: 00 [ 2084/27712 (  8%)], Train Loss: 0.96112\n",
      "Epoch: 00 [ 2124/27712 (  8%)], Train Loss: 0.95188\n",
      "Epoch: 00 [ 2164/27712 (  8%)], Train Loss: 0.94285\n",
      "Epoch: 00 [ 2204/27712 (  8%)], Train Loss: 0.93638\n",
      "Epoch: 00 [ 2244/27712 (  8%)], Train Loss: 0.93398\n",
      "Epoch: 00 [ 2284/27712 (  8%)], Train Loss: 0.92529\n",
      "Epoch: 00 [ 2324/27712 (  8%)], Train Loss: 0.91719\n",
      "Epoch: 00 [ 2364/27712 (  9%)], Train Loss: 0.91082\n",
      "Epoch: 00 [ 2404/27712 (  9%)], Train Loss: 0.90380\n",
      "Epoch: 00 [ 2444/27712 (  9%)], Train Loss: 0.89464\n",
      "Epoch: 00 [ 2484/27712 (  9%)], Train Loss: 0.88960\n",
      "Epoch: 00 [ 2524/27712 (  9%)], Train Loss: 0.88313\n",
      "Epoch: 00 [ 2564/27712 (  9%)], Train Loss: 0.87893\n",
      "Epoch: 00 [ 2604/27712 (  9%)], Train Loss: 0.87280\n",
      "Epoch: 00 [ 2644/27712 ( 10%)], Train Loss: 0.86607\n",
      "Epoch: 00 [ 2684/27712 ( 10%)], Train Loss: 0.85937\n",
      "Epoch: 00 [ 2724/27712 ( 10%)], Train Loss: 0.85322\n",
      "Epoch: 00 [ 2764/27712 ( 10%)], Train Loss: 0.84876\n",
      "Epoch: 00 [ 2804/27712 ( 10%)], Train Loss: 0.84226\n",
      "Epoch: 00 [ 2844/27712 ( 10%)], Train Loss: 0.83719\n",
      "Epoch: 00 [ 2884/27712 ( 10%)], Train Loss: 0.83333\n",
      "Epoch: 00 [ 2924/27712 ( 11%)], Train Loss: 0.83111\n",
      "Epoch: 00 [ 2964/27712 ( 11%)], Train Loss: 0.82586\n",
      "Epoch: 00 [ 3004/27712 ( 11%)], Train Loss: 0.82168\n",
      "Epoch: 00 [ 3044/27712 ( 11%)], Train Loss: 0.81447\n",
      "Epoch: 00 [ 3084/27712 ( 11%)], Train Loss: 0.81077\n",
      "Epoch: 00 [ 3124/27712 ( 11%)], Train Loss: 0.80685\n",
      "Epoch: 00 [ 3164/27712 ( 11%)], Train Loss: 0.80351\n",
      "Epoch: 00 [ 3204/27712 ( 12%)], Train Loss: 0.79807\n",
      "Epoch: 00 [ 3244/27712 ( 12%)], Train Loss: 0.79247\n",
      "Epoch: 00 [ 3284/27712 ( 12%)], Train Loss: 0.78755\n",
      "Epoch: 00 [ 3324/27712 ( 12%)], Train Loss: 0.78450\n",
      "Epoch: 00 [ 3364/27712 ( 12%)], Train Loss: 0.78073\n",
      "Epoch: 00 [ 3404/27712 ( 12%)], Train Loss: 0.77574\n",
      "Epoch: 00 [ 3444/27712 ( 12%)], Train Loss: 0.77128\n",
      "Epoch: 00 [ 3484/27712 ( 13%)], Train Loss: 0.76846\n",
      "Epoch: 00 [ 3524/27712 ( 13%)], Train Loss: 0.76407\n",
      "Epoch: 00 [ 3564/27712 ( 13%)], Train Loss: 0.76025\n",
      "Epoch: 00 [ 3604/27712 ( 13%)], Train Loss: 0.75704\n",
      "Epoch: 00 [ 3644/27712 ( 13%)], Train Loss: 0.75440\n",
      "Epoch: 00 [ 3684/27712 ( 13%)], Train Loss: 0.75161\n",
      "Epoch: 00 [ 3724/27712 ( 13%)], Train Loss: 0.74835\n",
      "Epoch: 00 [ 3764/27712 ( 14%)], Train Loss: 0.74401\n",
      "Epoch: 00 [ 3804/27712 ( 14%)], Train Loss: 0.74012\n",
      "Epoch: 00 [ 3844/27712 ( 14%)], Train Loss: 0.73610\n",
      "Epoch: 00 [ 3884/27712 ( 14%)], Train Loss: 0.73369\n",
      "Epoch: 00 [ 3924/27712 ( 14%)], Train Loss: 0.73106\n",
      "Epoch: 00 [ 3964/27712 ( 14%)], Train Loss: 0.72925\n",
      "Epoch: 00 [ 4004/27712 ( 14%)], Train Loss: 0.72494\n",
      "Epoch: 00 [ 4044/27712 ( 15%)], Train Loss: 0.72100\n",
      "Epoch: 00 [ 4084/27712 ( 15%)], Train Loss: 0.71746\n",
      "Epoch: 00 [ 4124/27712 ( 15%)], Train Loss: 0.71576\n",
      "Epoch: 00 [ 4164/27712 ( 15%)], Train Loss: 0.71437\n",
      "Epoch: 00 [ 4204/27712 ( 15%)], Train Loss: 0.71194\n",
      "Epoch: 00 [ 4244/27712 ( 15%)], Train Loss: 0.71182\n",
      "Epoch: 00 [ 4284/27712 ( 15%)], Train Loss: 0.71034\n",
      "Epoch: 00 [ 4324/27712 ( 16%)], Train Loss: 0.70816\n",
      "Epoch: 00 [ 4364/27712 ( 16%)], Train Loss: 0.70517\n",
      "Epoch: 00 [ 4404/27712 ( 16%)], Train Loss: 0.70162\n",
      "Epoch: 00 [ 4444/27712 ( 16%)], Train Loss: 0.69907\n",
      "Epoch: 00 [ 4484/27712 ( 16%)], Train Loss: 0.69704\n",
      "Epoch: 00 [ 4524/27712 ( 16%)], Train Loss: 0.69425\n",
      "Epoch: 00 [ 4564/27712 ( 16%)], Train Loss: 0.69045\n",
      "Epoch: 00 [ 4604/27712 ( 17%)], Train Loss: 0.68857\n",
      "Epoch: 00 [ 4644/27712 ( 17%)], Train Loss: 0.68725\n",
      "Epoch: 00 [ 4684/27712 ( 17%)], Train Loss: 0.68520\n",
      "Epoch: 00 [ 4724/27712 ( 17%)], Train Loss: 0.68170\n",
      "Epoch: 00 [ 4764/27712 ( 17%)], Train Loss: 0.68071\n",
      "Epoch: 00 [ 4804/27712 ( 17%)], Train Loss: 0.67831\n",
      "Epoch: 00 [ 4844/27712 ( 17%)], Train Loss: 0.67603\n",
      "Epoch: 00 [ 4884/27712 ( 18%)], Train Loss: 0.67437\n",
      "Epoch: 00 [ 4924/27712 ( 18%)], Train Loss: 0.67262\n",
      "Epoch: 00 [ 4964/27712 ( 18%)], Train Loss: 0.67037\n",
      "Epoch: 00 [ 5004/27712 ( 18%)], Train Loss: 0.66885\n",
      "Epoch: 00 [ 5044/27712 ( 18%)], Train Loss: 0.66669\n",
      "Epoch: 00 [ 5084/27712 ( 18%)], Train Loss: 0.66349\n",
      "Epoch: 00 [ 5124/27712 ( 18%)], Train Loss: 0.66249\n",
      "Epoch: 00 [ 5164/27712 ( 19%)], Train Loss: 0.66065\n",
      "Epoch: 00 [ 5204/27712 ( 19%)], Train Loss: 0.65763\n",
      "Epoch: 00 [ 5244/27712 ( 19%)], Train Loss: 0.65477\n",
      "Epoch: 00 [ 5284/27712 ( 19%)], Train Loss: 0.65357\n",
      "Epoch: 00 [ 5324/27712 ( 19%)], Train Loss: 0.65197\n",
      "Epoch: 00 [ 5364/27712 ( 19%)], Train Loss: 0.65031\n",
      "Epoch: 00 [ 5404/27712 ( 20%)], Train Loss: 0.64797\n",
      "Epoch: 00 [ 5444/27712 ( 20%)], Train Loss: 0.64572\n",
      "Epoch: 00 [ 5484/27712 ( 20%)], Train Loss: 0.64411\n",
      "Epoch: 00 [ 5524/27712 ( 20%)], Train Loss: 0.64234\n",
      "Epoch: 00 [ 5564/27712 ( 20%)], Train Loss: 0.64210\n",
      "Epoch: 00 [ 5604/27712 ( 20%)], Train Loss: 0.64203\n",
      "Epoch: 00 [ 5644/27712 ( 20%)], Train Loss: 0.63989\n",
      "Epoch: 00 [ 5684/27712 ( 21%)], Train Loss: 0.63962\n",
      "Epoch: 00 [ 5724/27712 ( 21%)], Train Loss: 0.63813\n",
      "Epoch: 00 [ 5764/27712 ( 21%)], Train Loss: 0.63588\n",
      "Epoch: 00 [ 5804/27712 ( 21%)], Train Loss: 0.63460\n",
      "Epoch: 00 [ 5844/27712 ( 21%)], Train Loss: 0.63279\n",
      "Epoch: 00 [ 5884/27712 ( 21%)], Train Loss: 0.63188\n",
      "Epoch: 00 [ 5924/27712 ( 21%)], Train Loss: 0.63036\n",
      "Epoch: 00 [ 5964/27712 ( 22%)], Train Loss: 0.62963\n",
      "Epoch: 00 [ 6004/27712 ( 22%)], Train Loss: 0.62820\n",
      "Epoch: 00 [ 6044/27712 ( 22%)], Train Loss: 0.62680\n",
      "Epoch: 00 [ 6084/27712 ( 22%)], Train Loss: 0.62606\n",
      "Epoch: 00 [ 6124/27712 ( 22%)], Train Loss: 0.62372\n",
      "Epoch: 00 [ 6164/27712 ( 22%)], Train Loss: 0.62216\n",
      "Epoch: 00 [ 6204/27712 ( 22%)], Train Loss: 0.61996\n",
      "Epoch: 00 [ 6244/27712 ( 23%)], Train Loss: 0.61789\n",
      "Epoch: 00 [ 6284/27712 ( 23%)], Train Loss: 0.61588\n",
      "Epoch: 00 [ 6324/27712 ( 23%)], Train Loss: 0.61418\n",
      "Epoch: 00 [ 6364/27712 ( 23%)], Train Loss: 0.61247\n",
      "Epoch: 00 [ 6404/27712 ( 23%)], Train Loss: 0.61127\n",
      "Epoch: 00 [ 6444/27712 ( 23%)], Train Loss: 0.61046\n",
      "Epoch: 00 [ 6484/27712 ( 23%)], Train Loss: 0.60845\n",
      "Epoch: 00 [ 6524/27712 ( 24%)], Train Loss: 0.60753\n",
      "Epoch: 00 [ 6564/27712 ( 24%)], Train Loss: 0.60661\n",
      "Epoch: 00 [ 6604/27712 ( 24%)], Train Loss: 0.60544\n",
      "Epoch: 00 [ 6644/27712 ( 24%)], Train Loss: 0.60448\n",
      "Epoch: 00 [ 6684/27712 ( 24%)], Train Loss: 0.60391\n",
      "Epoch: 00 [ 6724/27712 ( 24%)], Train Loss: 0.60356\n",
      "Epoch: 00 [ 6764/27712 ( 24%)], Train Loss: 0.60161\n",
      "Epoch: 00 [ 6804/27712 ( 25%)], Train Loss: 0.60126\n",
      "Epoch: 00 [ 6844/27712 ( 25%)], Train Loss: 0.60015\n",
      "Epoch: 00 [ 6884/27712 ( 25%)], Train Loss: 0.59847\n",
      "Epoch: 00 [ 6924/27712 ( 25%)], Train Loss: 0.59675\n",
      "Epoch: 00 [ 6964/27712 ( 25%)], Train Loss: 0.59513\n",
      "Epoch: 00 [ 7004/27712 ( 25%)], Train Loss: 0.59469\n",
      "Epoch: 00 [ 7044/27712 ( 25%)], Train Loss: 0.59347\n",
      "Epoch: 00 [ 7084/27712 ( 26%)], Train Loss: 0.59269\n",
      "Epoch: 00 [ 7124/27712 ( 26%)], Train Loss: 0.59173\n",
      "Epoch: 00 [ 7164/27712 ( 26%)], Train Loss: 0.59078\n",
      "Epoch: 00 [ 7204/27712 ( 26%)], Train Loss: 0.59045\n",
      "Epoch: 00 [ 7244/27712 ( 26%)], Train Loss: 0.58907\n",
      "Epoch: 00 [ 7284/27712 ( 26%)], Train Loss: 0.58754\n",
      "Epoch: 00 [ 7324/27712 ( 26%)], Train Loss: 0.58651\n",
      "Epoch: 00 [ 7364/27712 ( 27%)], Train Loss: 0.58568\n",
      "Epoch: 00 [ 7404/27712 ( 27%)], Train Loss: 0.58428\n",
      "Epoch: 00 [ 7444/27712 ( 27%)], Train Loss: 0.58315\n",
      "Epoch: 00 [ 7484/27712 ( 27%)], Train Loss: 0.58263\n",
      "Epoch: 00 [ 7524/27712 ( 27%)], Train Loss: 0.58194\n",
      "Epoch: 00 [ 7564/27712 ( 27%)], Train Loss: 0.58063\n",
      "Epoch: 00 [ 7604/27712 ( 27%)], Train Loss: 0.58042\n",
      "Epoch: 00 [ 7644/27712 ( 28%)], Train Loss: 0.57956\n",
      "Epoch: 00 [ 7684/27712 ( 28%)], Train Loss: 0.57784\n",
      "Epoch: 00 [ 7724/27712 ( 28%)], Train Loss: 0.57694\n",
      "Epoch: 00 [ 7764/27712 ( 28%)], Train Loss: 0.57581\n",
      "Epoch: 00 [ 7804/27712 ( 28%)], Train Loss: 0.57496\n",
      "Epoch: 00 [ 7844/27712 ( 28%)], Train Loss: 0.57373\n",
      "Epoch: 00 [ 7884/27712 ( 28%)], Train Loss: 0.57294\n",
      "Epoch: 00 [ 7924/27712 ( 29%)], Train Loss: 0.57201\n",
      "Epoch: 00 [ 7964/27712 ( 29%)], Train Loss: 0.57140\n",
      "Epoch: 00 [ 8004/27712 ( 29%)], Train Loss: 0.57037\n",
      "Epoch: 00 [ 8044/27712 ( 29%)], Train Loss: 0.56984\n",
      "Epoch: 00 [ 8084/27712 ( 29%)], Train Loss: 0.56898\n",
      "Epoch: 00 [ 8124/27712 ( 29%)], Train Loss: 0.56762\n",
      "Epoch: 00 [ 8164/27712 ( 29%)], Train Loss: 0.56627\n",
      "Epoch: 00 [ 8204/27712 ( 30%)], Train Loss: 0.56514\n",
      "Epoch: 00 [ 8244/27712 ( 30%)], Train Loss: 0.56485\n",
      "Epoch: 00 [ 8284/27712 ( 30%)], Train Loss: 0.56368\n",
      "Epoch: 00 [ 8324/27712 ( 30%)], Train Loss: 0.56452\n",
      "Epoch: 00 [ 8364/27712 ( 30%)], Train Loss: 0.56375\n",
      "Epoch: 00 [ 8404/27712 ( 30%)], Train Loss: 0.56256\n",
      "Epoch: 00 [ 8444/27712 ( 30%)], Train Loss: 0.56178\n",
      "Epoch: 00 [ 8484/27712 ( 31%)], Train Loss: 0.56067\n",
      "Epoch: 00 [ 8524/27712 ( 31%)], Train Loss: 0.55960\n",
      "Epoch: 00 [ 8564/27712 ( 31%)], Train Loss: 0.55904\n",
      "Epoch: 00 [ 8604/27712 ( 31%)], Train Loss: 0.55834\n",
      "Epoch: 00 [ 8644/27712 ( 31%)], Train Loss: 0.55774\n",
      "Epoch: 00 [ 8684/27712 ( 31%)], Train Loss: 0.55683\n",
      "Epoch: 00 [ 8724/27712 ( 31%)], Train Loss: 0.55611\n",
      "Epoch: 00 [ 8764/27712 ( 32%)], Train Loss: 0.55602\n",
      "Epoch: 00 [ 8804/27712 ( 32%)], Train Loss: 0.55548\n",
      "Epoch: 00 [ 8844/27712 ( 32%)], Train Loss: 0.55486\n",
      "Epoch: 00 [ 8884/27712 ( 32%)], Train Loss: 0.55377\n",
      "Epoch: 00 [ 8924/27712 ( 32%)], Train Loss: 0.55351\n",
      "Epoch: 00 [ 8964/27712 ( 32%)], Train Loss: 0.55248\n",
      "Epoch: 00 [ 9004/27712 ( 32%)], Train Loss: 0.55129\n",
      "Epoch: 00 [ 9044/27712 ( 33%)], Train Loss: 0.54993\n",
      "Epoch: 00 [ 9084/27712 ( 33%)], Train Loss: 0.54869\n",
      "Epoch: 00 [ 9124/27712 ( 33%)], Train Loss: 0.54903\n",
      "Epoch: 00 [ 9164/27712 ( 33%)], Train Loss: 0.54836\n",
      "Epoch: 00 [ 9204/27712 ( 33%)], Train Loss: 0.54790\n",
      "Epoch: 00 [ 9244/27712 ( 33%)], Train Loss: 0.54662\n",
      "Epoch: 00 [ 9284/27712 ( 34%)], Train Loss: 0.54705\n",
      "Epoch: 00 [ 9324/27712 ( 34%)], Train Loss: 0.54613\n",
      "Epoch: 00 [ 9364/27712 ( 34%)], Train Loss: 0.54526\n",
      "Epoch: 00 [ 9404/27712 ( 34%)], Train Loss: 0.54547\n",
      "Epoch: 00 [ 9444/27712 ( 34%)], Train Loss: 0.54608\n",
      "Epoch: 00 [ 9484/27712 ( 34%)], Train Loss: 0.54500\n",
      "Epoch: 00 [ 9524/27712 ( 34%)], Train Loss: 0.54415\n",
      "Epoch: 00 [ 9564/27712 ( 35%)], Train Loss: 0.54317\n",
      "Epoch: 00 [ 9604/27712 ( 35%)], Train Loss: 0.54260\n",
      "Epoch: 00 [ 9644/27712 ( 35%)], Train Loss: 0.54172\n",
      "Epoch: 00 [ 9684/27712 ( 35%)], Train Loss: 0.54067\n",
      "Epoch: 00 [ 9724/27712 ( 35%)], Train Loss: 0.53993\n",
      "Epoch: 00 [ 9764/27712 ( 35%)], Train Loss: 0.53920\n",
      "Epoch: 00 [ 9804/27712 ( 35%)], Train Loss: 0.53933\n",
      "Epoch: 00 [ 9844/27712 ( 36%)], Train Loss: 0.53801\n",
      "Epoch: 00 [ 9884/27712 ( 36%)], Train Loss: 0.53731\n",
      "Epoch: 00 [ 9924/27712 ( 36%)], Train Loss: 0.53661\n",
      "Epoch: 00 [ 9964/27712 ( 36%)], Train Loss: 0.53622\n",
      "Epoch: 00 [10004/27712 ( 36%)], Train Loss: 0.53557\n",
      "Epoch: 00 [10044/27712 ( 36%)], Train Loss: 0.53532\n",
      "Epoch: 00 [10084/27712 ( 36%)], Train Loss: 0.53459\n",
      "Epoch: 00 [10124/27712 ( 37%)], Train Loss: 0.53434\n",
      "Epoch: 00 [10164/27712 ( 37%)], Train Loss: 0.53325\n",
      "Epoch: 00 [10204/27712 ( 37%)], Train Loss: 0.53296\n",
      "Epoch: 00 [10244/27712 ( 37%)], Train Loss: 0.53187\n",
      "Epoch: 00 [10284/27712 ( 37%)], Train Loss: 0.53177\n",
      "Epoch: 00 [10324/27712 ( 37%)], Train Loss: 0.53105\n",
      "Epoch: 00 [10364/27712 ( 37%)], Train Loss: 0.53008\n",
      "Epoch: 00 [10404/27712 ( 38%)], Train Loss: 0.52947\n",
      "Epoch: 00 [10444/27712 ( 38%)], Train Loss: 0.52863\n",
      "Epoch: 00 [10484/27712 ( 38%)], Train Loss: 0.52820\n",
      "Epoch: 00 [10524/27712 ( 38%)], Train Loss: 0.52829\n",
      "Epoch: 00 [10564/27712 ( 38%)], Train Loss: 0.52786\n",
      "Epoch: 00 [10604/27712 ( 38%)], Train Loss: 0.52739\n",
      "Epoch: 00 [10644/27712 ( 38%)], Train Loss: 0.52703\n",
      "Epoch: 00 [10684/27712 ( 39%)], Train Loss: 0.52610\n",
      "Epoch: 00 [10724/27712 ( 39%)], Train Loss: 0.52496\n",
      "Epoch: 00 [10764/27712 ( 39%)], Train Loss: 0.52453\n",
      "Epoch: 00 [10804/27712 ( 39%)], Train Loss: 0.52358\n",
      "Epoch: 00 [10844/27712 ( 39%)], Train Loss: 0.52353\n",
      "Epoch: 00 [10884/27712 ( 39%)], Train Loss: 0.52287\n",
      "Epoch: 00 [10924/27712 ( 39%)], Train Loss: 0.52250\n",
      "Epoch: 00 [10964/27712 ( 40%)], Train Loss: 0.52238\n",
      "Epoch: 00 [11004/27712 ( 40%)], Train Loss: 0.52171\n",
      "Epoch: 00 [11044/27712 ( 40%)], Train Loss: 0.52187\n",
      "Epoch: 00 [11084/27712 ( 40%)], Train Loss: 0.52113\n",
      "Epoch: 00 [11124/27712 ( 40%)], Train Loss: 0.52065\n",
      "Epoch: 00 [11164/27712 ( 40%)], Train Loss: 0.52012\n",
      "Epoch: 00 [11204/27712 ( 40%)], Train Loss: 0.51934\n",
      "Epoch: 00 [11244/27712 ( 41%)], Train Loss: 0.51834\n",
      "Epoch: 00 [11284/27712 ( 41%)], Train Loss: 0.51795\n",
      "Epoch: 00 [11324/27712 ( 41%)], Train Loss: 0.51796\n",
      "Epoch: 00 [11364/27712 ( 41%)], Train Loss: 0.51743\n",
      "Epoch: 00 [11404/27712 ( 41%)], Train Loss: 0.51689\n",
      "Epoch: 00 [11444/27712 ( 41%)], Train Loss: 0.51611\n",
      "Epoch: 00 [11484/27712 ( 41%)], Train Loss: 0.51537\n",
      "Epoch: 00 [11524/27712 ( 42%)], Train Loss: 0.51506\n",
      "Epoch: 00 [11564/27712 ( 42%)], Train Loss: 0.51435\n",
      "Epoch: 00 [11604/27712 ( 42%)], Train Loss: 0.51467\n",
      "Epoch: 00 [11644/27712 ( 42%)], Train Loss: 0.51386\n",
      "Epoch: 00 [11684/27712 ( 42%)], Train Loss: 0.51266\n",
      "Epoch: 00 [11724/27712 ( 42%)], Train Loss: 0.51205\n",
      "Epoch: 00 [11764/27712 ( 42%)], Train Loss: 0.51179\n",
      "Epoch: 00 [11804/27712 ( 43%)], Train Loss: 0.51131\n",
      "Epoch: 00 [11844/27712 ( 43%)], Train Loss: 0.51091\n",
      "Epoch: 00 [11884/27712 ( 43%)], Train Loss: 0.51039\n",
      "Epoch: 00 [11924/27712 ( 43%)], Train Loss: 0.50973\n",
      "Epoch: 00 [11964/27712 ( 43%)], Train Loss: 0.50892\n",
      "Epoch: 00 [12004/27712 ( 43%)], Train Loss: 0.50816\n",
      "Epoch: 00 [12044/27712 ( 43%)], Train Loss: 0.50756\n",
      "Epoch: 00 [12084/27712 ( 44%)], Train Loss: 0.50691\n",
      "Epoch: 00 [12124/27712 ( 44%)], Train Loss: 0.50619\n",
      "Epoch: 00 [12164/27712 ( 44%)], Train Loss: 0.50558\n",
      "Epoch: 00 [12204/27712 ( 44%)], Train Loss: 0.50511\n",
      "Epoch: 00 [12244/27712 ( 44%)], Train Loss: 0.50479\n",
      "Epoch: 00 [12284/27712 ( 44%)], Train Loss: 0.50417\n",
      "Epoch: 00 [12324/27712 ( 44%)], Train Loss: 0.50359\n",
      "Epoch: 00 [12364/27712 ( 45%)], Train Loss: 0.50277\n",
      "Epoch: 00 [12404/27712 ( 45%)], Train Loss: 0.50209\n",
      "Epoch: 00 [12444/27712 ( 45%)], Train Loss: 0.50099\n",
      "Epoch: 00 [12484/27712 ( 45%)], Train Loss: 0.50088\n",
      "Epoch: 00 [12524/27712 ( 45%)], Train Loss: 0.50056\n",
      "Epoch: 00 [12564/27712 ( 45%)], Train Loss: 0.50012\n",
      "Epoch: 00 [12604/27712 ( 45%)], Train Loss: 0.49995\n",
      "Epoch: 00 [12644/27712 ( 46%)], Train Loss: 0.50026\n",
      "Epoch: 00 [12684/27712 ( 46%)], Train Loss: 0.49967\n",
      "Epoch: 00 [12724/27712 ( 46%)], Train Loss: 0.49902\n",
      "Epoch: 00 [12764/27712 ( 46%)], Train Loss: 0.49842\n",
      "Epoch: 00 [12804/27712 ( 46%)], Train Loss: 0.49781\n",
      "Epoch: 00 [12844/27712 ( 46%)], Train Loss: 0.49766\n",
      "Epoch: 00 [12884/27712 ( 46%)], Train Loss: 0.49757\n",
      "Epoch: 00 [12924/27712 ( 47%)], Train Loss: 0.49748\n",
      "Epoch: 00 [12964/27712 ( 47%)], Train Loss: 0.49731\n",
      "Epoch: 00 [13004/27712 ( 47%)], Train Loss: 0.49694\n",
      "Epoch: 00 [13044/27712 ( 47%)], Train Loss: 0.49687\n",
      "Epoch: 00 [13084/27712 ( 47%)], Train Loss: 0.49718\n",
      "Epoch: 00 [13124/27712 ( 47%)], Train Loss: 0.49680\n",
      "Epoch: 00 [13164/27712 ( 48%)], Train Loss: 0.49614\n",
      "Epoch: 00 [13204/27712 ( 48%)], Train Loss: 0.49520\n",
      "Epoch: 00 [13244/27712 ( 48%)], Train Loss: 0.49476\n",
      "Epoch: 00 [13284/27712 ( 48%)], Train Loss: 0.49474\n",
      "Epoch: 00 [13324/27712 ( 48%)], Train Loss: 0.49414\n",
      "Epoch: 00 [13364/27712 ( 48%)], Train Loss: 0.49406\n",
      "Epoch: 00 [13404/27712 ( 48%)], Train Loss: 0.49390\n",
      "Epoch: 00 [13444/27712 ( 49%)], Train Loss: 0.49320\n",
      "Epoch: 00 [13484/27712 ( 49%)], Train Loss: 0.49264\n",
      "Epoch: 00 [13524/27712 ( 49%)], Train Loss: 0.49200\n",
      "Epoch: 00 [13564/27712 ( 49%)], Train Loss: 0.49144\n",
      "Epoch: 00 [13604/27712 ( 49%)], Train Loss: 0.49097\n",
      "Epoch: 00 [13644/27712 ( 49%)], Train Loss: 0.49127\n",
      "Epoch: 00 [13684/27712 ( 49%)], Train Loss: 0.49116\n",
      "Epoch: 00 [13724/27712 ( 50%)], Train Loss: 0.49060\n",
      "Epoch: 00 [13764/27712 ( 50%)], Train Loss: 0.49091\n",
      "Epoch: 00 [13804/27712 ( 50%)], Train Loss: 0.49051\n",
      "Epoch: 00 [13844/27712 ( 50%)], Train Loss: 0.49059\n",
      "Epoch: 00 [13884/27712 ( 50%)], Train Loss: 0.49012\n",
      "Epoch: 00 [13924/27712 ( 50%)], Train Loss: 0.48962\n",
      "Epoch: 00 [13964/27712 ( 50%)], Train Loss: 0.48915\n",
      "Epoch: 00 [14004/27712 ( 51%)], Train Loss: 0.48851\n",
      "Epoch: 00 [14044/27712 ( 51%)], Train Loss: 0.48851\n",
      "Epoch: 00 [14084/27712 ( 51%)], Train Loss: 0.48834\n",
      "Epoch: 00 [14124/27712 ( 51%)], Train Loss: 0.48780\n",
      "Epoch: 00 [14164/27712 ( 51%)], Train Loss: 0.48745\n",
      "Epoch: 00 [14204/27712 ( 51%)], Train Loss: 0.48722\n",
      "Epoch: 00 [14244/27712 ( 51%)], Train Loss: 0.48698\n",
      "Epoch: 00 [14284/27712 ( 52%)], Train Loss: 0.48663\n",
      "Epoch: 00 [14324/27712 ( 52%)], Train Loss: 0.48638\n",
      "Epoch: 00 [14364/27712 ( 52%)], Train Loss: 0.48579\n",
      "Epoch: 00 [14404/27712 ( 52%)], Train Loss: 0.48508\n",
      "Epoch: 00 [14444/27712 ( 52%)], Train Loss: 0.48492\n",
      "Epoch: 00 [14484/27712 ( 52%)], Train Loss: 0.48461\n",
      "Epoch: 00 [14524/27712 ( 52%)], Train Loss: 0.48423\n",
      "Epoch: 00 [14564/27712 ( 53%)], Train Loss: 0.48407\n",
      "Epoch: 00 [14604/27712 ( 53%)], Train Loss: 0.48345\n",
      "Epoch: 00 [14644/27712 ( 53%)], Train Loss: 0.48338\n",
      "Epoch: 00 [14684/27712 ( 53%)], Train Loss: 0.48308\n",
      "Epoch: 00 [14724/27712 ( 53%)], Train Loss: 0.48263\n",
      "Epoch: 00 [14764/27712 ( 53%)], Train Loss: 0.48194\n",
      "Epoch: 00 [14804/27712 ( 53%)], Train Loss: 0.48142\n",
      "Epoch: 00 [14844/27712 ( 54%)], Train Loss: 0.48129\n",
      "Epoch: 00 [14884/27712 ( 54%)], Train Loss: 0.48098\n",
      "Epoch: 00 [14924/27712 ( 54%)], Train Loss: 0.48033\n",
      "Epoch: 00 [14964/27712 ( 54%)], Train Loss: 0.47978\n",
      "Epoch: 00 [15004/27712 ( 54%)], Train Loss: 0.47900\n",
      "Epoch: 00 [15044/27712 ( 54%)], Train Loss: 0.47928\n",
      "Epoch: 00 [15084/27712 ( 54%)], Train Loss: 0.47931\n",
      "Epoch: 00 [15124/27712 ( 55%)], Train Loss: 0.47869\n",
      "Epoch: 00 [15164/27712 ( 55%)], Train Loss: 0.47813\n",
      "Epoch: 00 [15204/27712 ( 55%)], Train Loss: 0.47725\n",
      "Epoch: 00 [15244/27712 ( 55%)], Train Loss: 0.47651\n",
      "Epoch: 00 [15284/27712 ( 55%)], Train Loss: 0.47575\n",
      "Epoch: 00 [15324/27712 ( 55%)], Train Loss: 0.47515\n",
      "Epoch: 00 [15364/27712 ( 55%)], Train Loss: 0.47491\n",
      "Epoch: 00 [15404/27712 ( 56%)], Train Loss: 0.47422\n",
      "Epoch: 00 [15444/27712 ( 56%)], Train Loss: 0.47345\n",
      "Epoch: 00 [15484/27712 ( 56%)], Train Loss: 0.47330\n",
      "Epoch: 00 [15524/27712 ( 56%)], Train Loss: 0.47285\n",
      "Epoch: 00 [15564/27712 ( 56%)], Train Loss: 0.47234\n",
      "Epoch: 00 [15604/27712 ( 56%)], Train Loss: 0.47226\n",
      "Epoch: 00 [15644/27712 ( 56%)], Train Loss: 0.47210\n",
      "Epoch: 00 [15684/27712 ( 57%)], Train Loss: 0.47184\n",
      "Epoch: 00 [15724/27712 ( 57%)], Train Loss: 0.47136\n",
      "Epoch: 00 [15764/27712 ( 57%)], Train Loss: 0.47088\n",
      "Epoch: 00 [15804/27712 ( 57%)], Train Loss: 0.47050\n",
      "Epoch: 00 [15844/27712 ( 57%)], Train Loss: 0.47003\n",
      "Epoch: 00 [15884/27712 ( 57%)], Train Loss: 0.47011\n",
      "Epoch: 00 [15924/27712 ( 57%)], Train Loss: 0.46957\n",
      "Epoch: 00 [15964/27712 ( 58%)], Train Loss: 0.46957\n",
      "Epoch: 00 [16004/27712 ( 58%)], Train Loss: 0.46902\n",
      "Epoch: 00 [16044/27712 ( 58%)], Train Loss: 0.46884\n",
      "Epoch: 00 [16084/27712 ( 58%)], Train Loss: 0.46839\n",
      "Epoch: 00 [16124/27712 ( 58%)], Train Loss: 0.46827\n",
      "Epoch: 00 [16164/27712 ( 58%)], Train Loss: 0.46801\n",
      "Epoch: 00 [16204/27712 ( 58%)], Train Loss: 0.46756\n",
      "Epoch: 00 [16244/27712 ( 59%)], Train Loss: 0.46694\n",
      "Epoch: 00 [16284/27712 ( 59%)], Train Loss: 0.46664\n",
      "Epoch: 00 [16324/27712 ( 59%)], Train Loss: 0.46636\n",
      "Epoch: 00 [16364/27712 ( 59%)], Train Loss: 0.46589\n",
      "Epoch: 00 [16404/27712 ( 59%)], Train Loss: 0.46573\n",
      "Epoch: 00 [16444/27712 ( 59%)], Train Loss: 0.46540\n",
      "Epoch: 00 [16484/27712 ( 59%)], Train Loss: 0.46495\n",
      "Epoch: 00 [16524/27712 ( 60%)], Train Loss: 0.46460\n",
      "Epoch: 00 [16564/27712 ( 60%)], Train Loss: 0.46431\n",
      "Epoch: 00 [16604/27712 ( 60%)], Train Loss: 0.46393\n",
      "Epoch: 00 [16644/27712 ( 60%)], Train Loss: 0.46365\n",
      "Epoch: 00 [16684/27712 ( 60%)], Train Loss: 0.46313\n",
      "Epoch: 00 [16724/27712 ( 60%)], Train Loss: 0.46352\n",
      "Epoch: 00 [16764/27712 ( 60%)], Train Loss: 0.46347\n",
      "Epoch: 00 [16804/27712 ( 61%)], Train Loss: 0.46322\n",
      "Epoch: 00 [16844/27712 ( 61%)], Train Loss: 0.46272\n",
      "Epoch: 00 [16884/27712 ( 61%)], Train Loss: 0.46231\n",
      "Epoch: 00 [16924/27712 ( 61%)], Train Loss: 0.46244\n",
      "Epoch: 00 [16964/27712 ( 61%)], Train Loss: 0.46187\n",
      "Epoch: 00 [17004/27712 ( 61%)], Train Loss: 0.46135\n",
      "Epoch: 00 [17044/27712 ( 62%)], Train Loss: 0.46083\n",
      "Epoch: 00 [17084/27712 ( 62%)], Train Loss: 0.46046\n",
      "Epoch: 00 [17124/27712 ( 62%)], Train Loss: 0.46025\n",
      "Epoch: 00 [17164/27712 ( 62%)], Train Loss: 0.45978\n",
      "Epoch: 00 [17204/27712 ( 62%)], Train Loss: 0.45990\n",
      "Epoch: 00 [17244/27712 ( 62%)], Train Loss: 0.45952\n",
      "Epoch: 00 [17284/27712 ( 62%)], Train Loss: 0.45903\n",
      "Epoch: 00 [17324/27712 ( 63%)], Train Loss: 0.45842\n",
      "Epoch: 00 [17364/27712 ( 63%)], Train Loss: 0.45781\n",
      "Epoch: 00 [17404/27712 ( 63%)], Train Loss: 0.45757\n",
      "Epoch: 00 [17444/27712 ( 63%)], Train Loss: 0.45733\n",
      "Epoch: 00 [17484/27712 ( 63%)], Train Loss: 0.45708\n",
      "Epoch: 00 [17524/27712 ( 63%)], Train Loss: 0.45679\n",
      "Epoch: 00 [17564/27712 ( 63%)], Train Loss: 0.45616\n",
      "Epoch: 00 [17604/27712 ( 64%)], Train Loss: 0.45572\n",
      "Epoch: 00 [17644/27712 ( 64%)], Train Loss: 0.45571\n",
      "Epoch: 00 [17684/27712 ( 64%)], Train Loss: 0.45568\n",
      "Epoch: 00 [17724/27712 ( 64%)], Train Loss: 0.45523\n",
      "Epoch: 00 [17764/27712 ( 64%)], Train Loss: 0.45488\n",
      "Epoch: 00 [17804/27712 ( 64%)], Train Loss: 0.45455\n",
      "Epoch: 00 [17844/27712 ( 64%)], Train Loss: 0.45413\n",
      "Epoch: 00 [17884/27712 ( 65%)], Train Loss: 0.45359\n",
      "Epoch: 00 [17924/27712 ( 65%)], Train Loss: 0.45322\n",
      "Epoch: 00 [17964/27712 ( 65%)], Train Loss: 0.45295\n",
      "Epoch: 00 [18004/27712 ( 65%)], Train Loss: 0.45236\n",
      "Epoch: 00 [18044/27712 ( 65%)], Train Loss: 0.45216\n",
      "Epoch: 00 [18084/27712 ( 65%)], Train Loss: 0.45204\n",
      "Epoch: 00 [18124/27712 ( 65%)], Train Loss: 0.45182\n",
      "Epoch: 00 [18164/27712 ( 66%)], Train Loss: 0.45124\n",
      "Epoch: 00 [18204/27712 ( 66%)], Train Loss: 0.45057\n",
      "Epoch: 00 [18244/27712 ( 66%)], Train Loss: 0.45056\n",
      "Epoch: 00 [18284/27712 ( 66%)], Train Loss: 0.45021\n",
      "Epoch: 00 [18324/27712 ( 66%)], Train Loss: 0.45030\n",
      "Epoch: 00 [18364/27712 ( 66%)], Train Loss: 0.45028\n",
      "Epoch: 00 [18404/27712 ( 66%)], Train Loss: 0.44983\n",
      "Epoch: 00 [18444/27712 ( 67%)], Train Loss: 0.44972\n",
      "Epoch: 00 [18484/27712 ( 67%)], Train Loss: 0.44930\n",
      "Epoch: 00 [18524/27712 ( 67%)], Train Loss: 0.44936\n",
      "Epoch: 00 [18564/27712 ( 67%)], Train Loss: 0.44924\n",
      "Epoch: 00 [18604/27712 ( 67%)], Train Loss: 0.44946\n",
      "Epoch: 00 [18644/27712 ( 67%)], Train Loss: 0.44897\n",
      "Epoch: 00 [18684/27712 ( 67%)], Train Loss: 0.44844\n",
      "Epoch: 00 [18724/27712 ( 68%)], Train Loss: 0.44814\n",
      "Epoch: 00 [18764/27712 ( 68%)], Train Loss: 0.44809\n",
      "Epoch: 00 [18804/27712 ( 68%)], Train Loss: 0.44792\n",
      "Epoch: 00 [18844/27712 ( 68%)], Train Loss: 0.44782\n",
      "Epoch: 00 [18884/27712 ( 68%)], Train Loss: 0.44748\n",
      "Epoch: 00 [18924/27712 ( 68%)], Train Loss: 0.44745\n",
      "Epoch: 00 [18964/27712 ( 68%)], Train Loss: 0.44711\n",
      "Epoch: 00 [19004/27712 ( 69%)], Train Loss: 0.44727\n",
      "Epoch: 00 [19044/27712 ( 69%)], Train Loss: 0.44690\n",
      "Epoch: 00 [19084/27712 ( 69%)], Train Loss: 0.44653\n",
      "Epoch: 00 [19124/27712 ( 69%)], Train Loss: 0.44602\n",
      "Epoch: 00 [19164/27712 ( 69%)], Train Loss: 0.44568\n",
      "Epoch: 00 [19204/27712 ( 69%)], Train Loss: 0.44543\n",
      "Epoch: 00 [19244/27712 ( 69%)], Train Loss: 0.44517\n",
      "Epoch: 00 [19284/27712 ( 70%)], Train Loss: 0.44523\n",
      "Epoch: 00 [19324/27712 ( 70%)], Train Loss: 0.44504\n",
      "Epoch: 00 [19364/27712 ( 70%)], Train Loss: 0.44507\n",
      "Epoch: 00 [19404/27712 ( 70%)], Train Loss: 0.44476\n",
      "Epoch: 00 [19444/27712 ( 70%)], Train Loss: 0.44435\n",
      "Epoch: 00 [19484/27712 ( 70%)], Train Loss: 0.44431\n",
      "Epoch: 00 [19524/27712 ( 70%)], Train Loss: 0.44407\n",
      "Epoch: 00 [19564/27712 ( 71%)], Train Loss: 0.44383\n",
      "Epoch: 00 [19604/27712 ( 71%)], Train Loss: 0.44358\n",
      "Epoch: 00 [19644/27712 ( 71%)], Train Loss: 0.44306\n",
      "Epoch: 00 [19684/27712 ( 71%)], Train Loss: 0.44293\n",
      "Epoch: 00 [19724/27712 ( 71%)], Train Loss: 0.44278\n",
      "Epoch: 00 [19764/27712 ( 71%)], Train Loss: 0.44266\n",
      "Epoch: 00 [19804/27712 ( 71%)], Train Loss: 0.44224\n",
      "Epoch: 00 [19844/27712 ( 72%)], Train Loss: 0.44214\n",
      "Epoch: 00 [19884/27712 ( 72%)], Train Loss: 0.44195\n",
      "Epoch: 00 [19924/27712 ( 72%)], Train Loss: 0.44156\n",
      "Epoch: 00 [19964/27712 ( 72%)], Train Loss: 0.44160\n",
      "Epoch: 00 [20004/27712 ( 72%)], Train Loss: 0.44128\n",
      "Epoch: 00 [20044/27712 ( 72%)], Train Loss: 0.44123\n",
      "Epoch: 00 [20084/27712 ( 72%)], Train Loss: 0.44082\n",
      "Epoch: 00 [20124/27712 ( 73%)], Train Loss: 0.44054\n",
      "Epoch: 00 [20164/27712 ( 73%)], Train Loss: 0.44033\n",
      "Epoch: 00 [20204/27712 ( 73%)], Train Loss: 0.44028\n",
      "Epoch: 00 [20244/27712 ( 73%)], Train Loss: 0.43982\n",
      "Epoch: 00 [20284/27712 ( 73%)], Train Loss: 0.43943\n",
      "Epoch: 00 [20324/27712 ( 73%)], Train Loss: 0.43952\n",
      "Epoch: 00 [20364/27712 ( 73%)], Train Loss: 0.43946\n",
      "Epoch: 00 [20404/27712 ( 74%)], Train Loss: 0.43925\n",
      "Epoch: 00 [20444/27712 ( 74%)], Train Loss: 0.43888\n",
      "Epoch: 00 [20484/27712 ( 74%)], Train Loss: 0.43861\n",
      "Epoch: 00 [20524/27712 ( 74%)], Train Loss: 0.43848\n",
      "Epoch: 00 [20564/27712 ( 74%)], Train Loss: 0.43819\n",
      "Epoch: 00 [20604/27712 ( 74%)], Train Loss: 0.43796\n",
      "Epoch: 00 [20644/27712 ( 74%)], Train Loss: 0.43767\n",
      "Epoch: 00 [20684/27712 ( 75%)], Train Loss: 0.43730\n",
      "Epoch: 00 [20724/27712 ( 75%)], Train Loss: 0.43732\n",
      "Epoch: 00 [20764/27712 ( 75%)], Train Loss: 0.43719\n",
      "Epoch: 00 [20804/27712 ( 75%)], Train Loss: 0.43696\n",
      "Epoch: 00 [20844/27712 ( 75%)], Train Loss: 0.43687\n",
      "Epoch: 00 [20884/27712 ( 75%)], Train Loss: 0.43676\n",
      "Epoch: 00 [20924/27712 ( 76%)], Train Loss: 0.43657\n",
      "Epoch: 00 [20964/27712 ( 76%)], Train Loss: 0.43663\n",
      "Epoch: 00 [21004/27712 ( 76%)], Train Loss: 0.43634\n",
      "Epoch: 00 [21044/27712 ( 76%)], Train Loss: 0.43603\n",
      "Epoch: 00 [21084/27712 ( 76%)], Train Loss: 0.43589\n",
      "Epoch: 00 [21124/27712 ( 76%)], Train Loss: 0.43553\n",
      "Epoch: 00 [21164/27712 ( 76%)], Train Loss: 0.43518\n",
      "Epoch: 00 [21204/27712 ( 77%)], Train Loss: 0.43482\n",
      "Epoch: 00 [21244/27712 ( 77%)], Train Loss: 0.43492\n",
      "Epoch: 00 [21284/27712 ( 77%)], Train Loss: 0.43472\n",
      "Epoch: 00 [21324/27712 ( 77%)], Train Loss: 0.43451\n",
      "Epoch: 00 [21364/27712 ( 77%)], Train Loss: 0.43432\n",
      "Epoch: 00 [21404/27712 ( 77%)], Train Loss: 0.43414\n",
      "Epoch: 00 [21444/27712 ( 77%)], Train Loss: 0.43372\n",
      "Epoch: 00 [21484/27712 ( 78%)], Train Loss: 0.43362\n",
      "Epoch: 00 [21524/27712 ( 78%)], Train Loss: 0.43318\n",
      "Epoch: 00 [21564/27712 ( 78%)], Train Loss: 0.43286\n",
      "Epoch: 00 [21604/27712 ( 78%)], Train Loss: 0.43269\n",
      "Epoch: 00 [21644/27712 ( 78%)], Train Loss: 0.43243\n",
      "Epoch: 00 [21684/27712 ( 78%)], Train Loss: 0.43211\n",
      "Epoch: 00 [21724/27712 ( 78%)], Train Loss: 0.43190\n",
      "Epoch: 00 [21764/27712 ( 79%)], Train Loss: 0.43164\n",
      "Epoch: 00 [21804/27712 ( 79%)], Train Loss: 0.43150\n",
      "Epoch: 00 [21844/27712 ( 79%)], Train Loss: 0.43119\n",
      "Epoch: 00 [21884/27712 ( 79%)], Train Loss: 0.43091\n",
      "Epoch: 00 [21924/27712 ( 79%)], Train Loss: 0.43052\n",
      "Epoch: 00 [21964/27712 ( 79%)], Train Loss: 0.43043\n",
      "Epoch: 00 [22004/27712 ( 79%)], Train Loss: 0.43031\n",
      "Epoch: 00 [22044/27712 ( 80%)], Train Loss: 0.43049\n",
      "Epoch: 00 [22084/27712 ( 80%)], Train Loss: 0.43010\n",
      "Epoch: 00 [22124/27712 ( 80%)], Train Loss: 0.42970\n",
      "Epoch: 00 [22164/27712 ( 80%)], Train Loss: 0.42946\n",
      "Epoch: 00 [22204/27712 ( 80%)], Train Loss: 0.42907\n",
      "Epoch: 00 [22244/27712 ( 80%)], Train Loss: 0.42872\n",
      "Epoch: 00 [22284/27712 ( 80%)], Train Loss: 0.42810\n",
      "Epoch: 00 [22324/27712 ( 81%)], Train Loss: 0.42786\n",
      "Epoch: 00 [22364/27712 ( 81%)], Train Loss: 0.42760\n",
      "Epoch: 00 [22404/27712 ( 81%)], Train Loss: 0.42718\n",
      "Epoch: 00 [22444/27712 ( 81%)], Train Loss: 0.42705\n",
      "Epoch: 00 [22484/27712 ( 81%)], Train Loss: 0.42680\n",
      "Epoch: 00 [22524/27712 ( 81%)], Train Loss: 0.42645\n",
      "Epoch: 00 [22564/27712 ( 81%)], Train Loss: 0.42620\n",
      "Epoch: 00 [22604/27712 ( 82%)], Train Loss: 0.42589\n",
      "Epoch: 00 [22644/27712 ( 82%)], Train Loss: 0.42559\n",
      "Epoch: 00 [22684/27712 ( 82%)], Train Loss: 0.42577\n",
      "Epoch: 00 [22724/27712 ( 82%)], Train Loss: 0.42545\n",
      "Epoch: 00 [22764/27712 ( 82%)], Train Loss: 0.42548\n",
      "Epoch: 00 [22804/27712 ( 82%)], Train Loss: 0.42510\n",
      "Epoch: 00 [22844/27712 ( 82%)], Train Loss: 0.42493\n",
      "Epoch: 00 [22884/27712 ( 83%)], Train Loss: 0.42481\n",
      "Epoch: 00 [22924/27712 ( 83%)], Train Loss: 0.42441\n",
      "Epoch: 00 [22964/27712 ( 83%)], Train Loss: 0.42418\n",
      "Epoch: 00 [23004/27712 ( 83%)], Train Loss: 0.42396\n",
      "Epoch: 00 [23044/27712 ( 83%)], Train Loss: 0.42362\n",
      "Epoch: 00 [23084/27712 ( 83%)], Train Loss: 0.42360\n",
      "Epoch: 00 [23124/27712 ( 83%)], Train Loss: 0.42340\n",
      "Epoch: 00 [23164/27712 ( 84%)], Train Loss: 0.42318\n",
      "Epoch: 00 [23204/27712 ( 84%)], Train Loss: 0.42308\n",
      "Epoch: 00 [23244/27712 ( 84%)], Train Loss: 0.42302\n",
      "Epoch: 00 [23284/27712 ( 84%)], Train Loss: 0.42282\n",
      "Epoch: 00 [23324/27712 ( 84%)], Train Loss: 0.42249\n",
      "Epoch: 00 [23364/27712 ( 84%)], Train Loss: 0.42248\n",
      "Epoch: 00 [23404/27712 ( 84%)], Train Loss: 0.42224\n",
      "Epoch: 00 [23444/27712 ( 85%)], Train Loss: 0.42215\n",
      "Epoch: 00 [23484/27712 ( 85%)], Train Loss: 0.42212\n",
      "Epoch: 00 [23524/27712 ( 85%)], Train Loss: 0.42199\n",
      "Epoch: 00 [23564/27712 ( 85%)], Train Loss: 0.42171\n",
      "Epoch: 00 [23604/27712 ( 85%)], Train Loss: 0.42154\n",
      "Epoch: 00 [23644/27712 ( 85%)], Train Loss: 0.42140\n",
      "Epoch: 00 [23684/27712 ( 85%)], Train Loss: 0.42164\n",
      "Epoch: 00 [23724/27712 ( 86%)], Train Loss: 0.42128\n",
      "Epoch: 00 [23764/27712 ( 86%)], Train Loss: 0.42099\n",
      "Epoch: 00 [23804/27712 ( 86%)], Train Loss: 0.42074\n",
      "Epoch: 00 [23844/27712 ( 86%)], Train Loss: 0.42039\n",
      "Epoch: 00 [23884/27712 ( 86%)], Train Loss: 0.41998\n",
      "Epoch: 00 [23924/27712 ( 86%)], Train Loss: 0.41992\n",
      "Epoch: 00 [23964/27712 ( 86%)], Train Loss: 0.41962\n",
      "Epoch: 00 [24004/27712 ( 87%)], Train Loss: 0.41919\n",
      "Epoch: 00 [24044/27712 ( 87%)], Train Loss: 0.41889\n",
      "Epoch: 00 [24084/27712 ( 87%)], Train Loss: 0.41892\n",
      "Epoch: 00 [24124/27712 ( 87%)], Train Loss: 0.41864\n",
      "Epoch: 00 [24164/27712 ( 87%)], Train Loss: 0.41829\n",
      "Epoch: 00 [24204/27712 ( 87%)], Train Loss: 0.41810\n",
      "Epoch: 00 [24244/27712 ( 87%)], Train Loss: 0.41786\n",
      "Epoch: 00 [24284/27712 ( 88%)], Train Loss: 0.41775\n",
      "Epoch: 00 [24324/27712 ( 88%)], Train Loss: 0.41763\n",
      "Epoch: 00 [24364/27712 ( 88%)], Train Loss: 0.41743\n",
      "Epoch: 00 [24404/27712 ( 88%)], Train Loss: 0.41725\n",
      "Epoch: 00 [24444/27712 ( 88%)], Train Loss: 0.41692\n",
      "Epoch: 00 [24484/27712 ( 88%)], Train Loss: 0.41671\n",
      "Epoch: 00 [24524/27712 ( 88%)], Train Loss: 0.41656\n",
      "Epoch: 00 [24564/27712 ( 89%)], Train Loss: 0.41637\n",
      "Epoch: 00 [24604/27712 ( 89%)], Train Loss: 0.41617\n",
      "Epoch: 00 [24644/27712 ( 89%)], Train Loss: 0.41594\n",
      "Epoch: 00 [24684/27712 ( 89%)], Train Loss: 0.41574\n",
      "Epoch: 00 [24724/27712 ( 89%)], Train Loss: 0.41579\n",
      "Epoch: 00 [24764/27712 ( 89%)], Train Loss: 0.41546\n",
      "Epoch: 00 [24804/27712 ( 90%)], Train Loss: 0.41521\n",
      "Epoch: 00 [24844/27712 ( 90%)], Train Loss: 0.41500\n",
      "Epoch: 00 [24884/27712 ( 90%)], Train Loss: 0.41484\n",
      "Epoch: 00 [24924/27712 ( 90%)], Train Loss: 0.41470\n",
      "Epoch: 00 [24964/27712 ( 90%)], Train Loss: 0.41427\n",
      "Epoch: 00 [25004/27712 ( 90%)], Train Loss: 0.41386\n",
      "Epoch: 00 [25044/27712 ( 90%)], Train Loss: 0.41387\n",
      "Epoch: 00 [25084/27712 ( 91%)], Train Loss: 0.41361\n",
      "Epoch: 00 [25124/27712 ( 91%)], Train Loss: 0.41338\n",
      "Epoch: 00 [25164/27712 ( 91%)], Train Loss: 0.41318\n",
      "Epoch: 00 [25204/27712 ( 91%)], Train Loss: 0.41286\n",
      "Epoch: 00 [25244/27712 ( 91%)], Train Loss: 0.41247\n",
      "Epoch: 00 [25284/27712 ( 91%)], Train Loss: 0.41232\n",
      "Epoch: 00 [25324/27712 ( 91%)], Train Loss: 0.41221\n",
      "Epoch: 00 [25364/27712 ( 92%)], Train Loss: 0.41210\n",
      "Epoch: 00 [25404/27712 ( 92%)], Train Loss: 0.41202\n",
      "Epoch: 00 [25444/27712 ( 92%)], Train Loss: 0.41202\n",
      "Epoch: 00 [25484/27712 ( 92%)], Train Loss: 0.41171\n",
      "Epoch: 00 [25524/27712 ( 92%)], Train Loss: 0.41152\n",
      "Epoch: 00 [25564/27712 ( 92%)], Train Loss: 0.41131\n",
      "Epoch: 00 [25604/27712 ( 92%)], Train Loss: 0.41101\n",
      "Epoch: 00 [25644/27712 ( 93%)], Train Loss: 0.41101\n",
      "Epoch: 00 [25684/27712 ( 93%)], Train Loss: 0.41073\n",
      "Epoch: 00 [25724/27712 ( 93%)], Train Loss: 0.41048\n",
      "Epoch: 00 [25764/27712 ( 93%)], Train Loss: 0.41034\n",
      "Epoch: 00 [25804/27712 ( 93%)], Train Loss: 0.41001\n",
      "Epoch: 00 [25844/27712 ( 93%)], Train Loss: 0.40987\n",
      "Epoch: 00 [25884/27712 ( 93%)], Train Loss: 0.40989\n",
      "Epoch: 00 [25924/27712 ( 94%)], Train Loss: 0.40969\n",
      "Epoch: 00 [25964/27712 ( 94%)], Train Loss: 0.40943\n",
      "Epoch: 00 [26004/27712 ( 94%)], Train Loss: 0.40924\n",
      "Epoch: 00 [26044/27712 ( 94%)], Train Loss: 0.40905\n",
      "Epoch: 00 [26084/27712 ( 94%)], Train Loss: 0.40883\n",
      "Epoch: 00 [26124/27712 ( 94%)], Train Loss: 0.40861\n",
      "Epoch: 00 [26164/27712 ( 94%)], Train Loss: 0.40849\n",
      "Epoch: 00 [26204/27712 ( 95%)], Train Loss: 0.40851\n",
      "Epoch: 00 [26244/27712 ( 95%)], Train Loss: 0.40823\n",
      "Epoch: 00 [26284/27712 ( 95%)], Train Loss: 0.40797\n",
      "Epoch: 00 [26324/27712 ( 95%)], Train Loss: 0.40778\n",
      "Epoch: 00 [26364/27712 ( 95%)], Train Loss: 0.40753\n",
      "Epoch: 00 [26404/27712 ( 95%)], Train Loss: 0.40728\n",
      "Epoch: 00 [26444/27712 ( 95%)], Train Loss: 0.40710\n",
      "Epoch: 00 [26484/27712 ( 96%)], Train Loss: 0.40695\n",
      "Epoch: 00 [26524/27712 ( 96%)], Train Loss: 0.40668\n",
      "Epoch: 00 [26564/27712 ( 96%)], Train Loss: 0.40648\n",
      "Epoch: 00 [26604/27712 ( 96%)], Train Loss: 0.40619\n",
      "Epoch: 00 [26644/27712 ( 96%)], Train Loss: 0.40646\n",
      "Epoch: 00 [26684/27712 ( 96%)], Train Loss: 0.40625\n",
      "Epoch: 00 [26724/27712 ( 96%)], Train Loss: 0.40602\n",
      "Epoch: 00 [26764/27712 ( 97%)], Train Loss: 0.40593\n",
      "Epoch: 00 [26804/27712 ( 97%)], Train Loss: 0.40597\n",
      "Epoch: 00 [26844/27712 ( 97%)], Train Loss: 0.40597\n",
      "Epoch: 00 [26884/27712 ( 97%)], Train Loss: 0.40581\n",
      "Epoch: 00 [26924/27712 ( 97%)], Train Loss: 0.40584\n",
      "Epoch: 00 [26964/27712 ( 97%)], Train Loss: 0.40553\n",
      "Epoch: 00 [27004/27712 ( 97%)], Train Loss: 0.40526\n",
      "Epoch: 00 [27044/27712 ( 98%)], Train Loss: 0.40527\n",
      "Epoch: 00 [27084/27712 ( 98%)], Train Loss: 0.40498\n",
      "Epoch: 00 [27124/27712 ( 98%)], Train Loss: 0.40472\n",
      "Epoch: 00 [27164/27712 ( 98%)], Train Loss: 0.40451\n",
      "Epoch: 00 [27204/27712 ( 98%)], Train Loss: 0.40425\n",
      "Epoch: 00 [27244/27712 ( 98%)], Train Loss: 0.40388\n",
      "Epoch: 00 [27284/27712 ( 98%)], Train Loss: 0.40403\n",
      "Epoch: 00 [27324/27712 ( 99%)], Train Loss: 0.40371\n",
      "Epoch: 00 [27364/27712 ( 99%)], Train Loss: 0.40354\n",
      "Epoch: 00 [27404/27712 ( 99%)], Train Loss: 0.40350\n",
      "Epoch: 00 [27444/27712 ( 99%)], Train Loss: 0.40328\n",
      "Epoch: 00 [27484/27712 ( 99%)], Train Loss: 0.40336\n",
      "Epoch: 00 [27524/27712 ( 99%)], Train Loss: 0.40312\n",
      "Epoch: 00 [27564/27712 ( 99%)], Train Loss: 0.40282\n",
      "Epoch: 00 [27604/27712 (100%)], Train Loss: 0.40245\n",
      "Epoch: 00 [27644/27712 (100%)], Train Loss: 0.40219\n",
      "Epoch: 00 [27684/27712 (100%)], Train Loss: 0.40193\n",
      "Epoch: 00 [27712/27712 (100%)], Train Loss: 0.40178\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.58827\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.58827\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Total Training Time: 3222.7801706790924secs, Average Training Time per Epoch: 3222.7801706790924secs.\n",
      "Total Validation Time: 143.52550530433655secs, Average Validation Time per Epoch: 143.52550530433655secs.\n",
      "--------------------------------------------------\n",
      "FOLD: 1\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27795, Num examples Valid=3927\n",
      "Total Training Steps: 3475, Total Warmup Steps: 347\n",
      "Epoch: 00 [    4/27795 (  0%)], Train Loss: 2.75689\n",
      "Epoch: 00 [   44/27795 (  0%)], Train Loss: 2.79120\n",
      "Epoch: 00 [   84/27795 (  0%)], Train Loss: 2.77079\n",
      "Epoch: 00 [  124/27795 (  0%)], Train Loss: 2.75972\n",
      "Epoch: 00 [  164/27795 (  1%)], Train Loss: 2.73108\n",
      "Epoch: 00 [  204/27795 (  1%)], Train Loss: 2.70060\n",
      "Epoch: 00 [  244/27795 (  1%)], Train Loss: 2.65412\n",
      "Epoch: 00 [  284/27795 (  1%)], Train Loss: 2.60697\n",
      "Epoch: 00 [  324/27795 (  1%)], Train Loss: 2.54667\n",
      "Epoch: 00 [  364/27795 (  1%)], Train Loss: 2.48595\n",
      "Epoch: 00 [  404/27795 (  1%)], Train Loss: 2.41139\n",
      "Epoch: 00 [  444/27795 (  2%)], Train Loss: 2.32470\n",
      "Epoch: 00 [  484/27795 (  2%)], Train Loss: 2.23966\n",
      "Epoch: 00 [  524/27795 (  2%)], Train Loss: 2.15045\n",
      "Epoch: 00 [  564/27795 (  2%)], Train Loss: 2.05867\n",
      "Epoch: 00 [  604/27795 (  2%)], Train Loss: 1.97989\n",
      "Epoch: 00 [  644/27795 (  2%)], Train Loss: 1.89498\n",
      "Epoch: 00 [  684/27795 (  2%)], Train Loss: 1.82820\n",
      "Epoch: 00 [  724/27795 (  3%)], Train Loss: 1.76143\n",
      "Epoch: 00 [  764/27795 (  3%)], Train Loss: 1.70727\n",
      "Epoch: 00 [  804/27795 (  3%)], Train Loss: 1.65949\n",
      "Epoch: 00 [  844/27795 (  3%)], Train Loss: 1.61103\n",
      "Epoch: 00 [  884/27795 (  3%)], Train Loss: 1.56415\n",
      "Epoch: 00 [  924/27795 (  3%)], Train Loss: 1.52780\n",
      "Epoch: 00 [  964/27795 (  3%)], Train Loss: 1.48482\n",
      "Epoch: 00 [ 1004/27795 (  4%)], Train Loss: 1.44513\n",
      "Epoch: 00 [ 1044/27795 (  4%)], Train Loss: 1.41363\n",
      "Epoch: 00 [ 1084/27795 (  4%)], Train Loss: 1.38252\n",
      "Epoch: 00 [ 1124/27795 (  4%)], Train Loss: 1.34832\n",
      "Epoch: 00 [ 1164/27795 (  4%)], Train Loss: 1.31722\n",
      "Epoch: 00 [ 1204/27795 (  4%)], Train Loss: 1.28964\n",
      "Epoch: 00 [ 1244/27795 (  4%)], Train Loss: 1.26693\n",
      "Epoch: 00 [ 1284/27795 (  5%)], Train Loss: 1.25305\n",
      "Epoch: 00 [ 1324/27795 (  5%)], Train Loss: 1.23373\n",
      "Epoch: 00 [ 1364/27795 (  5%)], Train Loss: 1.21712\n",
      "Epoch: 00 [ 1404/27795 (  5%)], Train Loss: 1.19380\n",
      "Epoch: 00 [ 1444/27795 (  5%)], Train Loss: 1.17597\n",
      "Epoch: 00 [ 1484/27795 (  5%)], Train Loss: 1.15644\n",
      "Epoch: 00 [ 1524/27795 (  5%)], Train Loss: 1.14172\n",
      "Epoch: 00 [ 1564/27795 (  6%)], Train Loss: 1.12859\n",
      "Epoch: 00 [ 1604/27795 (  6%)], Train Loss: 1.10955\n",
      "Epoch: 00 [ 1644/27795 (  6%)], Train Loss: 1.09482\n",
      "Epoch: 00 [ 1684/27795 (  6%)], Train Loss: 1.08133\n",
      "Epoch: 00 [ 1724/27795 (  6%)], Train Loss: 1.07239\n",
      "Epoch: 00 [ 1764/27795 (  6%)], Train Loss: 1.06057\n",
      "Epoch: 00 [ 1804/27795 (  6%)], Train Loss: 1.05041\n",
      "Epoch: 00 [ 1844/27795 (  7%)], Train Loss: 1.03892\n",
      "Epoch: 00 [ 1884/27795 (  7%)], Train Loss: 1.02713\n",
      "Epoch: 00 [ 1924/27795 (  7%)], Train Loss: 1.01568\n",
      "Epoch: 00 [ 1964/27795 (  7%)], Train Loss: 1.00445\n",
      "Epoch: 00 [ 2004/27795 (  7%)], Train Loss: 0.99195\n",
      "Epoch: 00 [ 2044/27795 (  7%)], Train Loss: 0.98302\n",
      "Epoch: 00 [ 2084/27795 (  7%)], Train Loss: 0.97189\n",
      "Epoch: 00 [ 2124/27795 (  8%)], Train Loss: 0.96463\n",
      "Epoch: 00 [ 2164/27795 (  8%)], Train Loss: 0.95260\n",
      "Epoch: 00 [ 2204/27795 (  8%)], Train Loss: 0.94491\n",
      "Epoch: 00 [ 2244/27795 (  8%)], Train Loss: 0.93652\n",
      "Epoch: 00 [ 2284/27795 (  8%)], Train Loss: 0.92939\n",
      "Epoch: 00 [ 2324/27795 (  8%)], Train Loss: 0.92284\n",
      "Epoch: 00 [ 2364/27795 (  9%)], Train Loss: 0.91919\n",
      "Epoch: 00 [ 2404/27795 (  9%)], Train Loss: 0.91359\n",
      "Epoch: 00 [ 2444/27795 (  9%)], Train Loss: 0.90897\n",
      "Epoch: 00 [ 2484/27795 (  9%)], Train Loss: 0.90221\n",
      "Epoch: 00 [ 2524/27795 (  9%)], Train Loss: 0.89438\n",
      "Epoch: 00 [ 2564/27795 (  9%)], Train Loss: 0.88824\n",
      "Epoch: 00 [ 2604/27795 (  9%)], Train Loss: 0.88503\n",
      "Epoch: 00 [ 2644/27795 ( 10%)], Train Loss: 0.87645\n",
      "Epoch: 00 [ 2684/27795 ( 10%)], Train Loss: 0.86975\n",
      "Epoch: 00 [ 2724/27795 ( 10%)], Train Loss: 0.86242\n",
      "Epoch: 00 [ 2764/27795 ( 10%)], Train Loss: 0.85807\n",
      "Epoch: 00 [ 2804/27795 ( 10%)], Train Loss: 0.85223\n",
      "Epoch: 00 [ 2844/27795 ( 10%)], Train Loss: 0.84560\n",
      "Epoch: 00 [ 2884/27795 ( 10%)], Train Loss: 0.84278\n",
      "Epoch: 00 [ 2924/27795 ( 11%)], Train Loss: 0.84014\n",
      "Epoch: 00 [ 2964/27795 ( 11%)], Train Loss: 0.83554\n",
      "Epoch: 00 [ 3004/27795 ( 11%)], Train Loss: 0.83110\n",
      "Epoch: 00 [ 3044/27795 ( 11%)], Train Loss: 0.82428\n",
      "Epoch: 00 [ 3084/27795 ( 11%)], Train Loss: 0.82253\n",
      "Epoch: 00 [ 3124/27795 ( 11%)], Train Loss: 0.81841\n",
      "Epoch: 00 [ 3164/27795 ( 11%)], Train Loss: 0.81348\n",
      "Epoch: 00 [ 3204/27795 ( 12%)], Train Loss: 0.80968\n",
      "Epoch: 00 [ 3244/27795 ( 12%)], Train Loss: 0.80535\n",
      "Epoch: 00 [ 3284/27795 ( 12%)], Train Loss: 0.80342\n",
      "Epoch: 00 [ 3324/27795 ( 12%)], Train Loss: 0.79779\n",
      "Epoch: 00 [ 3364/27795 ( 12%)], Train Loss: 0.79232\n",
      "Epoch: 00 [ 3404/27795 ( 12%)], Train Loss: 0.78760\n",
      "Epoch: 00 [ 3444/27795 ( 12%)], Train Loss: 0.78279\n",
      "Epoch: 00 [ 3484/27795 ( 13%)], Train Loss: 0.77689\n",
      "Epoch: 00 [ 3524/27795 ( 13%)], Train Loss: 0.77204\n",
      "Epoch: 00 [ 3564/27795 ( 13%)], Train Loss: 0.76896\n",
      "Epoch: 00 [ 3604/27795 ( 13%)], Train Loss: 0.76480\n",
      "Epoch: 00 [ 3644/27795 ( 13%)], Train Loss: 0.76121\n",
      "Epoch: 00 [ 3684/27795 ( 13%)], Train Loss: 0.75698\n",
      "Epoch: 00 [ 3724/27795 ( 13%)], Train Loss: 0.75299\n",
      "Epoch: 00 [ 3764/27795 ( 14%)], Train Loss: 0.74806\n",
      "Epoch: 00 [ 3804/27795 ( 14%)], Train Loss: 0.74409\n",
      "Epoch: 00 [ 3844/27795 ( 14%)], Train Loss: 0.74136\n",
      "Epoch: 00 [ 3884/27795 ( 14%)], Train Loss: 0.73677\n",
      "Epoch: 00 [ 3924/27795 ( 14%)], Train Loss: 0.73226\n",
      "Epoch: 00 [ 3964/27795 ( 14%)], Train Loss: 0.73094\n",
      "Epoch: 00 [ 4004/27795 ( 14%)], Train Loss: 0.72761\n",
      "Epoch: 00 [ 4044/27795 ( 15%)], Train Loss: 0.72270\n",
      "Epoch: 00 [ 4084/27795 ( 15%)], Train Loss: 0.72163\n",
      "Epoch: 00 [ 4124/27795 ( 15%)], Train Loss: 0.72003\n",
      "Epoch: 00 [ 4164/27795 ( 15%)], Train Loss: 0.71735\n",
      "Epoch: 00 [ 4204/27795 ( 15%)], Train Loss: 0.71486\n",
      "Epoch: 00 [ 4244/27795 ( 15%)], Train Loss: 0.71317\n",
      "Epoch: 00 [ 4284/27795 ( 15%)], Train Loss: 0.71027\n",
      "Epoch: 00 [ 4324/27795 ( 16%)], Train Loss: 0.70733\n",
      "Epoch: 00 [ 4364/27795 ( 16%)], Train Loss: 0.70504\n",
      "Epoch: 00 [ 4404/27795 ( 16%)], Train Loss: 0.70226\n",
      "Epoch: 00 [ 4444/27795 ( 16%)], Train Loss: 0.70004\n",
      "Epoch: 00 [ 4484/27795 ( 16%)], Train Loss: 0.69710\n",
      "Epoch: 00 [ 4524/27795 ( 16%)], Train Loss: 0.69333\n",
      "Epoch: 00 [ 4564/27795 ( 16%)], Train Loss: 0.69263\n",
      "Epoch: 00 [ 4604/27795 ( 17%)], Train Loss: 0.68852\n",
      "Epoch: 00 [ 4644/27795 ( 17%)], Train Loss: 0.68505\n",
      "Epoch: 00 [ 4684/27795 ( 17%)], Train Loss: 0.68310\n",
      "Epoch: 00 [ 4724/27795 ( 17%)], Train Loss: 0.68058\n",
      "Epoch: 00 [ 4764/27795 ( 17%)], Train Loss: 0.67742\n",
      "Epoch: 00 [ 4804/27795 ( 17%)], Train Loss: 0.67609\n",
      "Epoch: 00 [ 4844/27795 ( 17%)], Train Loss: 0.67532\n",
      "Epoch: 00 [ 4884/27795 ( 18%)], Train Loss: 0.67260\n",
      "Epoch: 00 [ 4924/27795 ( 18%)], Train Loss: 0.67111\n",
      "Epoch: 00 [ 4964/27795 ( 18%)], Train Loss: 0.66952\n",
      "Epoch: 00 [ 5004/27795 ( 18%)], Train Loss: 0.66710\n",
      "Epoch: 00 [ 5044/27795 ( 18%)], Train Loss: 0.66430\n",
      "Epoch: 00 [ 5084/27795 ( 18%)], Train Loss: 0.66155\n",
      "Epoch: 00 [ 5124/27795 ( 18%)], Train Loss: 0.65955\n",
      "Epoch: 00 [ 5164/27795 ( 19%)], Train Loss: 0.65744\n",
      "Epoch: 00 [ 5204/27795 ( 19%)], Train Loss: 0.65531\n",
      "Epoch: 00 [ 5244/27795 ( 19%)], Train Loss: 0.65606\n",
      "Epoch: 00 [ 5284/27795 ( 19%)], Train Loss: 0.65546\n",
      "Epoch: 00 [ 5324/27795 ( 19%)], Train Loss: 0.65297\n",
      "Epoch: 00 [ 5364/27795 ( 19%)], Train Loss: 0.65026\n",
      "Epoch: 00 [ 5404/27795 ( 19%)], Train Loss: 0.64828\n",
      "Epoch: 00 [ 5444/27795 ( 20%)], Train Loss: 0.64638\n",
      "Epoch: 00 [ 5484/27795 ( 20%)], Train Loss: 0.64451\n",
      "Epoch: 00 [ 5524/27795 ( 20%)], Train Loss: 0.64206\n",
      "Epoch: 00 [ 5564/27795 ( 20%)], Train Loss: 0.64058\n",
      "Epoch: 00 [ 5604/27795 ( 20%)], Train Loss: 0.63893\n",
      "Epoch: 00 [ 5644/27795 ( 20%)], Train Loss: 0.63673\n",
      "Epoch: 00 [ 5684/27795 ( 20%)], Train Loss: 0.63495\n",
      "Epoch: 00 [ 5724/27795 ( 21%)], Train Loss: 0.63357\n",
      "Epoch: 00 [ 5764/27795 ( 21%)], Train Loss: 0.63215\n",
      "Epoch: 00 [ 5804/27795 ( 21%)], Train Loss: 0.63025\n",
      "Epoch: 00 [ 5844/27795 ( 21%)], Train Loss: 0.62928\n",
      "Epoch: 00 [ 5884/27795 ( 21%)], Train Loss: 0.62827\n",
      "Epoch: 00 [ 5924/27795 ( 21%)], Train Loss: 0.62635\n",
      "Epoch: 00 [ 5964/27795 ( 21%)], Train Loss: 0.62600\n",
      "Epoch: 00 [ 6004/27795 ( 22%)], Train Loss: 0.62462\n",
      "Epoch: 00 [ 6044/27795 ( 22%)], Train Loss: 0.62269\n",
      "Epoch: 00 [ 6084/27795 ( 22%)], Train Loss: 0.62086\n",
      "Epoch: 00 [ 6124/27795 ( 22%)], Train Loss: 0.62023\n",
      "Epoch: 00 [ 6164/27795 ( 22%)], Train Loss: 0.62005\n",
      "Epoch: 00 [ 6204/27795 ( 22%)], Train Loss: 0.61813\n",
      "Epoch: 00 [ 6244/27795 ( 22%)], Train Loss: 0.61678\n",
      "Epoch: 00 [ 6284/27795 ( 23%)], Train Loss: 0.61512\n",
      "Epoch: 00 [ 6324/27795 ( 23%)], Train Loss: 0.61326\n",
      "Epoch: 00 [ 6364/27795 ( 23%)], Train Loss: 0.61145\n",
      "Epoch: 00 [ 6404/27795 ( 23%)], Train Loss: 0.61073\n",
      "Epoch: 00 [ 6444/27795 ( 23%)], Train Loss: 0.60966\n",
      "Epoch: 00 [ 6484/27795 ( 23%)], Train Loss: 0.60897\n",
      "Epoch: 00 [ 6524/27795 ( 23%)], Train Loss: 0.60715\n",
      "Epoch: 00 [ 6564/27795 ( 24%)], Train Loss: 0.60744\n",
      "Epoch: 00 [ 6604/27795 ( 24%)], Train Loss: 0.60593\n",
      "Epoch: 00 [ 6644/27795 ( 24%)], Train Loss: 0.60470\n",
      "Epoch: 00 [ 6684/27795 ( 24%)], Train Loss: 0.60371\n",
      "Epoch: 00 [ 6724/27795 ( 24%)], Train Loss: 0.60308\n",
      "Epoch: 00 [ 6764/27795 ( 24%)], Train Loss: 0.60119\n",
      "Epoch: 00 [ 6804/27795 ( 24%)], Train Loss: 0.60092\n",
      "Epoch: 00 [ 6844/27795 ( 25%)], Train Loss: 0.59914\n",
      "Epoch: 00 [ 6884/27795 ( 25%)], Train Loss: 0.59816\n",
      "Epoch: 00 [ 6924/27795 ( 25%)], Train Loss: 0.59771\n",
      "Epoch: 00 [ 6964/27795 ( 25%)], Train Loss: 0.59658\n",
      "Epoch: 00 [ 7004/27795 ( 25%)], Train Loss: 0.59479\n",
      "Epoch: 00 [ 7044/27795 ( 25%)], Train Loss: 0.59354\n",
      "Epoch: 00 [ 7084/27795 ( 25%)], Train Loss: 0.59310\n",
      "Epoch: 00 [ 7124/27795 ( 26%)], Train Loss: 0.59154\n",
      "Epoch: 00 [ 7164/27795 ( 26%)], Train Loss: 0.58980\n",
      "Epoch: 00 [ 7204/27795 ( 26%)], Train Loss: 0.58823\n",
      "Epoch: 00 [ 7244/27795 ( 26%)], Train Loss: 0.58707\n",
      "Epoch: 00 [ 7284/27795 ( 26%)], Train Loss: 0.58639\n",
      "Epoch: 00 [ 7324/27795 ( 26%)], Train Loss: 0.58475\n",
      "Epoch: 00 [ 7364/27795 ( 26%)], Train Loss: 0.58384\n",
      "Epoch: 00 [ 7404/27795 ( 27%)], Train Loss: 0.58260\n",
      "Epoch: 00 [ 7444/27795 ( 27%)], Train Loss: 0.58145\n",
      "Epoch: 00 [ 7484/27795 ( 27%)], Train Loss: 0.58011\n",
      "Epoch: 00 [ 7524/27795 ( 27%)], Train Loss: 0.57828\n",
      "Epoch: 00 [ 7564/27795 ( 27%)], Train Loss: 0.57760\n",
      "Epoch: 00 [ 7604/27795 ( 27%)], Train Loss: 0.57671\n",
      "Epoch: 00 [ 7644/27795 ( 28%)], Train Loss: 0.57628\n",
      "Epoch: 00 [ 7684/27795 ( 28%)], Train Loss: 0.57545\n",
      "Epoch: 00 [ 7724/27795 ( 28%)], Train Loss: 0.57382\n",
      "Epoch: 00 [ 7764/27795 ( 28%)], Train Loss: 0.57270\n",
      "Epoch: 00 [ 7804/27795 ( 28%)], Train Loss: 0.57118\n",
      "Epoch: 00 [ 7844/27795 ( 28%)], Train Loss: 0.57076\n",
      "Epoch: 00 [ 7884/27795 ( 28%)], Train Loss: 0.57018\n",
      "Epoch: 00 [ 7924/27795 ( 29%)], Train Loss: 0.56887\n",
      "Epoch: 00 [ 7964/27795 ( 29%)], Train Loss: 0.56761\n",
      "Epoch: 00 [ 8004/27795 ( 29%)], Train Loss: 0.56626\n",
      "Epoch: 00 [ 8044/27795 ( 29%)], Train Loss: 0.56563\n",
      "Epoch: 00 [ 8084/27795 ( 29%)], Train Loss: 0.56457\n",
      "Epoch: 00 [ 8124/27795 ( 29%)], Train Loss: 0.56336\n",
      "Epoch: 00 [ 8164/27795 ( 29%)], Train Loss: 0.56275\n",
      "Epoch: 00 [ 8204/27795 ( 30%)], Train Loss: 0.56142\n",
      "Epoch: 00 [ 8244/27795 ( 30%)], Train Loss: 0.56063\n",
      "Epoch: 00 [ 8284/27795 ( 30%)], Train Loss: 0.56006\n",
      "Epoch: 00 [ 8324/27795 ( 30%)], Train Loss: 0.55970\n",
      "Epoch: 00 [ 8364/27795 ( 30%)], Train Loss: 0.55966\n",
      "Epoch: 00 [ 8404/27795 ( 30%)], Train Loss: 0.55869\n",
      "Epoch: 00 [ 8444/27795 ( 30%)], Train Loss: 0.55740\n",
      "Epoch: 00 [ 8484/27795 ( 31%)], Train Loss: 0.55668\n",
      "Epoch: 00 [ 8524/27795 ( 31%)], Train Loss: 0.55578\n",
      "Epoch: 00 [ 8564/27795 ( 31%)], Train Loss: 0.55528\n",
      "Epoch: 00 [ 8604/27795 ( 31%)], Train Loss: 0.55476\n",
      "Epoch: 00 [ 8644/27795 ( 31%)], Train Loss: 0.55352\n",
      "Epoch: 00 [ 8684/27795 ( 31%)], Train Loss: 0.55238\n",
      "Epoch: 00 [ 8724/27795 ( 31%)], Train Loss: 0.55107\n",
      "Epoch: 00 [ 8764/27795 ( 32%)], Train Loss: 0.55105\n",
      "Epoch: 00 [ 8804/27795 ( 32%)], Train Loss: 0.55069\n",
      "Epoch: 00 [ 8844/27795 ( 32%)], Train Loss: 0.54964\n",
      "Epoch: 00 [ 8884/27795 ( 32%)], Train Loss: 0.55038\n",
      "Epoch: 00 [ 8924/27795 ( 32%)], Train Loss: 0.55004\n",
      "Epoch: 00 [ 8964/27795 ( 32%)], Train Loss: 0.54988\n",
      "Epoch: 00 [ 9004/27795 ( 32%)], Train Loss: 0.54978\n",
      "Epoch: 00 [ 9044/27795 ( 33%)], Train Loss: 0.54828\n",
      "Epoch: 00 [ 9084/27795 ( 33%)], Train Loss: 0.54821\n",
      "Epoch: 00 [ 9124/27795 ( 33%)], Train Loss: 0.54773\n",
      "Epoch: 00 [ 9164/27795 ( 33%)], Train Loss: 0.54668\n",
      "Epoch: 00 [ 9204/27795 ( 33%)], Train Loss: 0.54610\n",
      "Epoch: 00 [ 9244/27795 ( 33%)], Train Loss: 0.54540\n",
      "Epoch: 00 [ 9284/27795 ( 33%)], Train Loss: 0.54509\n",
      "Epoch: 00 [ 9324/27795 ( 34%)], Train Loss: 0.54413\n",
      "Epoch: 00 [ 9364/27795 ( 34%)], Train Loss: 0.54399\n",
      "Epoch: 00 [ 9404/27795 ( 34%)], Train Loss: 0.54308\n",
      "Epoch: 00 [ 9444/27795 ( 34%)], Train Loss: 0.54333\n",
      "Epoch: 00 [ 9484/27795 ( 34%)], Train Loss: 0.54325\n",
      "Epoch: 00 [ 9524/27795 ( 34%)], Train Loss: 0.54226\n",
      "Epoch: 00 [ 9564/27795 ( 34%)], Train Loss: 0.54171\n",
      "Epoch: 00 [ 9604/27795 ( 35%)], Train Loss: 0.54061\n",
      "Epoch: 00 [ 9644/27795 ( 35%)], Train Loss: 0.53959\n",
      "Epoch: 00 [ 9684/27795 ( 35%)], Train Loss: 0.53893\n",
      "Epoch: 00 [ 9724/27795 ( 35%)], Train Loss: 0.53839\n",
      "Epoch: 00 [ 9764/27795 ( 35%)], Train Loss: 0.53740\n",
      "Epoch: 00 [ 9804/27795 ( 35%)], Train Loss: 0.53669\n",
      "Epoch: 00 [ 9844/27795 ( 35%)], Train Loss: 0.53614\n",
      "Epoch: 00 [ 9884/27795 ( 36%)], Train Loss: 0.53540\n",
      "Epoch: 00 [ 9924/27795 ( 36%)], Train Loss: 0.53461\n",
      "Epoch: 00 [ 9964/27795 ( 36%)], Train Loss: 0.53378\n",
      "Epoch: 00 [10004/27795 ( 36%)], Train Loss: 0.53323\n",
      "Epoch: 00 [10044/27795 ( 36%)], Train Loss: 0.53229\n",
      "Epoch: 00 [10084/27795 ( 36%)], Train Loss: 0.53125\n",
      "Epoch: 00 [10124/27795 ( 36%)], Train Loss: 0.53074\n",
      "Epoch: 00 [10164/27795 ( 37%)], Train Loss: 0.52976\n",
      "Epoch: 00 [10204/27795 ( 37%)], Train Loss: 0.52937\n",
      "Epoch: 00 [10244/27795 ( 37%)], Train Loss: 0.52878\n",
      "Epoch: 00 [10284/27795 ( 37%)], Train Loss: 0.52786\n",
      "Epoch: 00 [10324/27795 ( 37%)], Train Loss: 0.52766\n",
      "Epoch: 00 [10364/27795 ( 37%)], Train Loss: 0.52706\n",
      "Epoch: 00 [10404/27795 ( 37%)], Train Loss: 0.52651\n",
      "Epoch: 00 [10444/27795 ( 38%)], Train Loss: 0.52599\n",
      "Epoch: 00 [10484/27795 ( 38%)], Train Loss: 0.52525\n",
      "Epoch: 00 [10524/27795 ( 38%)], Train Loss: 0.52544\n",
      "Epoch: 00 [10564/27795 ( 38%)], Train Loss: 0.52500\n",
      "Epoch: 00 [10604/27795 ( 38%)], Train Loss: 0.52432\n",
      "Epoch: 00 [10644/27795 ( 38%)], Train Loss: 0.52369\n",
      "Epoch: 00 [10684/27795 ( 38%)], Train Loss: 0.52316\n",
      "Epoch: 00 [10724/27795 ( 39%)], Train Loss: 0.52290\n",
      "Epoch: 00 [10764/27795 ( 39%)], Train Loss: 0.52294\n",
      "Epoch: 00 [10804/27795 ( 39%)], Train Loss: 0.52277\n",
      "Epoch: 00 [10844/27795 ( 39%)], Train Loss: 0.52279\n",
      "Epoch: 00 [10884/27795 ( 39%)], Train Loss: 0.52194\n",
      "Epoch: 00 [10924/27795 ( 39%)], Train Loss: 0.52119\n",
      "Epoch: 00 [10964/27795 ( 39%)], Train Loss: 0.52051\n",
      "Epoch: 00 [11004/27795 ( 40%)], Train Loss: 0.52016\n",
      "Epoch: 00 [11044/27795 ( 40%)], Train Loss: 0.52020\n",
      "Epoch: 00 [11084/27795 ( 40%)], Train Loss: 0.51987\n",
      "Epoch: 00 [11124/27795 ( 40%)], Train Loss: 0.51909\n",
      "Epoch: 00 [11164/27795 ( 40%)], Train Loss: 0.51854\n",
      "Epoch: 00 [11204/27795 ( 40%)], Train Loss: 0.51800\n",
      "Epoch: 00 [11244/27795 ( 40%)], Train Loss: 0.51728\n",
      "Epoch: 00 [11284/27795 ( 41%)], Train Loss: 0.51664\n",
      "Epoch: 00 [11324/27795 ( 41%)], Train Loss: 0.51618\n",
      "Epoch: 00 [11364/27795 ( 41%)], Train Loss: 0.51518\n",
      "Epoch: 00 [11404/27795 ( 41%)], Train Loss: 0.51476\n",
      "Epoch: 00 [11444/27795 ( 41%)], Train Loss: 0.51453\n",
      "Epoch: 00 [11484/27795 ( 41%)], Train Loss: 0.51379\n",
      "Epoch: 00 [11524/27795 ( 41%)], Train Loss: 0.51289\n",
      "Epoch: 00 [11564/27795 ( 42%)], Train Loss: 0.51233\n",
      "Epoch: 00 [11604/27795 ( 42%)], Train Loss: 0.51195\n",
      "Epoch: 00 [11644/27795 ( 42%)], Train Loss: 0.51141\n",
      "Epoch: 00 [11684/27795 ( 42%)], Train Loss: 0.51078\n",
      "Epoch: 00 [11724/27795 ( 42%)], Train Loss: 0.50993\n",
      "Epoch: 00 [11764/27795 ( 42%)], Train Loss: 0.50948\n",
      "Epoch: 00 [11804/27795 ( 42%)], Train Loss: 0.50894\n",
      "Epoch: 00 [11844/27795 ( 43%)], Train Loss: 0.50838\n",
      "Epoch: 00 [11884/27795 ( 43%)], Train Loss: 0.50780\n",
      "Epoch: 00 [11924/27795 ( 43%)], Train Loss: 0.50707\n",
      "Epoch: 00 [11964/27795 ( 43%)], Train Loss: 0.50697\n",
      "Epoch: 00 [12004/27795 ( 43%)], Train Loss: 0.50665\n",
      "Epoch: 00 [12044/27795 ( 43%)], Train Loss: 0.50616\n",
      "Epoch: 00 [12084/27795 ( 43%)], Train Loss: 0.50560\n",
      "Epoch: 00 [12124/27795 ( 44%)], Train Loss: 0.50473\n",
      "Epoch: 00 [12164/27795 ( 44%)], Train Loss: 0.50418\n",
      "Epoch: 00 [12204/27795 ( 44%)], Train Loss: 0.50398\n",
      "Epoch: 00 [12244/27795 ( 44%)], Train Loss: 0.50362\n",
      "Epoch: 00 [12284/27795 ( 44%)], Train Loss: 0.50285\n",
      "Epoch: 00 [12324/27795 ( 44%)], Train Loss: 0.50248\n",
      "Epoch: 00 [12364/27795 ( 44%)], Train Loss: 0.50225\n",
      "Epoch: 00 [12404/27795 ( 45%)], Train Loss: 0.50148\n",
      "Epoch: 00 [12444/27795 ( 45%)], Train Loss: 0.50099\n",
      "Epoch: 00 [12484/27795 ( 45%)], Train Loss: 0.50037\n",
      "Epoch: 00 [12524/27795 ( 45%)], Train Loss: 0.50041\n",
      "Epoch: 00 [12564/27795 ( 45%)], Train Loss: 0.49972\n",
      "Epoch: 00 [12604/27795 ( 45%)], Train Loss: 0.49994\n",
      "Epoch: 00 [12644/27795 ( 45%)], Train Loss: 0.50015\n",
      "Epoch: 00 [12684/27795 ( 46%)], Train Loss: 0.50023\n",
      "Epoch: 00 [12724/27795 ( 46%)], Train Loss: 0.50016\n",
      "Epoch: 00 [12764/27795 ( 46%)], Train Loss: 0.49969\n",
      "Epoch: 00 [12804/27795 ( 46%)], Train Loss: 0.49915\n",
      "Epoch: 00 [12844/27795 ( 46%)], Train Loss: 0.49843\n",
      "Epoch: 00 [12884/27795 ( 46%)], Train Loss: 0.49784\n",
      "Epoch: 00 [12924/27795 ( 46%)], Train Loss: 0.49748\n",
      "Epoch: 00 [12964/27795 ( 47%)], Train Loss: 0.49694\n",
      "Epoch: 00 [13004/27795 ( 47%)], Train Loss: 0.49618\n",
      "Epoch: 00 [13044/27795 ( 47%)], Train Loss: 0.49556\n",
      "Epoch: 00 [13084/27795 ( 47%)], Train Loss: 0.49490\n",
      "Epoch: 00 [13124/27795 ( 47%)], Train Loss: 0.49428\n",
      "Epoch: 00 [13164/27795 ( 47%)], Train Loss: 0.49395\n",
      "Epoch: 00 [13204/27795 ( 48%)], Train Loss: 0.49346\n",
      "Epoch: 00 [13244/27795 ( 48%)], Train Loss: 0.49301\n",
      "Epoch: 00 [13284/27795 ( 48%)], Train Loss: 0.49270\n",
      "Epoch: 00 [13324/27795 ( 48%)], Train Loss: 0.49250\n",
      "Epoch: 00 [13364/27795 ( 48%)], Train Loss: 0.49200\n",
      "Epoch: 00 [13404/27795 ( 48%)], Train Loss: 0.49215\n",
      "Epoch: 00 [13444/27795 ( 48%)], Train Loss: 0.49150\n",
      "Epoch: 00 [13484/27795 ( 49%)], Train Loss: 0.49145\n",
      "Epoch: 00 [13524/27795 ( 49%)], Train Loss: 0.49103\n",
      "Epoch: 00 [13564/27795 ( 49%)], Train Loss: 0.49079\n",
      "Epoch: 00 [13604/27795 ( 49%)], Train Loss: 0.49020\n",
      "Epoch: 00 [13644/27795 ( 49%)], Train Loss: 0.48950\n",
      "Epoch: 00 [13684/27795 ( 49%)], Train Loss: 0.48905\n",
      "Epoch: 00 [13724/27795 ( 49%)], Train Loss: 0.48849\n",
      "Epoch: 00 [13764/27795 ( 50%)], Train Loss: 0.48865\n",
      "Epoch: 00 [13804/27795 ( 50%)], Train Loss: 0.48806\n",
      "Epoch: 00 [13844/27795 ( 50%)], Train Loss: 0.48743\n",
      "Epoch: 00 [13884/27795 ( 50%)], Train Loss: 0.48727\n",
      "Epoch: 00 [13924/27795 ( 50%)], Train Loss: 0.48689\n",
      "Epoch: 00 [13964/27795 ( 50%)], Train Loss: 0.48632\n",
      "Epoch: 00 [14004/27795 ( 50%)], Train Loss: 0.48642\n",
      "Epoch: 00 [14044/27795 ( 51%)], Train Loss: 0.48630\n",
      "Epoch: 00 [14084/27795 ( 51%)], Train Loss: 0.48596\n",
      "Epoch: 00 [14124/27795 ( 51%)], Train Loss: 0.48519\n",
      "Epoch: 00 [14164/27795 ( 51%)], Train Loss: 0.48493\n",
      "Epoch: 00 [14204/27795 ( 51%)], Train Loss: 0.48459\n",
      "Epoch: 00 [14244/27795 ( 51%)], Train Loss: 0.48423\n",
      "Epoch: 00 [14284/27795 ( 51%)], Train Loss: 0.48375\n",
      "Epoch: 00 [14324/27795 ( 52%)], Train Loss: 0.48299\n",
      "Epoch: 00 [14364/27795 ( 52%)], Train Loss: 0.48298\n",
      "Epoch: 00 [14404/27795 ( 52%)], Train Loss: 0.48282\n",
      "Epoch: 00 [14444/27795 ( 52%)], Train Loss: 0.48253\n",
      "Epoch: 00 [14484/27795 ( 52%)], Train Loss: 0.48201\n",
      "Epoch: 00 [14524/27795 ( 52%)], Train Loss: 0.48164\n",
      "Epoch: 00 [14564/27795 ( 52%)], Train Loss: 0.48168\n",
      "Epoch: 00 [14604/27795 ( 53%)], Train Loss: 0.48112\n",
      "Epoch: 00 [14644/27795 ( 53%)], Train Loss: 0.48109\n",
      "Epoch: 00 [14684/27795 ( 53%)], Train Loss: 0.48036\n",
      "Epoch: 00 [14724/27795 ( 53%)], Train Loss: 0.47975\n",
      "Epoch: 00 [14764/27795 ( 53%)], Train Loss: 0.47979\n",
      "Epoch: 00 [14804/27795 ( 53%)], Train Loss: 0.47899\n",
      "Epoch: 00 [14844/27795 ( 53%)], Train Loss: 0.47871\n",
      "Epoch: 00 [14884/27795 ( 54%)], Train Loss: 0.47806\n",
      "Epoch: 00 [14924/27795 ( 54%)], Train Loss: 0.47752\n",
      "Epoch: 00 [14964/27795 ( 54%)], Train Loss: 0.47691\n",
      "Epoch: 00 [15004/27795 ( 54%)], Train Loss: 0.47638\n",
      "Epoch: 00 [15044/27795 ( 54%)], Train Loss: 0.47612\n",
      "Epoch: 00 [15084/27795 ( 54%)], Train Loss: 0.47551\n",
      "Epoch: 00 [15124/27795 ( 54%)], Train Loss: 0.47510\n",
      "Epoch: 00 [15164/27795 ( 55%)], Train Loss: 0.47478\n",
      "Epoch: 00 [15204/27795 ( 55%)], Train Loss: 0.47428\n",
      "Epoch: 00 [15244/27795 ( 55%)], Train Loss: 0.47418\n",
      "Epoch: 00 [15284/27795 ( 55%)], Train Loss: 0.47383\n",
      "Epoch: 00 [15324/27795 ( 55%)], Train Loss: 0.47357\n",
      "Epoch: 00 [15364/27795 ( 55%)], Train Loss: 0.47340\n",
      "Epoch: 00 [15404/27795 ( 55%)], Train Loss: 0.47296\n",
      "Epoch: 00 [15444/27795 ( 56%)], Train Loss: 0.47269\n",
      "Epoch: 00 [15484/27795 ( 56%)], Train Loss: 0.47228\n",
      "Epoch: 00 [15524/27795 ( 56%)], Train Loss: 0.47189\n",
      "Epoch: 00 [15564/27795 ( 56%)], Train Loss: 0.47132\n",
      "Epoch: 00 [15604/27795 ( 56%)], Train Loss: 0.47161\n",
      "Epoch: 00 [15644/27795 ( 56%)], Train Loss: 0.47166\n",
      "Epoch: 00 [15684/27795 ( 56%)], Train Loss: 0.47121\n",
      "Epoch: 00 [15724/27795 ( 57%)], Train Loss: 0.47058\n",
      "Epoch: 00 [15764/27795 ( 57%)], Train Loss: 0.47028\n",
      "Epoch: 00 [15804/27795 ( 57%)], Train Loss: 0.47011\n",
      "Epoch: 00 [15844/27795 ( 57%)], Train Loss: 0.46984\n",
      "Epoch: 00 [15884/27795 ( 57%)], Train Loss: 0.46942\n",
      "Epoch: 00 [15924/27795 ( 57%)], Train Loss: 0.46888\n",
      "Epoch: 00 [15964/27795 ( 57%)], Train Loss: 0.46847\n",
      "Epoch: 00 [16004/27795 ( 58%)], Train Loss: 0.46778\n",
      "Epoch: 00 [16044/27795 ( 58%)], Train Loss: 0.46744\n",
      "Epoch: 00 [16084/27795 ( 58%)], Train Loss: 0.46724\n",
      "Epoch: 00 [16124/27795 ( 58%)], Train Loss: 0.46699\n",
      "Epoch: 00 [16164/27795 ( 58%)], Train Loss: 0.46664\n",
      "Epoch: 00 [16204/27795 ( 58%)], Train Loss: 0.46682\n",
      "Epoch: 00 [16244/27795 ( 58%)], Train Loss: 0.46629\n",
      "Epoch: 00 [16284/27795 ( 59%)], Train Loss: 0.46592\n",
      "Epoch: 00 [16324/27795 ( 59%)], Train Loss: 0.46545\n",
      "Epoch: 00 [16364/27795 ( 59%)], Train Loss: 0.46510\n",
      "Epoch: 00 [16404/27795 ( 59%)], Train Loss: 0.46472\n",
      "Epoch: 00 [16444/27795 ( 59%)], Train Loss: 0.46489\n",
      "Epoch: 00 [16484/27795 ( 59%)], Train Loss: 0.46464\n",
      "Epoch: 00 [16524/27795 ( 59%)], Train Loss: 0.46433\n",
      "Epoch: 00 [16564/27795 ( 60%)], Train Loss: 0.46396\n",
      "Epoch: 00 [16604/27795 ( 60%)], Train Loss: 0.46349\n",
      "Epoch: 00 [16644/27795 ( 60%)], Train Loss: 0.46308\n",
      "Epoch: 00 [16684/27795 ( 60%)], Train Loss: 0.46272\n",
      "Epoch: 00 [16724/27795 ( 60%)], Train Loss: 0.46244\n",
      "Epoch: 00 [16764/27795 ( 60%)], Train Loss: 0.46198\n",
      "Epoch: 00 [16804/27795 ( 60%)], Train Loss: 0.46172\n",
      "Epoch: 00 [16844/27795 ( 61%)], Train Loss: 0.46131\n",
      "Epoch: 00 [16884/27795 ( 61%)], Train Loss: 0.46090\n",
      "Epoch: 00 [16924/27795 ( 61%)], Train Loss: 0.46067\n",
      "Epoch: 00 [16964/27795 ( 61%)], Train Loss: 0.46013\n",
      "Epoch: 00 [17004/27795 ( 61%)], Train Loss: 0.46034\n",
      "Epoch: 00 [17044/27795 ( 61%)], Train Loss: 0.45999\n",
      "Epoch: 00 [17084/27795 ( 61%)], Train Loss: 0.45961\n",
      "Epoch: 00 [17124/27795 ( 62%)], Train Loss: 0.45949\n",
      "Epoch: 00 [17164/27795 ( 62%)], Train Loss: 0.45915\n",
      "Epoch: 00 [17204/27795 ( 62%)], Train Loss: 0.45898\n",
      "Epoch: 00 [17244/27795 ( 62%)], Train Loss: 0.45846\n",
      "Epoch: 00 [17284/27795 ( 62%)], Train Loss: 0.45810\n",
      "Epoch: 00 [17324/27795 ( 62%)], Train Loss: 0.45783\n",
      "Epoch: 00 [17364/27795 ( 62%)], Train Loss: 0.45777\n",
      "Epoch: 00 [17404/27795 ( 63%)], Train Loss: 0.45719\n",
      "Epoch: 00 [17444/27795 ( 63%)], Train Loss: 0.45675\n",
      "Epoch: 00 [17484/27795 ( 63%)], Train Loss: 0.45621\n",
      "Epoch: 00 [17524/27795 ( 63%)], Train Loss: 0.45569\n",
      "Epoch: 00 [17564/27795 ( 63%)], Train Loss: 0.45526\n",
      "Epoch: 00 [17604/27795 ( 63%)], Train Loss: 0.45489\n",
      "Epoch: 00 [17644/27795 ( 63%)], Train Loss: 0.45469\n",
      "Epoch: 00 [17684/27795 ( 64%)], Train Loss: 0.45451\n",
      "Epoch: 00 [17724/27795 ( 64%)], Train Loss: 0.45424\n",
      "Epoch: 00 [17764/27795 ( 64%)], Train Loss: 0.45382\n",
      "Epoch: 00 [17804/27795 ( 64%)], Train Loss: 0.45343\n",
      "Epoch: 00 [17844/27795 ( 64%)], Train Loss: 0.45311\n",
      "Epoch: 00 [17884/27795 ( 64%)], Train Loss: 0.45251\n",
      "Epoch: 00 [17924/27795 ( 64%)], Train Loss: 0.45198\n",
      "Epoch: 00 [17964/27795 ( 65%)], Train Loss: 0.45191\n",
      "Epoch: 00 [18004/27795 ( 65%)], Train Loss: 0.45160\n",
      "Epoch: 00 [18044/27795 ( 65%)], Train Loss: 0.45128\n",
      "Epoch: 00 [18084/27795 ( 65%)], Train Loss: 0.45070\n",
      "Epoch: 00 [18124/27795 ( 65%)], Train Loss: 0.45033\n",
      "Epoch: 00 [18164/27795 ( 65%)], Train Loss: 0.44970\n",
      "Epoch: 00 [18204/27795 ( 65%)], Train Loss: 0.44930\n",
      "Epoch: 00 [18244/27795 ( 66%)], Train Loss: 0.44916\n",
      "Epoch: 00 [18284/27795 ( 66%)], Train Loss: 0.44881\n",
      "Epoch: 00 [18324/27795 ( 66%)], Train Loss: 0.44853\n",
      "Epoch: 00 [18364/27795 ( 66%)], Train Loss: 0.44815\n",
      "Epoch: 00 [18404/27795 ( 66%)], Train Loss: 0.44773\n",
      "Epoch: 00 [18444/27795 ( 66%)], Train Loss: 0.44767\n",
      "Epoch: 00 [18484/27795 ( 67%)], Train Loss: 0.44717\n",
      "Epoch: 00 [18524/27795 ( 67%)], Train Loss: 0.44721\n",
      "Epoch: 00 [18564/27795 ( 67%)], Train Loss: 0.44682\n",
      "Epoch: 00 [18604/27795 ( 67%)], Train Loss: 0.44629\n",
      "Epoch: 00 [18644/27795 ( 67%)], Train Loss: 0.44598\n",
      "Epoch: 00 [18684/27795 ( 67%)], Train Loss: 0.44573\n",
      "Epoch: 00 [18724/27795 ( 67%)], Train Loss: 0.44572\n",
      "Epoch: 00 [18764/27795 ( 68%)], Train Loss: 0.44554\n",
      "Epoch: 00 [18804/27795 ( 68%)], Train Loss: 0.44539\n",
      "Epoch: 00 [18844/27795 ( 68%)], Train Loss: 0.44521\n",
      "Epoch: 00 [18884/27795 ( 68%)], Train Loss: 0.44498\n",
      "Epoch: 00 [18924/27795 ( 68%)], Train Loss: 0.44476\n",
      "Epoch: 00 [18964/27795 ( 68%)], Train Loss: 0.44434\n",
      "Epoch: 00 [19004/27795 ( 68%)], Train Loss: 0.44434\n",
      "Epoch: 00 [19044/27795 ( 69%)], Train Loss: 0.44458\n",
      "Epoch: 00 [19084/27795 ( 69%)], Train Loss: 0.44418\n",
      "Epoch: 00 [19124/27795 ( 69%)], Train Loss: 0.44377\n",
      "Epoch: 00 [19164/27795 ( 69%)], Train Loss: 0.44366\n",
      "Epoch: 00 [19204/27795 ( 69%)], Train Loss: 0.44370\n",
      "Epoch: 00 [19244/27795 ( 69%)], Train Loss: 0.44346\n",
      "Epoch: 00 [19284/27795 ( 69%)], Train Loss: 0.44355\n",
      "Epoch: 00 [19324/27795 ( 70%)], Train Loss: 0.44314\n",
      "Epoch: 00 [19364/27795 ( 70%)], Train Loss: 0.44288\n",
      "Epoch: 00 [19404/27795 ( 70%)], Train Loss: 0.44243\n",
      "Epoch: 00 [19444/27795 ( 70%)], Train Loss: 0.44223\n",
      "Epoch: 00 [19484/27795 ( 70%)], Train Loss: 0.44181\n",
      "Epoch: 00 [19524/27795 ( 70%)], Train Loss: 0.44171\n",
      "Epoch: 00 [19564/27795 ( 70%)], Train Loss: 0.44130\n",
      "Epoch: 00 [19604/27795 ( 71%)], Train Loss: 0.44135\n",
      "Epoch: 00 [19644/27795 ( 71%)], Train Loss: 0.44108\n",
      "Epoch: 00 [19684/27795 ( 71%)], Train Loss: 0.44076\n",
      "Epoch: 00 [19724/27795 ( 71%)], Train Loss: 0.44040\n",
      "Epoch: 00 [19764/27795 ( 71%)], Train Loss: 0.44029\n",
      "Epoch: 00 [19804/27795 ( 71%)], Train Loss: 0.43991\n",
      "Epoch: 00 [19844/27795 ( 71%)], Train Loss: 0.43964\n",
      "Epoch: 00 [19884/27795 ( 72%)], Train Loss: 0.43909\n",
      "Epoch: 00 [19924/27795 ( 72%)], Train Loss: 0.43869\n",
      "Epoch: 00 [19964/27795 ( 72%)], Train Loss: 0.43830\n",
      "Epoch: 00 [20004/27795 ( 72%)], Train Loss: 0.43826\n",
      "Epoch: 00 [20044/27795 ( 72%)], Train Loss: 0.43813\n",
      "Epoch: 00 [20084/27795 ( 72%)], Train Loss: 0.43773\n",
      "Epoch: 00 [20124/27795 ( 72%)], Train Loss: 0.43749\n",
      "Epoch: 00 [20164/27795 ( 73%)], Train Loss: 0.43723\n",
      "Epoch: 00 [20204/27795 ( 73%)], Train Loss: 0.43676\n",
      "Epoch: 00 [20244/27795 ( 73%)], Train Loss: 0.43627\n",
      "Epoch: 00 [20284/27795 ( 73%)], Train Loss: 0.43604\n",
      "Epoch: 00 [20324/27795 ( 73%)], Train Loss: 0.43585\n",
      "Epoch: 00 [20364/27795 ( 73%)], Train Loss: 0.43560\n",
      "Epoch: 00 [20404/27795 ( 73%)], Train Loss: 0.43531\n",
      "Epoch: 00 [20444/27795 ( 74%)], Train Loss: 0.43477\n",
      "Epoch: 00 [20484/27795 ( 74%)], Train Loss: 0.43449\n",
      "Epoch: 00 [20524/27795 ( 74%)], Train Loss: 0.43396\n",
      "Epoch: 00 [20564/27795 ( 74%)], Train Loss: 0.43353\n",
      "Epoch: 00 [20604/27795 ( 74%)], Train Loss: 0.43396\n",
      "Epoch: 00 [20644/27795 ( 74%)], Train Loss: 0.43385\n",
      "Epoch: 00 [20684/27795 ( 74%)], Train Loss: 0.43367\n",
      "Epoch: 00 [20724/27795 ( 75%)], Train Loss: 0.43338\n",
      "Epoch: 00 [20764/27795 ( 75%)], Train Loss: 0.43316\n",
      "Epoch: 00 [20804/27795 ( 75%)], Train Loss: 0.43295\n",
      "Epoch: 00 [20844/27795 ( 75%)], Train Loss: 0.43267\n",
      "Epoch: 00 [20884/27795 ( 75%)], Train Loss: 0.43222\n",
      "Epoch: 00 [20924/27795 ( 75%)], Train Loss: 0.43190\n",
      "Epoch: 00 [20964/27795 ( 75%)], Train Loss: 0.43160\n",
      "Epoch: 00 [21004/27795 ( 76%)], Train Loss: 0.43136\n",
      "Epoch: 00 [21044/27795 ( 76%)], Train Loss: 0.43124\n",
      "Epoch: 00 [21084/27795 ( 76%)], Train Loss: 0.43093\n",
      "Epoch: 00 [21124/27795 ( 76%)], Train Loss: 0.43067\n",
      "Epoch: 00 [21164/27795 ( 76%)], Train Loss: 0.43040\n",
      "Epoch: 00 [21204/27795 ( 76%)], Train Loss: 0.43031\n",
      "Epoch: 00 [21244/27795 ( 76%)], Train Loss: 0.43001\n",
      "Epoch: 00 [21284/27795 ( 77%)], Train Loss: 0.42965\n",
      "Epoch: 00 [21324/27795 ( 77%)], Train Loss: 0.42969\n",
      "Epoch: 00 [21364/27795 ( 77%)], Train Loss: 0.42962\n",
      "Epoch: 00 [21404/27795 ( 77%)], Train Loss: 0.42912\n",
      "Epoch: 00 [21444/27795 ( 77%)], Train Loss: 0.42897\n",
      "Epoch: 00 [21484/27795 ( 77%)], Train Loss: 0.42892\n",
      "Epoch: 00 [21524/27795 ( 77%)], Train Loss: 0.42872\n",
      "Epoch: 00 [21564/27795 ( 78%)], Train Loss: 0.42852\n",
      "Epoch: 00 [21604/27795 ( 78%)], Train Loss: 0.42834\n",
      "Epoch: 00 [21644/27795 ( 78%)], Train Loss: 0.42796\n",
      "Epoch: 00 [21684/27795 ( 78%)], Train Loss: 0.42775\n",
      "Epoch: 00 [21724/27795 ( 78%)], Train Loss: 0.42758\n",
      "Epoch: 00 [21764/27795 ( 78%)], Train Loss: 0.42737\n",
      "Epoch: 00 [21804/27795 ( 78%)], Train Loss: 0.42703\n",
      "Epoch: 00 [21844/27795 ( 79%)], Train Loss: 0.42687\n",
      "Epoch: 00 [21884/27795 ( 79%)], Train Loss: 0.42646\n",
      "Epoch: 00 [21924/27795 ( 79%)], Train Loss: 0.42623\n",
      "Epoch: 00 [21964/27795 ( 79%)], Train Loss: 0.42600\n",
      "Epoch: 00 [22004/27795 ( 79%)], Train Loss: 0.42563\n",
      "Epoch: 00 [22044/27795 ( 79%)], Train Loss: 0.42515\n",
      "Epoch: 00 [22084/27795 ( 79%)], Train Loss: 0.42485\n",
      "Epoch: 00 [22124/27795 ( 80%)], Train Loss: 0.42524\n",
      "Epoch: 00 [22164/27795 ( 80%)], Train Loss: 0.42500\n",
      "Epoch: 00 [22204/27795 ( 80%)], Train Loss: 0.42472\n",
      "Epoch: 00 [22244/27795 ( 80%)], Train Loss: 0.42486\n",
      "Epoch: 00 [22284/27795 ( 80%)], Train Loss: 0.42482\n",
      "Epoch: 00 [22324/27795 ( 80%)], Train Loss: 0.42480\n",
      "Epoch: 00 [22364/27795 ( 80%)], Train Loss: 0.42457\n",
      "Epoch: 00 [22404/27795 ( 81%)], Train Loss: 0.42419\n",
      "Epoch: 00 [22444/27795 ( 81%)], Train Loss: 0.42399\n",
      "Epoch: 00 [22484/27795 ( 81%)], Train Loss: 0.42371\n",
      "Epoch: 00 [22524/27795 ( 81%)], Train Loss: 0.42354\n",
      "Epoch: 00 [22564/27795 ( 81%)], Train Loss: 0.42317\n",
      "Epoch: 00 [22604/27795 ( 81%)], Train Loss: 0.42339\n",
      "Epoch: 00 [22644/27795 ( 81%)], Train Loss: 0.42341\n",
      "Epoch: 00 [22684/27795 ( 82%)], Train Loss: 0.42320\n",
      "Epoch: 00 [22724/27795 ( 82%)], Train Loss: 0.42327\n",
      "Epoch: 00 [22764/27795 ( 82%)], Train Loss: 0.42326\n",
      "Epoch: 00 [22804/27795 ( 82%)], Train Loss: 0.42306\n",
      "Epoch: 00 [22844/27795 ( 82%)], Train Loss: 0.42313\n",
      "Epoch: 00 [22884/27795 ( 82%)], Train Loss: 0.42284\n",
      "Epoch: 00 [22924/27795 ( 82%)], Train Loss: 0.42265\n",
      "Epoch: 00 [22964/27795 ( 83%)], Train Loss: 0.42264\n",
      "Epoch: 00 [23004/27795 ( 83%)], Train Loss: 0.42250\n",
      "Epoch: 00 [23044/27795 ( 83%)], Train Loss: 0.42247\n",
      "Epoch: 00 [23084/27795 ( 83%)], Train Loss: 0.42210\n",
      "Epoch: 00 [23124/27795 ( 83%)], Train Loss: 0.42223\n",
      "Epoch: 00 [23164/27795 ( 83%)], Train Loss: 0.42229\n",
      "Epoch: 00 [23204/27795 ( 83%)], Train Loss: 0.42191\n",
      "Epoch: 00 [23244/27795 ( 84%)], Train Loss: 0.42193\n",
      "Epoch: 00 [23284/27795 ( 84%)], Train Loss: 0.42171\n",
      "Epoch: 00 [23324/27795 ( 84%)], Train Loss: 0.42138\n",
      "Epoch: 00 [23364/27795 ( 84%)], Train Loss: 0.42132\n",
      "Epoch: 00 [23404/27795 ( 84%)], Train Loss: 0.42099\n",
      "Epoch: 00 [23444/27795 ( 84%)], Train Loss: 0.42088\n",
      "Epoch: 00 [23484/27795 ( 84%)], Train Loss: 0.42055\n",
      "Epoch: 00 [23524/27795 ( 85%)], Train Loss: 0.42047\n",
      "Epoch: 00 [23564/27795 ( 85%)], Train Loss: 0.42031\n",
      "Epoch: 00 [23604/27795 ( 85%)], Train Loss: 0.42015\n",
      "Epoch: 00 [23644/27795 ( 85%)], Train Loss: 0.42002\n",
      "Epoch: 00 [23684/27795 ( 85%)], Train Loss: 0.41977\n",
      "Epoch: 00 [23724/27795 ( 85%)], Train Loss: 0.41973\n",
      "Epoch: 00 [23764/27795 ( 85%)], Train Loss: 0.41957\n",
      "Epoch: 00 [23804/27795 ( 86%)], Train Loss: 0.41936\n",
      "Epoch: 00 [23844/27795 ( 86%)], Train Loss: 0.41931\n",
      "Epoch: 00 [23884/27795 ( 86%)], Train Loss: 0.41890\n",
      "Epoch: 00 [23924/27795 ( 86%)], Train Loss: 0.41903\n",
      "Epoch: 00 [23964/27795 ( 86%)], Train Loss: 0.41879\n",
      "Epoch: 00 [24004/27795 ( 86%)], Train Loss: 0.41877\n",
      "Epoch: 00 [24044/27795 ( 87%)], Train Loss: 0.41851\n",
      "Epoch: 00 [24084/27795 ( 87%)], Train Loss: 0.41823\n",
      "Epoch: 00 [24124/27795 ( 87%)], Train Loss: 0.41814\n",
      "Epoch: 00 [24164/27795 ( 87%)], Train Loss: 0.41811\n",
      "Epoch: 00 [24204/27795 ( 87%)], Train Loss: 0.41799\n",
      "Epoch: 00 [24244/27795 ( 87%)], Train Loss: 0.41782\n",
      "Epoch: 00 [24284/27795 ( 87%)], Train Loss: 0.41749\n",
      "Epoch: 00 [24324/27795 ( 88%)], Train Loss: 0.41719\n",
      "Epoch: 00 [24364/27795 ( 88%)], Train Loss: 0.41703\n",
      "Epoch: 00 [24404/27795 ( 88%)], Train Loss: 0.41718\n",
      "Epoch: 00 [24444/27795 ( 88%)], Train Loss: 0.41689\n",
      "Epoch: 00 [24484/27795 ( 88%)], Train Loss: 0.41668\n",
      "Epoch: 00 [24524/27795 ( 88%)], Train Loss: 0.41657\n",
      "Epoch: 00 [24564/27795 ( 88%)], Train Loss: 0.41649\n",
      "Epoch: 00 [24604/27795 ( 89%)], Train Loss: 0.41623\n",
      "Epoch: 00 [24644/27795 ( 89%)], Train Loss: 0.41620\n",
      "Epoch: 00 [24684/27795 ( 89%)], Train Loss: 0.41597\n",
      "Epoch: 00 [24724/27795 ( 89%)], Train Loss: 0.41567\n",
      "Epoch: 00 [24764/27795 ( 89%)], Train Loss: 0.41577\n",
      "Epoch: 00 [24804/27795 ( 89%)], Train Loss: 0.41557\n",
      "Epoch: 00 [24844/27795 ( 89%)], Train Loss: 0.41534\n",
      "Epoch: 00 [24884/27795 ( 90%)], Train Loss: 0.41525\n",
      "Epoch: 00 [24924/27795 ( 90%)], Train Loss: 0.41507\n",
      "Epoch: 00 [24964/27795 ( 90%)], Train Loss: 0.41501\n",
      "Epoch: 00 [25004/27795 ( 90%)], Train Loss: 0.41481\n",
      "Epoch: 00 [25044/27795 ( 90%)], Train Loss: 0.41461\n",
      "Epoch: 00 [25084/27795 ( 90%)], Train Loss: 0.41428\n",
      "Epoch: 00 [25124/27795 ( 90%)], Train Loss: 0.41426\n",
      "Epoch: 00 [25164/27795 ( 91%)], Train Loss: 0.41435\n",
      "Epoch: 00 [25204/27795 ( 91%)], Train Loss: 0.41417\n",
      "Epoch: 00 [25244/27795 ( 91%)], Train Loss: 0.41386\n",
      "Epoch: 00 [25284/27795 ( 91%)], Train Loss: 0.41348\n",
      "Epoch: 00 [25324/27795 ( 91%)], Train Loss: 0.41356\n",
      "Epoch: 00 [25364/27795 ( 91%)], Train Loss: 0.41338\n",
      "Epoch: 00 [25404/27795 ( 91%)], Train Loss: 0.41310\n",
      "Epoch: 00 [25444/27795 ( 92%)], Train Loss: 0.41287\n",
      "Epoch: 00 [25484/27795 ( 92%)], Train Loss: 0.41248\n",
      "Epoch: 00 [25524/27795 ( 92%)], Train Loss: 0.41230\n",
      "Epoch: 00 [25564/27795 ( 92%)], Train Loss: 0.41209\n",
      "Epoch: 00 [25604/27795 ( 92%)], Train Loss: 0.41189\n",
      "Epoch: 00 [25644/27795 ( 92%)], Train Loss: 0.41170\n",
      "Epoch: 00 [25684/27795 ( 92%)], Train Loss: 0.41152\n",
      "Epoch: 00 [25724/27795 ( 93%)], Train Loss: 0.41143\n",
      "Epoch: 00 [25764/27795 ( 93%)], Train Loss: 0.41100\n",
      "Epoch: 00 [25804/27795 ( 93%)], Train Loss: 0.41078\n",
      "Epoch: 00 [25844/27795 ( 93%)], Train Loss: 0.41063\n",
      "Epoch: 00 [25884/27795 ( 93%)], Train Loss: 0.41030\n",
      "Epoch: 00 [25924/27795 ( 93%)], Train Loss: 0.40994\n",
      "Epoch: 00 [25964/27795 ( 93%)], Train Loss: 0.40970\n",
      "Epoch: 00 [26004/27795 ( 94%)], Train Loss: 0.40966\n",
      "Epoch: 00 [26044/27795 ( 94%)], Train Loss: 0.40942\n",
      "Epoch: 00 [26084/27795 ( 94%)], Train Loss: 0.40917\n",
      "Epoch: 00 [26124/27795 ( 94%)], Train Loss: 0.40890\n",
      "Epoch: 00 [26164/27795 ( 94%)], Train Loss: 0.40890\n",
      "Epoch: 00 [26204/27795 ( 94%)], Train Loss: 0.40856\n",
      "Epoch: 00 [26244/27795 ( 94%)], Train Loss: 0.40843\n",
      "Epoch: 00 [26284/27795 ( 95%)], Train Loss: 0.40815\n",
      "Epoch: 00 [26324/27795 ( 95%)], Train Loss: 0.40787\n",
      "Epoch: 00 [26364/27795 ( 95%)], Train Loss: 0.40757\n",
      "Epoch: 00 [26404/27795 ( 95%)], Train Loss: 0.40734\n",
      "Epoch: 00 [26444/27795 ( 95%)], Train Loss: 0.40712\n",
      "Epoch: 00 [26484/27795 ( 95%)], Train Loss: 0.40703\n",
      "Epoch: 00 [26524/27795 ( 95%)], Train Loss: 0.40693\n",
      "Epoch: 00 [26564/27795 ( 96%)], Train Loss: 0.40677\n",
      "Epoch: 00 [26604/27795 ( 96%)], Train Loss: 0.40661\n",
      "Epoch: 00 [26644/27795 ( 96%)], Train Loss: 0.40643\n",
      "Epoch: 00 [26684/27795 ( 96%)], Train Loss: 0.40611\n",
      "Epoch: 00 [26724/27795 ( 96%)], Train Loss: 0.40602\n",
      "Epoch: 00 [26764/27795 ( 96%)], Train Loss: 0.40594\n",
      "Epoch: 00 [26804/27795 ( 96%)], Train Loss: 0.40569\n",
      "Epoch: 00 [26844/27795 ( 97%)], Train Loss: 0.40545\n",
      "Epoch: 00 [26884/27795 ( 97%)], Train Loss: 0.40520\n",
      "Epoch: 00 [26924/27795 ( 97%)], Train Loss: 0.40519\n",
      "Epoch: 00 [26964/27795 ( 97%)], Train Loss: 0.40514\n",
      "Epoch: 00 [27004/27795 ( 97%)], Train Loss: 0.40495\n",
      "Epoch: 00 [27044/27795 ( 97%)], Train Loss: 0.40502\n",
      "Epoch: 00 [27084/27795 ( 97%)], Train Loss: 0.40472\n",
      "Epoch: 00 [27124/27795 ( 98%)], Train Loss: 0.40454\n",
      "Epoch: 00 [27164/27795 ( 98%)], Train Loss: 0.40429\n",
      "Epoch: 00 [27204/27795 ( 98%)], Train Loss: 0.40397\n",
      "Epoch: 00 [27244/27795 ( 98%)], Train Loss: 0.40375\n",
      "Epoch: 00 [27284/27795 ( 98%)], Train Loss: 0.40358\n",
      "Epoch: 00 [27324/27795 ( 98%)], Train Loss: 0.40358\n",
      "Epoch: 00 [27364/27795 ( 98%)], Train Loss: 0.40338\n",
      "Epoch: 00 [27404/27795 ( 99%)], Train Loss: 0.40310\n",
      "Epoch: 00 [27444/27795 ( 99%)], Train Loss: 0.40293\n",
      "Epoch: 00 [27484/27795 ( 99%)], Train Loss: 0.40259\n",
      "Epoch: 00 [27524/27795 ( 99%)], Train Loss: 0.40246\n",
      "Epoch: 00 [27564/27795 ( 99%)], Train Loss: 0.40227\n",
      "Epoch: 00 [27604/27795 ( 99%)], Train Loss: 0.40195\n",
      "Epoch: 00 [27644/27795 ( 99%)], Train Loss: 0.40168\n",
      "Epoch: 00 [27684/27795 (100%)], Train Loss: 0.40128\n",
      "Epoch: 00 [27724/27795 (100%)], Train Loss: 0.40122\n",
      "Epoch: 00 [27764/27795 (100%)], Train Loss: 0.40103\n",
      "Epoch: 00 [27795/27795 (100%)], Train Loss: 0.40085\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.57611\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.57611\n",
      "Saving model checkpoint to output/checkpoint-fold-1.\n",
      "\n",
      "Total Training Time: 3231.356919527054secs, Average Training Time per Epoch: 3231.356919527054secs.\n",
      "Total Validation Time: 140.61825370788574secs, Average Validation Time per Epoch: 140.61825370788574secs.\n",
      "--------------------------------------------------\n",
      "FOLD: 2\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27797, Num examples Valid=3925\n",
      "Total Training Steps: 3475, Total Warmup Steps: 347\n",
      "Epoch: 00 [    4/27797 (  0%)], Train Loss: 2.83885\n",
      "Epoch: 00 [   44/27797 (  0%)], Train Loss: 2.79904\n",
      "Epoch: 00 [   84/27797 (  0%)], Train Loss: 2.78572\n",
      "Epoch: 00 [  124/27797 (  0%)], Train Loss: 2.76503\n",
      "Epoch: 00 [  164/27797 (  1%)], Train Loss: 2.73000\n",
      "Epoch: 00 [  204/27797 (  1%)], Train Loss: 2.69540\n",
      "Epoch: 00 [  244/27797 (  1%)], Train Loss: 2.65485\n",
      "Epoch: 00 [  284/27797 (  1%)], Train Loss: 2.60598\n",
      "Epoch: 00 [  324/27797 (  1%)], Train Loss: 2.54537\n",
      "Epoch: 00 [  364/27797 (  1%)], Train Loss: 2.47768\n",
      "Epoch: 00 [  404/27797 (  1%)], Train Loss: 2.40970\n",
      "Epoch: 00 [  444/27797 (  2%)], Train Loss: 2.33070\n",
      "Epoch: 00 [  484/27797 (  2%)], Train Loss: 2.25721\n",
      "Epoch: 00 [  524/27797 (  2%)], Train Loss: 2.18652\n",
      "Epoch: 00 [  564/27797 (  2%)], Train Loss: 2.11083\n",
      "Epoch: 00 [  604/27797 (  2%)], Train Loss: 2.02047\n",
      "Epoch: 00 [  644/27797 (  2%)], Train Loss: 1.95163\n",
      "Epoch: 00 [  684/27797 (  2%)], Train Loss: 1.88019\n",
      "Epoch: 00 [  724/27797 (  3%)], Train Loss: 1.83484\n",
      "Epoch: 00 [  764/27797 (  3%)], Train Loss: 1.77591\n",
      "Epoch: 00 [  804/27797 (  3%)], Train Loss: 1.73432\n",
      "Epoch: 00 [  844/27797 (  3%)], Train Loss: 1.67817\n",
      "Epoch: 00 [  884/27797 (  3%)], Train Loss: 1.63447\n",
      "Epoch: 00 [  924/27797 (  3%)], Train Loss: 1.59816\n",
      "Epoch: 00 [  964/27797 (  3%)], Train Loss: 1.54760\n",
      "Epoch: 00 [ 1004/27797 (  4%)], Train Loss: 1.50525\n",
      "Epoch: 00 [ 1044/27797 (  4%)], Train Loss: 1.46506\n",
      "Epoch: 00 [ 1084/27797 (  4%)], Train Loss: 1.43667\n",
      "Epoch: 00 [ 1124/27797 (  4%)], Train Loss: 1.40553\n",
      "Epoch: 00 [ 1164/27797 (  4%)], Train Loss: 1.37764\n",
      "Epoch: 00 [ 1204/27797 (  4%)], Train Loss: 1.35107\n",
      "Epoch: 00 [ 1244/27797 (  4%)], Train Loss: 1.32761\n",
      "Epoch: 00 [ 1284/27797 (  5%)], Train Loss: 1.30618\n",
      "Epoch: 00 [ 1324/27797 (  5%)], Train Loss: 1.27698\n",
      "Epoch: 00 [ 1364/27797 (  5%)], Train Loss: 1.24879\n",
      "Epoch: 00 [ 1404/27797 (  5%)], Train Loss: 1.22362\n",
      "Epoch: 00 [ 1444/27797 (  5%)], Train Loss: 1.20893\n",
      "Epoch: 00 [ 1484/27797 (  5%)], Train Loss: 1.19043\n",
      "Epoch: 00 [ 1524/27797 (  5%)], Train Loss: 1.17361\n",
      "Epoch: 00 [ 1564/27797 (  6%)], Train Loss: 1.15611\n",
      "Epoch: 00 [ 1604/27797 (  6%)], Train Loss: 1.14734\n",
      "Epoch: 00 [ 1644/27797 (  6%)], Train Loss: 1.13268\n",
      "Epoch: 00 [ 1684/27797 (  6%)], Train Loss: 1.11824\n",
      "Epoch: 00 [ 1724/27797 (  6%)], Train Loss: 1.10163\n",
      "Epoch: 00 [ 1764/27797 (  6%)], Train Loss: 1.09064\n",
      "Epoch: 00 [ 1804/27797 (  6%)], Train Loss: 1.07907\n",
      "Epoch: 00 [ 1844/27797 (  7%)], Train Loss: 1.06551\n",
      "Epoch: 00 [ 1884/27797 (  7%)], Train Loss: 1.05130\n",
      "Epoch: 00 [ 1924/27797 (  7%)], Train Loss: 1.03806\n",
      "Epoch: 00 [ 1964/27797 (  7%)], Train Loss: 1.02719\n",
      "Epoch: 00 [ 2004/27797 (  7%)], Train Loss: 1.01630\n",
      "Epoch: 00 [ 2044/27797 (  7%)], Train Loss: 1.00394\n",
      "Epoch: 00 [ 2084/27797 (  7%)], Train Loss: 0.99392\n",
      "Epoch: 00 [ 2124/27797 (  8%)], Train Loss: 0.98250\n",
      "Epoch: 00 [ 2164/27797 (  8%)], Train Loss: 0.97100\n",
      "Epoch: 00 [ 2204/27797 (  8%)], Train Loss: 0.96198\n",
      "Epoch: 00 [ 2244/27797 (  8%)], Train Loss: 0.95561\n",
      "Epoch: 00 [ 2284/27797 (  8%)], Train Loss: 0.94558\n",
      "Epoch: 00 [ 2324/27797 (  8%)], Train Loss: 0.93491\n",
      "Epoch: 00 [ 2364/27797 (  9%)], Train Loss: 0.92927\n",
      "Epoch: 00 [ 2404/27797 (  9%)], Train Loss: 0.92072\n",
      "Epoch: 00 [ 2444/27797 (  9%)], Train Loss: 0.91366\n",
      "Epoch: 00 [ 2484/27797 (  9%)], Train Loss: 0.90639\n",
      "Epoch: 00 [ 2524/27797 (  9%)], Train Loss: 0.90140\n",
      "Epoch: 00 [ 2564/27797 (  9%)], Train Loss: 0.89685\n",
      "Epoch: 00 [ 2604/27797 (  9%)], Train Loss: 0.89112\n",
      "Epoch: 00 [ 2644/27797 ( 10%)], Train Loss: 0.88657\n",
      "Epoch: 00 [ 2684/27797 ( 10%)], Train Loss: 0.87899\n",
      "Epoch: 00 [ 2724/27797 ( 10%)], Train Loss: 0.87358\n",
      "Epoch: 00 [ 2764/27797 ( 10%)], Train Loss: 0.86515\n",
      "Epoch: 00 [ 2804/27797 ( 10%)], Train Loss: 0.85995\n",
      "Epoch: 00 [ 2844/27797 ( 10%)], Train Loss: 0.85182\n",
      "Epoch: 00 [ 2884/27797 ( 10%)], Train Loss: 0.84503\n",
      "Epoch: 00 [ 2924/27797 ( 11%)], Train Loss: 0.83758\n",
      "Epoch: 00 [ 2964/27797 ( 11%)], Train Loss: 0.82954\n",
      "Epoch: 00 [ 3004/27797 ( 11%)], Train Loss: 0.82760\n",
      "Epoch: 00 [ 3044/27797 ( 11%)], Train Loss: 0.82407\n",
      "Epoch: 00 [ 3084/27797 ( 11%)], Train Loss: 0.81821\n",
      "Epoch: 00 [ 3124/27797 ( 11%)], Train Loss: 0.81264\n",
      "Epoch: 00 [ 3164/27797 ( 11%)], Train Loss: 0.80751\n",
      "Epoch: 00 [ 3204/27797 ( 12%)], Train Loss: 0.80380\n",
      "Epoch: 00 [ 3244/27797 ( 12%)], Train Loss: 0.80309\n",
      "Epoch: 00 [ 3284/27797 ( 12%)], Train Loss: 0.79972\n",
      "Epoch: 00 [ 3324/27797 ( 12%)], Train Loss: 0.79627\n",
      "Epoch: 00 [ 3364/27797 ( 12%)], Train Loss: 0.79283\n",
      "Epoch: 00 [ 3404/27797 ( 12%)], Train Loss: 0.78899\n",
      "Epoch: 00 [ 3444/27797 ( 12%)], Train Loss: 0.78564\n",
      "Epoch: 00 [ 3484/27797 ( 13%)], Train Loss: 0.78079\n",
      "Epoch: 00 [ 3524/27797 ( 13%)], Train Loss: 0.77562\n",
      "Epoch: 00 [ 3564/27797 ( 13%)], Train Loss: 0.76996\n",
      "Epoch: 00 [ 3604/27797 ( 13%)], Train Loss: 0.76794\n",
      "Epoch: 00 [ 3644/27797 ( 13%)], Train Loss: 0.76354\n",
      "Epoch: 00 [ 3684/27797 ( 13%)], Train Loss: 0.75864\n",
      "Epoch: 00 [ 3724/27797 ( 13%)], Train Loss: 0.75360\n",
      "Epoch: 00 [ 3764/27797 ( 14%)], Train Loss: 0.75095\n",
      "Epoch: 00 [ 3804/27797 ( 14%)], Train Loss: 0.74714\n",
      "Epoch: 00 [ 3844/27797 ( 14%)], Train Loss: 0.74437\n",
      "Epoch: 00 [ 3884/27797 ( 14%)], Train Loss: 0.73934\n",
      "Epoch: 00 [ 3924/27797 ( 14%)], Train Loss: 0.73528\n",
      "Epoch: 00 [ 3964/27797 ( 14%)], Train Loss: 0.73154\n",
      "Epoch: 00 [ 4004/27797 ( 14%)], Train Loss: 0.72714\n",
      "Epoch: 00 [ 4044/27797 ( 15%)], Train Loss: 0.72570\n",
      "Epoch: 00 [ 4084/27797 ( 15%)], Train Loss: 0.72284\n",
      "Epoch: 00 [ 4124/27797 ( 15%)], Train Loss: 0.71977\n",
      "Epoch: 00 [ 4164/27797 ( 15%)], Train Loss: 0.71645\n",
      "Epoch: 00 [ 4204/27797 ( 15%)], Train Loss: 0.71659\n",
      "Epoch: 00 [ 4244/27797 ( 15%)], Train Loss: 0.71690\n",
      "Epoch: 00 [ 4284/27797 ( 15%)], Train Loss: 0.71357\n",
      "Epoch: 00 [ 4324/27797 ( 16%)], Train Loss: 0.71236\n",
      "Epoch: 00 [ 4364/27797 ( 16%)], Train Loss: 0.70887\n",
      "Epoch: 00 [ 4404/27797 ( 16%)], Train Loss: 0.70500\n",
      "Epoch: 00 [ 4444/27797 ( 16%)], Train Loss: 0.70273\n",
      "Epoch: 00 [ 4484/27797 ( 16%)], Train Loss: 0.70383\n",
      "Epoch: 00 [ 4524/27797 ( 16%)], Train Loss: 0.70319\n",
      "Epoch: 00 [ 4564/27797 ( 16%)], Train Loss: 0.70007\n",
      "Epoch: 00 [ 4604/27797 ( 17%)], Train Loss: 0.69935\n",
      "Epoch: 00 [ 4644/27797 ( 17%)], Train Loss: 0.69704\n",
      "Epoch: 00 [ 4684/27797 ( 17%)], Train Loss: 0.69523\n",
      "Epoch: 00 [ 4724/27797 ( 17%)], Train Loss: 0.69457\n",
      "Epoch: 00 [ 4764/27797 ( 17%)], Train Loss: 0.69125\n",
      "Epoch: 00 [ 4804/27797 ( 17%)], Train Loss: 0.68809\n",
      "Epoch: 00 [ 4844/27797 ( 17%)], Train Loss: 0.68563\n",
      "Epoch: 00 [ 4884/27797 ( 18%)], Train Loss: 0.68417\n",
      "Epoch: 00 [ 4924/27797 ( 18%)], Train Loss: 0.68320\n",
      "Epoch: 00 [ 4964/27797 ( 18%)], Train Loss: 0.68064\n",
      "Epoch: 00 [ 5004/27797 ( 18%)], Train Loss: 0.67853\n",
      "Epoch: 00 [ 5044/27797 ( 18%)], Train Loss: 0.67795\n",
      "Epoch: 00 [ 5084/27797 ( 18%)], Train Loss: 0.67448\n",
      "Epoch: 00 [ 5124/27797 ( 18%)], Train Loss: 0.67258\n",
      "Epoch: 00 [ 5164/27797 ( 19%)], Train Loss: 0.67028\n",
      "Epoch: 00 [ 5204/27797 ( 19%)], Train Loss: 0.66866\n",
      "Epoch: 00 [ 5244/27797 ( 19%)], Train Loss: 0.66582\n",
      "Epoch: 00 [ 5284/27797 ( 19%)], Train Loss: 0.66640\n",
      "Epoch: 00 [ 5324/27797 ( 19%)], Train Loss: 0.66531\n",
      "Epoch: 00 [ 5364/27797 ( 19%)], Train Loss: 0.66326\n",
      "Epoch: 00 [ 5404/27797 ( 19%)], Train Loss: 0.66071\n",
      "Epoch: 00 [ 5444/27797 ( 20%)], Train Loss: 0.65832\n",
      "Epoch: 00 [ 5484/27797 ( 20%)], Train Loss: 0.65698\n",
      "Epoch: 00 [ 5524/27797 ( 20%)], Train Loss: 0.65631\n",
      "Epoch: 00 [ 5564/27797 ( 20%)], Train Loss: 0.65601\n",
      "Epoch: 00 [ 5604/27797 ( 20%)], Train Loss: 0.65433\n",
      "Epoch: 00 [ 5644/27797 ( 20%)], Train Loss: 0.65243\n",
      "Epoch: 00 [ 5684/27797 ( 20%)], Train Loss: 0.65158\n",
      "Epoch: 00 [ 5724/27797 ( 21%)], Train Loss: 0.64909\n",
      "Epoch: 00 [ 5764/27797 ( 21%)], Train Loss: 0.64729\n",
      "Epoch: 00 [ 5804/27797 ( 21%)], Train Loss: 0.64628\n",
      "Epoch: 00 [ 5844/27797 ( 21%)], Train Loss: 0.64548\n",
      "Epoch: 00 [ 5884/27797 ( 21%)], Train Loss: 0.64392\n",
      "Epoch: 00 [ 5924/27797 ( 21%)], Train Loss: 0.64311\n",
      "Epoch: 00 [ 5964/27797 ( 21%)], Train Loss: 0.64183\n",
      "Epoch: 00 [ 6004/27797 ( 22%)], Train Loss: 0.64136\n",
      "Epoch: 00 [ 6044/27797 ( 22%)], Train Loss: 0.63952\n",
      "Epoch: 00 [ 6084/27797 ( 22%)], Train Loss: 0.63843\n",
      "Epoch: 00 [ 6124/27797 ( 22%)], Train Loss: 0.63683\n",
      "Epoch: 00 [ 6164/27797 ( 22%)], Train Loss: 0.63472\n",
      "Epoch: 00 [ 6204/27797 ( 22%)], Train Loss: 0.63385\n",
      "Epoch: 00 [ 6244/27797 ( 22%)], Train Loss: 0.63168\n",
      "Epoch: 00 [ 6284/27797 ( 23%)], Train Loss: 0.62973\n",
      "Epoch: 00 [ 6324/27797 ( 23%)], Train Loss: 0.62863\n",
      "Epoch: 00 [ 6364/27797 ( 23%)], Train Loss: 0.62677\n",
      "Epoch: 00 [ 6404/27797 ( 23%)], Train Loss: 0.62607\n",
      "Epoch: 00 [ 6444/27797 ( 23%)], Train Loss: 0.62540\n",
      "Epoch: 00 [ 6484/27797 ( 23%)], Train Loss: 0.62419\n",
      "Epoch: 00 [ 6524/27797 ( 23%)], Train Loss: 0.62332\n",
      "Epoch: 00 [ 6564/27797 ( 24%)], Train Loss: 0.62211\n",
      "Epoch: 00 [ 6604/27797 ( 24%)], Train Loss: 0.62122\n",
      "Epoch: 00 [ 6644/27797 ( 24%)], Train Loss: 0.61992\n",
      "Epoch: 00 [ 6684/27797 ( 24%)], Train Loss: 0.61866\n",
      "Epoch: 00 [ 6724/27797 ( 24%)], Train Loss: 0.61693\n",
      "Epoch: 00 [ 6764/27797 ( 24%)], Train Loss: 0.61506\n",
      "Epoch: 00 [ 6804/27797 ( 24%)], Train Loss: 0.61327\n",
      "Epoch: 00 [ 6844/27797 ( 25%)], Train Loss: 0.61207\n",
      "Epoch: 00 [ 6884/27797 ( 25%)], Train Loss: 0.61033\n",
      "Epoch: 00 [ 6924/27797 ( 25%)], Train Loss: 0.60954\n",
      "Epoch: 00 [ 6964/27797 ( 25%)], Train Loss: 0.60811\n",
      "Epoch: 00 [ 7004/27797 ( 25%)], Train Loss: 0.60691\n",
      "Epoch: 00 [ 7044/27797 ( 25%)], Train Loss: 0.60577\n",
      "Epoch: 00 [ 7084/27797 ( 25%)], Train Loss: 0.60461\n",
      "Epoch: 00 [ 7124/27797 ( 26%)], Train Loss: 0.60304\n",
      "Epoch: 00 [ 7164/27797 ( 26%)], Train Loss: 0.60169\n",
      "Epoch: 00 [ 7204/27797 ( 26%)], Train Loss: 0.60018\n",
      "Epoch: 00 [ 7244/27797 ( 26%)], Train Loss: 0.59915\n",
      "Epoch: 00 [ 7284/27797 ( 26%)], Train Loss: 0.59902\n",
      "Epoch: 00 [ 7324/27797 ( 26%)], Train Loss: 0.59829\n",
      "Epoch: 00 [ 7364/27797 ( 26%)], Train Loss: 0.59763\n",
      "Epoch: 00 [ 7404/27797 ( 27%)], Train Loss: 0.59638\n",
      "Epoch: 00 [ 7444/27797 ( 27%)], Train Loss: 0.59465\n",
      "Epoch: 00 [ 7484/27797 ( 27%)], Train Loss: 0.59350\n",
      "Epoch: 00 [ 7524/27797 ( 27%)], Train Loss: 0.59200\n",
      "Epoch: 00 [ 7564/27797 ( 27%)], Train Loss: 0.59082\n",
      "Epoch: 00 [ 7604/27797 ( 27%)], Train Loss: 0.58927\n",
      "Epoch: 00 [ 7644/27797 ( 27%)], Train Loss: 0.58806\n",
      "Epoch: 00 [ 7684/27797 ( 28%)], Train Loss: 0.58702\n",
      "Epoch: 00 [ 7724/27797 ( 28%)], Train Loss: 0.58688\n",
      "Epoch: 00 [ 7764/27797 ( 28%)], Train Loss: 0.58602\n",
      "Epoch: 00 [ 7804/27797 ( 28%)], Train Loss: 0.58521\n",
      "Epoch: 00 [ 7844/27797 ( 28%)], Train Loss: 0.58415\n",
      "Epoch: 00 [ 7884/27797 ( 28%)], Train Loss: 0.58314\n",
      "Epoch: 00 [ 7924/27797 ( 29%)], Train Loss: 0.58116\n",
      "Epoch: 00 [ 7964/27797 ( 29%)], Train Loss: 0.58165\n",
      "Epoch: 00 [ 8004/27797 ( 29%)], Train Loss: 0.58055\n",
      "Epoch: 00 [ 8044/27797 ( 29%)], Train Loss: 0.57906\n",
      "Epoch: 00 [ 8084/27797 ( 29%)], Train Loss: 0.57901\n",
      "Epoch: 00 [ 8124/27797 ( 29%)], Train Loss: 0.57863\n",
      "Epoch: 00 [ 8164/27797 ( 29%)], Train Loss: 0.57747\n",
      "Epoch: 00 [ 8204/27797 ( 30%)], Train Loss: 0.57715\n",
      "Epoch: 00 [ 8244/27797 ( 30%)], Train Loss: 0.57644\n",
      "Epoch: 00 [ 8284/27797 ( 30%)], Train Loss: 0.57531\n",
      "Epoch: 00 [ 8324/27797 ( 30%)], Train Loss: 0.57387\n",
      "Epoch: 00 [ 8364/27797 ( 30%)], Train Loss: 0.57234\n",
      "Epoch: 00 [ 8404/27797 ( 30%)], Train Loss: 0.57188\n",
      "Epoch: 00 [ 8444/27797 ( 30%)], Train Loss: 0.57104\n",
      "Epoch: 00 [ 8484/27797 ( 31%)], Train Loss: 0.56977\n",
      "Epoch: 00 [ 8524/27797 ( 31%)], Train Loss: 0.56920\n",
      "Epoch: 00 [ 8564/27797 ( 31%)], Train Loss: 0.56872\n",
      "Epoch: 00 [ 8604/27797 ( 31%)], Train Loss: 0.56746\n",
      "Epoch: 00 [ 8644/27797 ( 31%)], Train Loss: 0.56709\n",
      "Epoch: 00 [ 8684/27797 ( 31%)], Train Loss: 0.56549\n",
      "Epoch: 00 [ 8724/27797 ( 31%)], Train Loss: 0.56563\n",
      "Epoch: 00 [ 8764/27797 ( 32%)], Train Loss: 0.56437\n",
      "Epoch: 00 [ 8804/27797 ( 32%)], Train Loss: 0.56324\n",
      "Epoch: 00 [ 8844/27797 ( 32%)], Train Loss: 0.56201\n",
      "Epoch: 00 [ 8884/27797 ( 32%)], Train Loss: 0.56085\n",
      "Epoch: 00 [ 8924/27797 ( 32%)], Train Loss: 0.56139\n",
      "Epoch: 00 [ 8964/27797 ( 32%)], Train Loss: 0.56009\n",
      "Epoch: 00 [ 9004/27797 ( 32%)], Train Loss: 0.55974\n",
      "Epoch: 00 [ 9044/27797 ( 33%)], Train Loss: 0.55907\n",
      "Epoch: 00 [ 9084/27797 ( 33%)], Train Loss: 0.55838\n",
      "Epoch: 00 [ 9124/27797 ( 33%)], Train Loss: 0.55798\n",
      "Epoch: 00 [ 9164/27797 ( 33%)], Train Loss: 0.55688\n",
      "Epoch: 00 [ 9204/27797 ( 33%)], Train Loss: 0.55618\n",
      "Epoch: 00 [ 9244/27797 ( 33%)], Train Loss: 0.55501\n",
      "Epoch: 00 [ 9284/27797 ( 33%)], Train Loss: 0.55382\n",
      "Epoch: 00 [ 9324/27797 ( 34%)], Train Loss: 0.55320\n",
      "Epoch: 00 [ 9364/27797 ( 34%)], Train Loss: 0.55254\n",
      "Epoch: 00 [ 9404/27797 ( 34%)], Train Loss: 0.55206\n",
      "Epoch: 00 [ 9444/27797 ( 34%)], Train Loss: 0.55178\n",
      "Epoch: 00 [ 9484/27797 ( 34%)], Train Loss: 0.55059\n",
      "Epoch: 00 [ 9524/27797 ( 34%)], Train Loss: 0.55002\n",
      "Epoch: 00 [ 9564/27797 ( 34%)], Train Loss: 0.54934\n",
      "Epoch: 00 [ 9604/27797 ( 35%)], Train Loss: 0.54903\n",
      "Epoch: 00 [ 9644/27797 ( 35%)], Train Loss: 0.54834\n",
      "Epoch: 00 [ 9684/27797 ( 35%)], Train Loss: 0.54793\n",
      "Epoch: 00 [ 9724/27797 ( 35%)], Train Loss: 0.54718\n",
      "Epoch: 00 [ 9764/27797 ( 35%)], Train Loss: 0.54650\n",
      "Epoch: 00 [ 9804/27797 ( 35%)], Train Loss: 0.54546\n",
      "Epoch: 00 [ 9844/27797 ( 35%)], Train Loss: 0.54439\n",
      "Epoch: 00 [ 9884/27797 ( 36%)], Train Loss: 0.54385\n",
      "Epoch: 00 [ 9924/27797 ( 36%)], Train Loss: 0.54326\n",
      "Epoch: 00 [ 9964/27797 ( 36%)], Train Loss: 0.54205\n",
      "Epoch: 00 [10004/27797 ( 36%)], Train Loss: 0.54202\n",
      "Epoch: 00 [10044/27797 ( 36%)], Train Loss: 0.54153\n",
      "Epoch: 00 [10084/27797 ( 36%)], Train Loss: 0.54084\n",
      "Epoch: 00 [10124/27797 ( 36%)], Train Loss: 0.53984\n",
      "Epoch: 00 [10164/27797 ( 37%)], Train Loss: 0.53965\n",
      "Epoch: 00 [10204/27797 ( 37%)], Train Loss: 0.53843\n",
      "Epoch: 00 [10244/27797 ( 37%)], Train Loss: 0.53813\n",
      "Epoch: 00 [10284/27797 ( 37%)], Train Loss: 0.53725\n",
      "Epoch: 00 [10324/27797 ( 37%)], Train Loss: 0.53631\n",
      "Epoch: 00 [10364/27797 ( 37%)], Train Loss: 0.53507\n",
      "Epoch: 00 [10404/27797 ( 37%)], Train Loss: 0.53436\n",
      "Epoch: 00 [10444/27797 ( 38%)], Train Loss: 0.53389\n",
      "Epoch: 00 [10484/27797 ( 38%)], Train Loss: 0.53367\n",
      "Epoch: 00 [10524/27797 ( 38%)], Train Loss: 0.53301\n",
      "Epoch: 00 [10564/27797 ( 38%)], Train Loss: 0.53280\n",
      "Epoch: 00 [10604/27797 ( 38%)], Train Loss: 0.53283\n",
      "Epoch: 00 [10644/27797 ( 38%)], Train Loss: 0.53185\n",
      "Epoch: 00 [10684/27797 ( 38%)], Train Loss: 0.53150\n",
      "Epoch: 00 [10724/27797 ( 39%)], Train Loss: 0.53113\n",
      "Epoch: 00 [10764/27797 ( 39%)], Train Loss: 0.53070\n",
      "Epoch: 00 [10804/27797 ( 39%)], Train Loss: 0.52994\n",
      "Epoch: 00 [10844/27797 ( 39%)], Train Loss: 0.52924\n",
      "Epoch: 00 [10884/27797 ( 39%)], Train Loss: 0.52927\n",
      "Epoch: 00 [10924/27797 ( 39%)], Train Loss: 0.52908\n",
      "Epoch: 00 [10964/27797 ( 39%)], Train Loss: 0.52889\n",
      "Epoch: 00 [11004/27797 ( 40%)], Train Loss: 0.52855\n",
      "Epoch: 00 [11044/27797 ( 40%)], Train Loss: 0.52801\n",
      "Epoch: 00 [11084/27797 ( 40%)], Train Loss: 0.52773\n",
      "Epoch: 00 [11124/27797 ( 40%)], Train Loss: 0.52679\n",
      "Epoch: 00 [11164/27797 ( 40%)], Train Loss: 0.52572\n",
      "Epoch: 00 [11204/27797 ( 40%)], Train Loss: 0.52469\n",
      "Epoch: 00 [11244/27797 ( 40%)], Train Loss: 0.52438\n",
      "Epoch: 00 [11284/27797 ( 41%)], Train Loss: 0.52396\n",
      "Epoch: 00 [11324/27797 ( 41%)], Train Loss: 0.52329\n",
      "Epoch: 00 [11364/27797 ( 41%)], Train Loss: 0.52296\n",
      "Epoch: 00 [11404/27797 ( 41%)], Train Loss: 0.52274\n",
      "Epoch: 00 [11444/27797 ( 41%)], Train Loss: 0.52304\n",
      "Epoch: 00 [11484/27797 ( 41%)], Train Loss: 0.52224\n",
      "Epoch: 00 [11524/27797 ( 41%)], Train Loss: 0.52235\n",
      "Epoch: 00 [11564/27797 ( 42%)], Train Loss: 0.52164\n",
      "Epoch: 00 [11604/27797 ( 42%)], Train Loss: 0.52153\n",
      "Epoch: 00 [11644/27797 ( 42%)], Train Loss: 0.52112\n",
      "Epoch: 00 [11684/27797 ( 42%)], Train Loss: 0.52047\n",
      "Epoch: 00 [11724/27797 ( 42%)], Train Loss: 0.52010\n",
      "Epoch: 00 [11764/27797 ( 42%)], Train Loss: 0.52015\n",
      "Epoch: 00 [11804/27797 ( 42%)], Train Loss: 0.51931\n",
      "Epoch: 00 [11844/27797 ( 43%)], Train Loss: 0.51899\n",
      "Epoch: 00 [11884/27797 ( 43%)], Train Loss: 0.51887\n",
      "Epoch: 00 [11924/27797 ( 43%)], Train Loss: 0.51870\n",
      "Epoch: 00 [11964/27797 ( 43%)], Train Loss: 0.51821\n",
      "Epoch: 00 [12004/27797 ( 43%)], Train Loss: 0.51747\n",
      "Epoch: 00 [12044/27797 ( 43%)], Train Loss: 0.51726\n",
      "Epoch: 00 [12084/27797 ( 43%)], Train Loss: 0.51660\n",
      "Epoch: 00 [12124/27797 ( 44%)], Train Loss: 0.51602\n",
      "Epoch: 00 [12164/27797 ( 44%)], Train Loss: 0.51552\n",
      "Epoch: 00 [12204/27797 ( 44%)], Train Loss: 0.51494\n",
      "Epoch: 00 [12244/27797 ( 44%)], Train Loss: 0.51478\n",
      "Epoch: 00 [12284/27797 ( 44%)], Train Loss: 0.51417\n",
      "Epoch: 00 [12324/27797 ( 44%)], Train Loss: 0.51392\n",
      "Epoch: 00 [12364/27797 ( 44%)], Train Loss: 0.51390\n",
      "Epoch: 00 [12404/27797 ( 45%)], Train Loss: 0.51406\n",
      "Epoch: 00 [12444/27797 ( 45%)], Train Loss: 0.51369\n",
      "Epoch: 00 [12484/27797 ( 45%)], Train Loss: 0.51286\n",
      "Epoch: 00 [12524/27797 ( 45%)], Train Loss: 0.51290\n",
      "Epoch: 00 [12564/27797 ( 45%)], Train Loss: 0.51273\n",
      "Epoch: 00 [12604/27797 ( 45%)], Train Loss: 0.51245\n",
      "Epoch: 00 [12644/27797 ( 45%)], Train Loss: 0.51201\n",
      "Epoch: 00 [12684/27797 ( 46%)], Train Loss: 0.51160\n",
      "Epoch: 00 [12724/27797 ( 46%)], Train Loss: 0.51077\n",
      "Epoch: 00 [12764/27797 ( 46%)], Train Loss: 0.51049\n",
      "Epoch: 00 [12804/27797 ( 46%)], Train Loss: 0.50973\n",
      "Epoch: 00 [12844/27797 ( 46%)], Train Loss: 0.50913\n",
      "Epoch: 00 [12884/27797 ( 46%)], Train Loss: 0.50935\n",
      "Epoch: 00 [12924/27797 ( 46%)], Train Loss: 0.50873\n",
      "Epoch: 00 [12964/27797 ( 47%)], Train Loss: 0.50872\n",
      "Epoch: 00 [13004/27797 ( 47%)], Train Loss: 0.50868\n",
      "Epoch: 00 [13044/27797 ( 47%)], Train Loss: 0.50826\n",
      "Epoch: 00 [13084/27797 ( 47%)], Train Loss: 0.50801\n",
      "Epoch: 00 [13124/27797 ( 47%)], Train Loss: 0.50724\n",
      "Epoch: 00 [13164/27797 ( 47%)], Train Loss: 0.50631\n",
      "Epoch: 00 [13204/27797 ( 48%)], Train Loss: 0.50575\n",
      "Epoch: 00 [13244/27797 ( 48%)], Train Loss: 0.50545\n",
      "Epoch: 00 [13284/27797 ( 48%)], Train Loss: 0.50521\n",
      "Epoch: 00 [13324/27797 ( 48%)], Train Loss: 0.50488\n",
      "Epoch: 00 [13364/27797 ( 48%)], Train Loss: 0.50430\n",
      "Epoch: 00 [13404/27797 ( 48%)], Train Loss: 0.50371\n",
      "Epoch: 00 [13444/27797 ( 48%)], Train Loss: 0.50313\n",
      "Epoch: 00 [13484/27797 ( 49%)], Train Loss: 0.50305\n",
      "Epoch: 00 [13524/27797 ( 49%)], Train Loss: 0.50259\n",
      "Epoch: 00 [13564/27797 ( 49%)], Train Loss: 0.50211\n",
      "Epoch: 00 [13604/27797 ( 49%)], Train Loss: 0.50182\n",
      "Epoch: 00 [13644/27797 ( 49%)], Train Loss: 0.50128\n",
      "Epoch: 00 [13684/27797 ( 49%)], Train Loss: 0.50099\n",
      "Epoch: 00 [13724/27797 ( 49%)], Train Loss: 0.50062\n",
      "Epoch: 00 [13764/27797 ( 50%)], Train Loss: 0.49966\n",
      "Epoch: 00 [13804/27797 ( 50%)], Train Loss: 0.50011\n",
      "Epoch: 00 [13844/27797 ( 50%)], Train Loss: 0.49941\n",
      "Epoch: 00 [13884/27797 ( 50%)], Train Loss: 0.49915\n",
      "Epoch: 00 [13924/27797 ( 50%)], Train Loss: 0.49916\n",
      "Epoch: 00 [13964/27797 ( 50%)], Train Loss: 0.49891\n",
      "Epoch: 00 [14004/27797 ( 50%)], Train Loss: 0.49888\n",
      "Epoch: 00 [14044/27797 ( 51%)], Train Loss: 0.49813\n",
      "Epoch: 00 [14084/27797 ( 51%)], Train Loss: 0.49765\n",
      "Epoch: 00 [14124/27797 ( 51%)], Train Loss: 0.49741\n",
      "Epoch: 00 [14164/27797 ( 51%)], Train Loss: 0.49694\n",
      "Epoch: 00 [14204/27797 ( 51%)], Train Loss: 0.49652\n",
      "Epoch: 00 [14244/27797 ( 51%)], Train Loss: 0.49588\n",
      "Epoch: 00 [14284/27797 ( 51%)], Train Loss: 0.49542\n",
      "Epoch: 00 [14324/27797 ( 52%)], Train Loss: 0.49511\n",
      "Epoch: 00 [14364/27797 ( 52%)], Train Loss: 0.49444\n",
      "Epoch: 00 [14404/27797 ( 52%)], Train Loss: 0.49387\n",
      "Epoch: 00 [14444/27797 ( 52%)], Train Loss: 0.49329\n",
      "Epoch: 00 [14484/27797 ( 52%)], Train Loss: 0.49312\n",
      "Epoch: 00 [14524/27797 ( 52%)], Train Loss: 0.49263\n",
      "Epoch: 00 [14564/27797 ( 52%)], Train Loss: 0.49228\n",
      "Epoch: 00 [14604/27797 ( 53%)], Train Loss: 0.49145\n",
      "Epoch: 00 [14644/27797 ( 53%)], Train Loss: 0.49119\n",
      "Epoch: 00 [14684/27797 ( 53%)], Train Loss: 0.49120\n",
      "Epoch: 00 [14724/27797 ( 53%)], Train Loss: 0.49081\n",
      "Epoch: 00 [14764/27797 ( 53%)], Train Loss: 0.49011\n",
      "Epoch: 00 [14804/27797 ( 53%)], Train Loss: 0.48943\n",
      "Epoch: 00 [14844/27797 ( 53%)], Train Loss: 0.48972\n",
      "Epoch: 00 [14884/27797 ( 54%)], Train Loss: 0.48895\n",
      "Epoch: 00 [14924/27797 ( 54%)], Train Loss: 0.48866\n",
      "Epoch: 00 [14964/27797 ( 54%)], Train Loss: 0.48789\n",
      "Epoch: 00 [15004/27797 ( 54%)], Train Loss: 0.48776\n",
      "Epoch: 00 [15044/27797 ( 54%)], Train Loss: 0.48785\n",
      "Epoch: 00 [15084/27797 ( 54%)], Train Loss: 0.48776\n",
      "Epoch: 00 [15124/27797 ( 54%)], Train Loss: 0.48748\n",
      "Epoch: 00 [15164/27797 ( 55%)], Train Loss: 0.48713\n",
      "Epoch: 00 [15204/27797 ( 55%)], Train Loss: 0.48670\n",
      "Epoch: 00 [15244/27797 ( 55%)], Train Loss: 0.48649\n",
      "Epoch: 00 [15284/27797 ( 55%)], Train Loss: 0.48600\n",
      "Epoch: 00 [15324/27797 ( 55%)], Train Loss: 0.48604\n",
      "Epoch: 00 [15364/27797 ( 55%)], Train Loss: 0.48543\n",
      "Epoch: 00 [15404/27797 ( 55%)], Train Loss: 0.48510\n",
      "Epoch: 00 [15444/27797 ( 56%)], Train Loss: 0.48470\n",
      "Epoch: 00 [15484/27797 ( 56%)], Train Loss: 0.48426\n",
      "Epoch: 00 [15524/27797 ( 56%)], Train Loss: 0.48383\n",
      "Epoch: 00 [15564/27797 ( 56%)], Train Loss: 0.48343\n",
      "Epoch: 00 [15604/27797 ( 56%)], Train Loss: 0.48326\n",
      "Epoch: 00 [15644/27797 ( 56%)], Train Loss: 0.48297\n",
      "Epoch: 00 [15684/27797 ( 56%)], Train Loss: 0.48257\n",
      "Epoch: 00 [15724/27797 ( 57%)], Train Loss: 0.48188\n",
      "Epoch: 00 [15764/27797 ( 57%)], Train Loss: 0.48133\n",
      "Epoch: 00 [15804/27797 ( 57%)], Train Loss: 0.48088\n",
      "Epoch: 00 [15844/27797 ( 57%)], Train Loss: 0.48053\n",
      "Epoch: 00 [15884/27797 ( 57%)], Train Loss: 0.47993\n",
      "Epoch: 00 [15924/27797 ( 57%)], Train Loss: 0.47951\n",
      "Epoch: 00 [15964/27797 ( 57%)], Train Loss: 0.47890\n",
      "Epoch: 00 [16004/27797 ( 58%)], Train Loss: 0.47871\n",
      "Epoch: 00 [16044/27797 ( 58%)], Train Loss: 0.47867\n",
      "Epoch: 00 [16084/27797 ( 58%)], Train Loss: 0.47846\n",
      "Epoch: 00 [16124/27797 ( 58%)], Train Loss: 0.47822\n",
      "Epoch: 00 [16164/27797 ( 58%)], Train Loss: 0.47816\n",
      "Epoch: 00 [16204/27797 ( 58%)], Train Loss: 0.47761\n",
      "Epoch: 00 [16244/27797 ( 58%)], Train Loss: 0.47757\n",
      "Epoch: 00 [16284/27797 ( 59%)], Train Loss: 0.47751\n",
      "Epoch: 00 [16324/27797 ( 59%)], Train Loss: 0.47710\n",
      "Epoch: 00 [16364/27797 ( 59%)], Train Loss: 0.47653\n",
      "Epoch: 00 [16404/27797 ( 59%)], Train Loss: 0.47616\n",
      "Epoch: 00 [16444/27797 ( 59%)], Train Loss: 0.47593\n",
      "Epoch: 00 [16484/27797 ( 59%)], Train Loss: 0.47545\n",
      "Epoch: 00 [16524/27797 ( 59%)], Train Loss: 0.47527\n",
      "Epoch: 00 [16564/27797 ( 60%)], Train Loss: 0.47516\n",
      "Epoch: 00 [16604/27797 ( 60%)], Train Loss: 0.47487\n",
      "Epoch: 00 [16644/27797 ( 60%)], Train Loss: 0.47436\n",
      "Epoch: 00 [16684/27797 ( 60%)], Train Loss: 0.47397\n",
      "Epoch: 00 [16724/27797 ( 60%)], Train Loss: 0.47361\n",
      "Epoch: 00 [16764/27797 ( 60%)], Train Loss: 0.47299\n",
      "Epoch: 00 [16804/27797 ( 60%)], Train Loss: 0.47286\n",
      "Epoch: 00 [16844/27797 ( 61%)], Train Loss: 0.47240\n",
      "Epoch: 00 [16884/27797 ( 61%)], Train Loss: 0.47208\n",
      "Epoch: 00 [16924/27797 ( 61%)], Train Loss: 0.47173\n",
      "Epoch: 00 [16964/27797 ( 61%)], Train Loss: 0.47119\n",
      "Epoch: 00 [17004/27797 ( 61%)], Train Loss: 0.47068\n",
      "Epoch: 00 [17044/27797 ( 61%)], Train Loss: 0.47062\n",
      "Epoch: 00 [17084/27797 ( 61%)], Train Loss: 0.46994\n",
      "Epoch: 00 [17124/27797 ( 62%)], Train Loss: 0.46962\n",
      "Epoch: 00 [17164/27797 ( 62%)], Train Loss: 0.46912\n",
      "Epoch: 00 [17204/27797 ( 62%)], Train Loss: 0.46912\n",
      "Epoch: 00 [17244/27797 ( 62%)], Train Loss: 0.46924\n",
      "Epoch: 00 [17284/27797 ( 62%)], Train Loss: 0.46874\n",
      "Epoch: 00 [17324/27797 ( 62%)], Train Loss: 0.46819\n",
      "Epoch: 00 [17364/27797 ( 62%)], Train Loss: 0.46773\n",
      "Epoch: 00 [17404/27797 ( 63%)], Train Loss: 0.46710\n",
      "Epoch: 00 [17444/27797 ( 63%)], Train Loss: 0.46700\n",
      "Epoch: 00 [17484/27797 ( 63%)], Train Loss: 0.46685\n",
      "Epoch: 00 [17524/27797 ( 63%)], Train Loss: 0.46615\n",
      "Epoch: 00 [17564/27797 ( 63%)], Train Loss: 0.46566\n",
      "Epoch: 00 [17604/27797 ( 63%)], Train Loss: 0.46516\n",
      "Epoch: 00 [17644/27797 ( 63%)], Train Loss: 0.46501\n",
      "Epoch: 00 [17684/27797 ( 64%)], Train Loss: 0.46485\n",
      "Epoch: 00 [17724/27797 ( 64%)], Train Loss: 0.46498\n",
      "Epoch: 00 [17764/27797 ( 64%)], Train Loss: 0.46470\n",
      "Epoch: 00 [17804/27797 ( 64%)], Train Loss: 0.46414\n",
      "Epoch: 00 [17844/27797 ( 64%)], Train Loss: 0.46395\n",
      "Epoch: 00 [17884/27797 ( 64%)], Train Loss: 0.46370\n",
      "Epoch: 00 [17924/27797 ( 64%)], Train Loss: 0.46343\n",
      "Epoch: 00 [17964/27797 ( 65%)], Train Loss: 0.46340\n",
      "Epoch: 00 [18004/27797 ( 65%)], Train Loss: 0.46291\n",
      "Epoch: 00 [18044/27797 ( 65%)], Train Loss: 0.46233\n",
      "Epoch: 00 [18084/27797 ( 65%)], Train Loss: 0.46255\n",
      "Epoch: 00 [18124/27797 ( 65%)], Train Loss: 0.46237\n",
      "Epoch: 00 [18164/27797 ( 65%)], Train Loss: 0.46202\n",
      "Epoch: 00 [18204/27797 ( 65%)], Train Loss: 0.46153\n",
      "Epoch: 00 [18244/27797 ( 66%)], Train Loss: 0.46129\n",
      "Epoch: 00 [18284/27797 ( 66%)], Train Loss: 0.46073\n",
      "Epoch: 00 [18324/27797 ( 66%)], Train Loss: 0.46025\n",
      "Epoch: 00 [18364/27797 ( 66%)], Train Loss: 0.46013\n",
      "Epoch: 00 [18404/27797 ( 66%)], Train Loss: 0.46001\n",
      "Epoch: 00 [18444/27797 ( 66%)], Train Loss: 0.45955\n",
      "Epoch: 00 [18484/27797 ( 66%)], Train Loss: 0.45901\n",
      "Epoch: 00 [18524/27797 ( 67%)], Train Loss: 0.45856\n",
      "Epoch: 00 [18564/27797 ( 67%)], Train Loss: 0.45819\n",
      "Epoch: 00 [18604/27797 ( 67%)], Train Loss: 0.45752\n",
      "Epoch: 00 [18644/27797 ( 67%)], Train Loss: 0.45704\n",
      "Epoch: 00 [18684/27797 ( 67%)], Train Loss: 0.45723\n",
      "Epoch: 00 [18724/27797 ( 67%)], Train Loss: 0.45700\n",
      "Epoch: 00 [18764/27797 ( 68%)], Train Loss: 0.45661\n",
      "Epoch: 00 [18804/27797 ( 68%)], Train Loss: 0.45684\n",
      "Epoch: 00 [18844/27797 ( 68%)], Train Loss: 0.45698\n",
      "Epoch: 00 [18884/27797 ( 68%)], Train Loss: 0.45636\n",
      "Epoch: 00 [18924/27797 ( 68%)], Train Loss: 0.45607\n",
      "Epoch: 00 [18964/27797 ( 68%)], Train Loss: 0.45593\n",
      "Epoch: 00 [19004/27797 ( 68%)], Train Loss: 0.45555\n",
      "Epoch: 00 [19044/27797 ( 69%)], Train Loss: 0.45535\n",
      "Epoch: 00 [19084/27797 ( 69%)], Train Loss: 0.45491\n",
      "Epoch: 00 [19124/27797 ( 69%)], Train Loss: 0.45449\n",
      "Epoch: 00 [19164/27797 ( 69%)], Train Loss: 0.45420\n",
      "Epoch: 00 [19204/27797 ( 69%)], Train Loss: 0.45366\n",
      "Epoch: 00 [19244/27797 ( 69%)], Train Loss: 0.45332\n",
      "Epoch: 00 [19284/27797 ( 69%)], Train Loss: 0.45295\n",
      "Epoch: 00 [19324/27797 ( 70%)], Train Loss: 0.45277\n",
      "Epoch: 00 [19364/27797 ( 70%)], Train Loss: 0.45259\n",
      "Epoch: 00 [19404/27797 ( 70%)], Train Loss: 0.45256\n",
      "Epoch: 00 [19444/27797 ( 70%)], Train Loss: 0.45226\n",
      "Epoch: 00 [19484/27797 ( 70%)], Train Loss: 0.45173\n",
      "Epoch: 00 [19524/27797 ( 70%)], Train Loss: 0.45145\n",
      "Epoch: 00 [19564/27797 ( 70%)], Train Loss: 0.45120\n",
      "Epoch: 00 [19604/27797 ( 71%)], Train Loss: 0.45111\n",
      "Epoch: 00 [19644/27797 ( 71%)], Train Loss: 0.45073\n",
      "Epoch: 00 [19684/27797 ( 71%)], Train Loss: 0.45053\n",
      "Epoch: 00 [19724/27797 ( 71%)], Train Loss: 0.45021\n",
      "Epoch: 00 [19764/27797 ( 71%)], Train Loss: 0.44982\n",
      "Epoch: 00 [19804/27797 ( 71%)], Train Loss: 0.44934\n",
      "Epoch: 00 [19844/27797 ( 71%)], Train Loss: 0.44897\n",
      "Epoch: 00 [19884/27797 ( 72%)], Train Loss: 0.44884\n",
      "Epoch: 00 [19924/27797 ( 72%)], Train Loss: 0.44837\n",
      "Epoch: 00 [19964/27797 ( 72%)], Train Loss: 0.44794\n",
      "Epoch: 00 [20004/27797 ( 72%)], Train Loss: 0.44752\n",
      "Epoch: 00 [20044/27797 ( 72%)], Train Loss: 0.44737\n",
      "Epoch: 00 [20084/27797 ( 72%)], Train Loss: 0.44703\n",
      "Epoch: 00 [20124/27797 ( 72%)], Train Loss: 0.44667\n",
      "Epoch: 00 [20164/27797 ( 73%)], Train Loss: 0.44637\n",
      "Epoch: 00 [20204/27797 ( 73%)], Train Loss: 0.44619\n",
      "Epoch: 00 [20244/27797 ( 73%)], Train Loss: 0.44609\n",
      "Epoch: 00 [20284/27797 ( 73%)], Train Loss: 0.44612\n",
      "Epoch: 00 [20324/27797 ( 73%)], Train Loss: 0.44586\n",
      "Epoch: 00 [20364/27797 ( 73%)], Train Loss: 0.44574\n",
      "Epoch: 00 [20404/27797 ( 73%)], Train Loss: 0.44563\n",
      "Epoch: 00 [20444/27797 ( 74%)], Train Loss: 0.44561\n",
      "Epoch: 00 [20484/27797 ( 74%)], Train Loss: 0.44506\n",
      "Epoch: 00 [20524/27797 ( 74%)], Train Loss: 0.44478\n",
      "Epoch: 00 [20564/27797 ( 74%)], Train Loss: 0.44428\n",
      "Epoch: 00 [20604/27797 ( 74%)], Train Loss: 0.44385\n",
      "Epoch: 00 [20644/27797 ( 74%)], Train Loss: 0.44368\n",
      "Epoch: 00 [20684/27797 ( 74%)], Train Loss: 0.44325\n",
      "Epoch: 00 [20724/27797 ( 75%)], Train Loss: 0.44290\n",
      "Epoch: 00 [20764/27797 ( 75%)], Train Loss: 0.44278\n",
      "Epoch: 00 [20804/27797 ( 75%)], Train Loss: 0.44274\n",
      "Epoch: 00 [20844/27797 ( 75%)], Train Loss: 0.44263\n",
      "Epoch: 00 [20884/27797 ( 75%)], Train Loss: 0.44220\n",
      "Epoch: 00 [20924/27797 ( 75%)], Train Loss: 0.44194\n",
      "Epoch: 00 [20964/27797 ( 75%)], Train Loss: 0.44174\n",
      "Epoch: 00 [21004/27797 ( 76%)], Train Loss: 0.44142\n",
      "Epoch: 00 [21044/27797 ( 76%)], Train Loss: 0.44110\n",
      "Epoch: 00 [21084/27797 ( 76%)], Train Loss: 0.44077\n",
      "Epoch: 00 [21124/27797 ( 76%)], Train Loss: 0.44059\n",
      "Epoch: 00 [21164/27797 ( 76%)], Train Loss: 0.44016\n",
      "Epoch: 00 [21204/27797 ( 76%)], Train Loss: 0.43985\n",
      "Epoch: 00 [21244/27797 ( 76%)], Train Loss: 0.43973\n",
      "Epoch: 00 [21284/27797 ( 77%)], Train Loss: 0.43933\n",
      "Epoch: 00 [21324/27797 ( 77%)], Train Loss: 0.43922\n",
      "Epoch: 00 [21364/27797 ( 77%)], Train Loss: 0.43894\n",
      "Epoch: 00 [21404/27797 ( 77%)], Train Loss: 0.43874\n",
      "Epoch: 00 [21444/27797 ( 77%)], Train Loss: 0.43826\n",
      "Epoch: 00 [21484/27797 ( 77%)], Train Loss: 0.43823\n",
      "Epoch: 00 [21524/27797 ( 77%)], Train Loss: 0.43789\n",
      "Epoch: 00 [21564/27797 ( 78%)], Train Loss: 0.43770\n",
      "Epoch: 00 [21604/27797 ( 78%)], Train Loss: 0.43725\n",
      "Epoch: 00 [21644/27797 ( 78%)], Train Loss: 0.43699\n",
      "Epoch: 00 [21684/27797 ( 78%)], Train Loss: 0.43670\n",
      "Epoch: 00 [21724/27797 ( 78%)], Train Loss: 0.43613\n",
      "Epoch: 00 [21764/27797 ( 78%)], Train Loss: 0.43592\n",
      "Epoch: 00 [21804/27797 ( 78%)], Train Loss: 0.43576\n",
      "Epoch: 00 [21844/27797 ( 79%)], Train Loss: 0.43558\n",
      "Epoch: 00 [21884/27797 ( 79%)], Train Loss: 0.43520\n",
      "Epoch: 00 [21924/27797 ( 79%)], Train Loss: 0.43468\n",
      "Epoch: 00 [21964/27797 ( 79%)], Train Loss: 0.43453\n",
      "Epoch: 00 [22004/27797 ( 79%)], Train Loss: 0.43418\n",
      "Epoch: 00 [22044/27797 ( 79%)], Train Loss: 0.43370\n",
      "Epoch: 00 [22084/27797 ( 79%)], Train Loss: 0.43346\n",
      "Epoch: 00 [22124/27797 ( 80%)], Train Loss: 0.43332\n",
      "Epoch: 00 [22164/27797 ( 80%)], Train Loss: 0.43317\n",
      "Epoch: 00 [22204/27797 ( 80%)], Train Loss: 0.43301\n",
      "Epoch: 00 [22244/27797 ( 80%)], Train Loss: 0.43288\n",
      "Epoch: 00 [22284/27797 ( 80%)], Train Loss: 0.43257\n",
      "Epoch: 00 [22324/27797 ( 80%)], Train Loss: 0.43228\n",
      "Epoch: 00 [22364/27797 ( 80%)], Train Loss: 0.43191\n",
      "Epoch: 00 [22404/27797 ( 81%)], Train Loss: 0.43167\n",
      "Epoch: 00 [22444/27797 ( 81%)], Train Loss: 0.43146\n",
      "Epoch: 00 [22484/27797 ( 81%)], Train Loss: 0.43136\n",
      "Epoch: 00 [22524/27797 ( 81%)], Train Loss: 0.43104\n",
      "Epoch: 00 [22564/27797 ( 81%)], Train Loss: 0.43089\n",
      "Epoch: 00 [22604/27797 ( 81%)], Train Loss: 0.43083\n",
      "Epoch: 00 [22644/27797 ( 81%)], Train Loss: 0.43100\n",
      "Epoch: 00 [22684/27797 ( 82%)], Train Loss: 0.43072\n",
      "Epoch: 00 [22724/27797 ( 82%)], Train Loss: 0.43027\n",
      "Epoch: 00 [22764/27797 ( 82%)], Train Loss: 0.43012\n",
      "Epoch: 00 [22804/27797 ( 82%)], Train Loss: 0.42989\n",
      "Epoch: 00 [22844/27797 ( 82%)], Train Loss: 0.42951\n",
      "Epoch: 00 [22884/27797 ( 82%)], Train Loss: 0.42932\n",
      "Epoch: 00 [22924/27797 ( 82%)], Train Loss: 0.42901\n",
      "Epoch: 00 [22964/27797 ( 83%)], Train Loss: 0.42888\n",
      "Epoch: 00 [23004/27797 ( 83%)], Train Loss: 0.42850\n",
      "Epoch: 00 [23044/27797 ( 83%)], Train Loss: 0.42826\n",
      "Epoch: 00 [23084/27797 ( 83%)], Train Loss: 0.42823\n",
      "Epoch: 00 [23124/27797 ( 83%)], Train Loss: 0.42795\n",
      "Epoch: 00 [23164/27797 ( 83%)], Train Loss: 0.42807\n",
      "Epoch: 00 [23204/27797 ( 83%)], Train Loss: 0.42788\n",
      "Epoch: 00 [23244/27797 ( 84%)], Train Loss: 0.42771\n",
      "Epoch: 00 [23284/27797 ( 84%)], Train Loss: 0.42771\n",
      "Epoch: 00 [23324/27797 ( 84%)], Train Loss: 0.42755\n",
      "Epoch: 00 [23364/27797 ( 84%)], Train Loss: 0.42728\n",
      "Epoch: 00 [23404/27797 ( 84%)], Train Loss: 0.42682\n",
      "Epoch: 00 [23444/27797 ( 84%)], Train Loss: 0.42662\n",
      "Epoch: 00 [23484/27797 ( 84%)], Train Loss: 0.42628\n",
      "Epoch: 00 [23524/27797 ( 85%)], Train Loss: 0.42598\n",
      "Epoch: 00 [23564/27797 ( 85%)], Train Loss: 0.42569\n",
      "Epoch: 00 [23604/27797 ( 85%)], Train Loss: 0.42544\n",
      "Epoch: 00 [23644/27797 ( 85%)], Train Loss: 0.42516\n",
      "Epoch: 00 [23684/27797 ( 85%)], Train Loss: 0.42478\n",
      "Epoch: 00 [23724/27797 ( 85%)], Train Loss: 0.42462\n",
      "Epoch: 00 [23764/27797 ( 85%)], Train Loss: 0.42439\n",
      "Epoch: 00 [23804/27797 ( 86%)], Train Loss: 0.42418\n",
      "Epoch: 00 [23844/27797 ( 86%)], Train Loss: 0.42410\n",
      "Epoch: 00 [23884/27797 ( 86%)], Train Loss: 0.42384\n",
      "Epoch: 00 [23924/27797 ( 86%)], Train Loss: 0.42364\n",
      "Epoch: 00 [23964/27797 ( 86%)], Train Loss: 0.42328\n",
      "Epoch: 00 [24004/27797 ( 86%)], Train Loss: 0.42291\n",
      "Epoch: 00 [24044/27797 ( 86%)], Train Loss: 0.42284\n",
      "Epoch: 00 [24084/27797 ( 87%)], Train Loss: 0.42267\n",
      "Epoch: 00 [24124/27797 ( 87%)], Train Loss: 0.42273\n",
      "Epoch: 00 [24164/27797 ( 87%)], Train Loss: 0.42236\n",
      "Epoch: 00 [24204/27797 ( 87%)], Train Loss: 0.42196\n",
      "Epoch: 00 [24244/27797 ( 87%)], Train Loss: 0.42155\n",
      "Epoch: 00 [24284/27797 ( 87%)], Train Loss: 0.42117\n",
      "Epoch: 00 [24324/27797 ( 88%)], Train Loss: 0.42103\n",
      "Epoch: 00 [24364/27797 ( 88%)], Train Loss: 0.42058\n",
      "Epoch: 00 [24404/27797 ( 88%)], Train Loss: 0.42019\n",
      "Epoch: 00 [24444/27797 ( 88%)], Train Loss: 0.41999\n",
      "Epoch: 00 [24484/27797 ( 88%)], Train Loss: 0.41993\n",
      "Epoch: 00 [24524/27797 ( 88%)], Train Loss: 0.41957\n",
      "Epoch: 00 [24564/27797 ( 88%)], Train Loss: 0.41938\n",
      "Epoch: 00 [24604/27797 ( 89%)], Train Loss: 0.41931\n",
      "Epoch: 00 [24644/27797 ( 89%)], Train Loss: 0.41934\n",
      "Epoch: 00 [24684/27797 ( 89%)], Train Loss: 0.41902\n",
      "Epoch: 00 [24724/27797 ( 89%)], Train Loss: 0.41864\n",
      "Epoch: 00 [24764/27797 ( 89%)], Train Loss: 0.41846\n",
      "Epoch: 00 [24804/27797 ( 89%)], Train Loss: 0.41860\n",
      "Epoch: 00 [24844/27797 ( 89%)], Train Loss: 0.41843\n",
      "Epoch: 00 [24884/27797 ( 90%)], Train Loss: 0.41848\n",
      "Epoch: 00 [24924/27797 ( 90%)], Train Loss: 0.41840\n",
      "Epoch: 00 [24964/27797 ( 90%)], Train Loss: 0.41827\n",
      "Epoch: 00 [25004/27797 ( 90%)], Train Loss: 0.41792\n",
      "Epoch: 00 [25044/27797 ( 90%)], Train Loss: 0.41752\n",
      "Epoch: 00 [25084/27797 ( 90%)], Train Loss: 0.41752\n",
      "Epoch: 00 [25124/27797 ( 90%)], Train Loss: 0.41730\n",
      "Epoch: 00 [25164/27797 ( 91%)], Train Loss: 0.41690\n",
      "Epoch: 00 [25204/27797 ( 91%)], Train Loss: 0.41667\n",
      "Epoch: 00 [25244/27797 ( 91%)], Train Loss: 0.41634\n",
      "Epoch: 00 [25284/27797 ( 91%)], Train Loss: 0.41628\n",
      "Epoch: 00 [25324/27797 ( 91%)], Train Loss: 0.41607\n",
      "Epoch: 00 [25364/27797 ( 91%)], Train Loss: 0.41583\n",
      "Epoch: 00 [25404/27797 ( 91%)], Train Loss: 0.41590\n",
      "Epoch: 00 [25444/27797 ( 92%)], Train Loss: 0.41600\n",
      "Epoch: 00 [25484/27797 ( 92%)], Train Loss: 0.41579\n",
      "Epoch: 00 [25524/27797 ( 92%)], Train Loss: 0.41567\n",
      "Epoch: 00 [25564/27797 ( 92%)], Train Loss: 0.41546\n",
      "Epoch: 00 [25604/27797 ( 92%)], Train Loss: 0.41525\n",
      "Epoch: 00 [25644/27797 ( 92%)], Train Loss: 0.41504\n",
      "Epoch: 00 [25684/27797 ( 92%)], Train Loss: 0.41475\n",
      "Epoch: 00 [25724/27797 ( 93%)], Train Loss: 0.41454\n",
      "Epoch: 00 [25764/27797 ( 93%)], Train Loss: 0.41424\n",
      "Epoch: 00 [25804/27797 ( 93%)], Train Loss: 0.41416\n",
      "Epoch: 00 [25844/27797 ( 93%)], Train Loss: 0.41395\n",
      "Epoch: 00 [25884/27797 ( 93%)], Train Loss: 0.41392\n",
      "Epoch: 00 [25924/27797 ( 93%)], Train Loss: 0.41368\n",
      "Epoch: 00 [25964/27797 ( 93%)], Train Loss: 0.41365\n",
      "Epoch: 00 [26004/27797 ( 94%)], Train Loss: 0.41333\n",
      "Epoch: 00 [26044/27797 ( 94%)], Train Loss: 0.41336\n",
      "Epoch: 00 [26084/27797 ( 94%)], Train Loss: 0.41315\n",
      "Epoch: 00 [26124/27797 ( 94%)], Train Loss: 0.41308\n",
      "Epoch: 00 [26164/27797 ( 94%)], Train Loss: 0.41292\n",
      "Epoch: 00 [26204/27797 ( 94%)], Train Loss: 0.41262\n",
      "Epoch: 00 [26244/27797 ( 94%)], Train Loss: 0.41262\n",
      "Epoch: 00 [26284/27797 ( 95%)], Train Loss: 0.41249\n",
      "Epoch: 00 [26324/27797 ( 95%)], Train Loss: 0.41238\n",
      "Epoch: 00 [26364/27797 ( 95%)], Train Loss: 0.41227\n",
      "Epoch: 00 [26404/27797 ( 95%)], Train Loss: 0.41249\n",
      "Epoch: 00 [26444/27797 ( 95%)], Train Loss: 0.41218\n",
      "Epoch: 00 [26484/27797 ( 95%)], Train Loss: 0.41189\n",
      "Epoch: 00 [26524/27797 ( 95%)], Train Loss: 0.41157\n",
      "Epoch: 00 [26564/27797 ( 96%)], Train Loss: 0.41129\n",
      "Epoch: 00 [26604/27797 ( 96%)], Train Loss: 0.41114\n",
      "Epoch: 00 [26644/27797 ( 96%)], Train Loss: 0.41094\n",
      "Epoch: 00 [26684/27797 ( 96%)], Train Loss: 0.41073\n",
      "Epoch: 00 [26724/27797 ( 96%)], Train Loss: 0.41039\n",
      "Epoch: 00 [26764/27797 ( 96%)], Train Loss: 0.41025\n",
      "Epoch: 00 [26804/27797 ( 96%)], Train Loss: 0.41006\n",
      "Epoch: 00 [26844/27797 ( 97%)], Train Loss: 0.40982\n",
      "Epoch: 00 [26884/27797 ( 97%)], Train Loss: 0.40987\n",
      "Epoch: 00 [26924/27797 ( 97%)], Train Loss: 0.40964\n",
      "Epoch: 00 [26964/27797 ( 97%)], Train Loss: 0.40940\n",
      "Epoch: 00 [27004/27797 ( 97%)], Train Loss: 0.40926\n",
      "Epoch: 00 [27044/27797 ( 97%)], Train Loss: 0.40896\n",
      "Epoch: 00 [27084/27797 ( 97%)], Train Loss: 0.40878\n",
      "Epoch: 00 [27124/27797 ( 98%)], Train Loss: 0.40842\n",
      "Epoch: 00 [27164/27797 ( 98%)], Train Loss: 0.40849\n",
      "Epoch: 00 [27204/27797 ( 98%)], Train Loss: 0.40818\n",
      "Epoch: 00 [27244/27797 ( 98%)], Train Loss: 0.40801\n",
      "Epoch: 00 [27284/27797 ( 98%)], Train Loss: 0.40803\n",
      "Epoch: 00 [27324/27797 ( 98%)], Train Loss: 0.40775\n",
      "Epoch: 00 [27364/27797 ( 98%)], Train Loss: 0.40753\n",
      "Epoch: 00 [27404/27797 ( 99%)], Train Loss: 0.40728\n",
      "Epoch: 00 [27444/27797 ( 99%)], Train Loss: 0.40707\n",
      "Epoch: 00 [27484/27797 ( 99%)], Train Loss: 0.40690\n",
      "Epoch: 00 [27524/27797 ( 99%)], Train Loss: 0.40663\n",
      "Epoch: 00 [27564/27797 ( 99%)], Train Loss: 0.40622\n",
      "Epoch: 00 [27604/27797 ( 99%)], Train Loss: 0.40591\n",
      "Epoch: 00 [27644/27797 ( 99%)], Train Loss: 0.40587\n",
      "Epoch: 00 [27684/27797 (100%)], Train Loss: 0.40604\n",
      "Epoch: 00 [27724/27797 (100%)], Train Loss: 0.40611\n",
      "Epoch: 00 [27764/27797 (100%)], Train Loss: 0.40592\n",
      "Epoch: 00 [27797/27797 (100%)], Train Loss: 0.40577\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.56154\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.56154\n",
      "Saving model checkpoint to output/checkpoint-fold-2.\n",
      "\n",
      "Total Training Time: 3235.651193857193secs, Average Training Time per Epoch: 3235.651193857193secs.\n",
      "Total Validation Time: 140.51539254188538secs, Average Validation Time per Epoch: 140.51539254188538secs.\n",
      "--------------------------------------------------\n",
      "FOLD: 3\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27660, Num examples Valid=4062\n",
      "Total Training Steps: 3458, Total Warmup Steps: 345\n",
      "Epoch: 00 [    4/27660 (  0%)], Train Loss: 2.82564\n",
      "Epoch: 00 [   44/27660 (  0%)], Train Loss: 2.80195\n",
      "Epoch: 00 [   84/27660 (  0%)], Train Loss: 2.77106\n",
      "Epoch: 00 [  124/27660 (  0%)], Train Loss: 2.75991\n",
      "Epoch: 00 [  164/27660 (  1%)], Train Loss: 2.73324\n",
      "Epoch: 00 [  204/27660 (  1%)], Train Loss: 2.69816\n",
      "Epoch: 00 [  244/27660 (  1%)], Train Loss: 2.65722\n",
      "Epoch: 00 [  284/27660 (  1%)], Train Loss: 2.61047\n",
      "Epoch: 00 [  324/27660 (  1%)], Train Loss: 2.54670\n",
      "Epoch: 00 [  364/27660 (  1%)], Train Loss: 2.48418\n",
      "Epoch: 00 [  404/27660 (  1%)], Train Loss: 2.40917\n",
      "Epoch: 00 [  444/27660 (  2%)], Train Loss: 2.32770\n",
      "Epoch: 00 [  484/27660 (  2%)], Train Loss: 2.24747\n",
      "Epoch: 00 [  524/27660 (  2%)], Train Loss: 2.16086\n",
      "Epoch: 00 [  564/27660 (  2%)], Train Loss: 2.06741\n",
      "Epoch: 00 [  604/27660 (  2%)], Train Loss: 2.00114\n",
      "Epoch: 00 [  644/27660 (  2%)], Train Loss: 1.91214\n",
      "Epoch: 00 [  684/27660 (  2%)], Train Loss: 1.83594\n",
      "Epoch: 00 [  724/27660 (  3%)], Train Loss: 1.76730\n",
      "Epoch: 00 [  764/27660 (  3%)], Train Loss: 1.69999\n",
      "Epoch: 00 [  804/27660 (  3%)], Train Loss: 1.65177\n",
      "Epoch: 00 [  844/27660 (  3%)], Train Loss: 1.60628\n",
      "Epoch: 00 [  884/27660 (  3%)], Train Loss: 1.56033\n",
      "Epoch: 00 [  924/27660 (  3%)], Train Loss: 1.51854\n",
      "Epoch: 00 [  964/27660 (  3%)], Train Loss: 1.48140\n",
      "Epoch: 00 [ 1004/27660 (  4%)], Train Loss: 1.43871\n",
      "Epoch: 00 [ 1044/27660 (  4%)], Train Loss: 1.40218\n",
      "Epoch: 00 [ 1084/27660 (  4%)], Train Loss: 1.36536\n",
      "Epoch: 00 [ 1124/27660 (  4%)], Train Loss: 1.33344\n",
      "Epoch: 00 [ 1164/27660 (  4%)], Train Loss: 1.30782\n",
      "Epoch: 00 [ 1204/27660 (  4%)], Train Loss: 1.27993\n",
      "Epoch: 00 [ 1244/27660 (  4%)], Train Loss: 1.25274\n",
      "Epoch: 00 [ 1284/27660 (  5%)], Train Loss: 1.23062\n",
      "Epoch: 00 [ 1324/27660 (  5%)], Train Loss: 1.20719\n",
      "Epoch: 00 [ 1364/27660 (  5%)], Train Loss: 1.19121\n",
      "Epoch: 00 [ 1404/27660 (  5%)], Train Loss: 1.16804\n",
      "Epoch: 00 [ 1444/27660 (  5%)], Train Loss: 1.14705\n",
      "Epoch: 00 [ 1484/27660 (  5%)], Train Loss: 1.12803\n",
      "Epoch: 00 [ 1524/27660 (  6%)], Train Loss: 1.10867\n",
      "Epoch: 00 [ 1564/27660 (  6%)], Train Loss: 1.09688\n",
      "Epoch: 00 [ 1604/27660 (  6%)], Train Loss: 1.08625\n",
      "Epoch: 00 [ 1644/27660 (  6%)], Train Loss: 1.07087\n",
      "Epoch: 00 [ 1684/27660 (  6%)], Train Loss: 1.05481\n",
      "Epoch: 00 [ 1724/27660 (  6%)], Train Loss: 1.03729\n",
      "Epoch: 00 [ 1764/27660 (  6%)], Train Loss: 1.02551\n",
      "Epoch: 00 [ 1804/27660 (  7%)], Train Loss: 1.01177\n",
      "Epoch: 00 [ 1844/27660 (  7%)], Train Loss: 1.00386\n",
      "Epoch: 00 [ 1884/27660 (  7%)], Train Loss: 0.99570\n",
      "Epoch: 00 [ 1924/27660 (  7%)], Train Loss: 0.98476\n",
      "Epoch: 00 [ 1964/27660 (  7%)], Train Loss: 0.97380\n",
      "Epoch: 00 [ 2004/27660 (  7%)], Train Loss: 0.96105\n",
      "Epoch: 00 [ 2044/27660 (  7%)], Train Loss: 0.95377\n",
      "Epoch: 00 [ 2084/27660 (  8%)], Train Loss: 0.94324\n",
      "Epoch: 00 [ 2124/27660 (  8%)], Train Loss: 0.93577\n",
      "Epoch: 00 [ 2164/27660 (  8%)], Train Loss: 0.92744\n",
      "Epoch: 00 [ 2204/27660 (  8%)], Train Loss: 0.91999\n",
      "Epoch: 00 [ 2244/27660 (  8%)], Train Loss: 0.90981\n",
      "Epoch: 00 [ 2284/27660 (  8%)], Train Loss: 0.90191\n",
      "Epoch: 00 [ 2324/27660 (  8%)], Train Loss: 0.89669\n",
      "Epoch: 00 [ 2364/27660 (  9%)], Train Loss: 0.88806\n",
      "Epoch: 00 [ 2404/27660 (  9%)], Train Loss: 0.87955\n",
      "Epoch: 00 [ 2444/27660 (  9%)], Train Loss: 0.87315\n",
      "Epoch: 00 [ 2484/27660 (  9%)], Train Loss: 0.86837\n",
      "Epoch: 00 [ 2524/27660 (  9%)], Train Loss: 0.86213\n",
      "Epoch: 00 [ 2564/27660 (  9%)], Train Loss: 0.85504\n",
      "Epoch: 00 [ 2604/27660 (  9%)], Train Loss: 0.84769\n",
      "Epoch: 00 [ 2644/27660 ( 10%)], Train Loss: 0.84197\n",
      "Epoch: 00 [ 2684/27660 ( 10%)], Train Loss: 0.83805\n",
      "Epoch: 00 [ 2724/27660 ( 10%)], Train Loss: 0.83225\n",
      "Epoch: 00 [ 2764/27660 ( 10%)], Train Loss: 0.82941\n",
      "Epoch: 00 [ 2804/27660 ( 10%)], Train Loss: 0.82665\n",
      "Epoch: 00 [ 2844/27660 ( 10%)], Train Loss: 0.82203\n",
      "Epoch: 00 [ 2884/27660 ( 10%)], Train Loss: 0.81560\n",
      "Epoch: 00 [ 2924/27660 ( 11%)], Train Loss: 0.81067\n",
      "Epoch: 00 [ 2964/27660 ( 11%)], Train Loss: 0.80678\n",
      "Epoch: 00 [ 3004/27660 ( 11%)], Train Loss: 0.80176\n",
      "Epoch: 00 [ 3044/27660 ( 11%)], Train Loss: 0.79710\n",
      "Epoch: 00 [ 3084/27660 ( 11%)], Train Loss: 0.79527\n",
      "Epoch: 00 [ 3124/27660 ( 11%)], Train Loss: 0.79253\n",
      "Epoch: 00 [ 3164/27660 ( 11%)], Train Loss: 0.78871\n",
      "Epoch: 00 [ 3204/27660 ( 12%)], Train Loss: 0.78435\n",
      "Epoch: 00 [ 3244/27660 ( 12%)], Train Loss: 0.78288\n",
      "Epoch: 00 [ 3284/27660 ( 12%)], Train Loss: 0.77824\n",
      "Epoch: 00 [ 3324/27660 ( 12%)], Train Loss: 0.77549\n",
      "Epoch: 00 [ 3364/27660 ( 12%)], Train Loss: 0.77295\n",
      "Epoch: 00 [ 3404/27660 ( 12%)], Train Loss: 0.77166\n",
      "Epoch: 00 [ 3444/27660 ( 12%)], Train Loss: 0.76759\n",
      "Epoch: 00 [ 3484/27660 ( 13%)], Train Loss: 0.76368\n",
      "Epoch: 00 [ 3524/27660 ( 13%)], Train Loss: 0.76036\n",
      "Epoch: 00 [ 3564/27660 ( 13%)], Train Loss: 0.75871\n",
      "Epoch: 00 [ 3604/27660 ( 13%)], Train Loss: 0.75349\n",
      "Epoch: 00 [ 3644/27660 ( 13%)], Train Loss: 0.75076\n",
      "Epoch: 00 [ 3684/27660 ( 13%)], Train Loss: 0.74780\n",
      "Epoch: 00 [ 3724/27660 ( 13%)], Train Loss: 0.74281\n",
      "Epoch: 00 [ 3764/27660 ( 14%)], Train Loss: 0.73971\n",
      "Epoch: 00 [ 3804/27660 ( 14%)], Train Loss: 0.73711\n",
      "Epoch: 00 [ 3844/27660 ( 14%)], Train Loss: 0.73250\n",
      "Epoch: 00 [ 3884/27660 ( 14%)], Train Loss: 0.72766\n",
      "Epoch: 00 [ 3924/27660 ( 14%)], Train Loss: 0.72525\n",
      "Epoch: 00 [ 3964/27660 ( 14%)], Train Loss: 0.72165\n",
      "Epoch: 00 [ 4004/27660 ( 14%)], Train Loss: 0.71784\n",
      "Epoch: 00 [ 4044/27660 ( 15%)], Train Loss: 0.71503\n",
      "Epoch: 00 [ 4084/27660 ( 15%)], Train Loss: 0.71051\n",
      "Epoch: 00 [ 4124/27660 ( 15%)], Train Loss: 0.71022\n",
      "Epoch: 00 [ 4164/27660 ( 15%)], Train Loss: 0.70695\n",
      "Epoch: 00 [ 4204/27660 ( 15%)], Train Loss: 0.70482\n",
      "Epoch: 00 [ 4244/27660 ( 15%)], Train Loss: 0.70185\n",
      "Epoch: 00 [ 4284/27660 ( 15%)], Train Loss: 0.69828\n",
      "Epoch: 00 [ 4324/27660 ( 16%)], Train Loss: 0.69665\n",
      "Epoch: 00 [ 4364/27660 ( 16%)], Train Loss: 0.69509\n",
      "Epoch: 00 [ 4404/27660 ( 16%)], Train Loss: 0.69275\n",
      "Epoch: 00 [ 4444/27660 ( 16%)], Train Loss: 0.68936\n",
      "Epoch: 00 [ 4484/27660 ( 16%)], Train Loss: 0.68849\n",
      "Epoch: 00 [ 4524/27660 ( 16%)], Train Loss: 0.68738\n",
      "Epoch: 00 [ 4564/27660 ( 17%)], Train Loss: 0.68358\n",
      "Epoch: 00 [ 4604/27660 ( 17%)], Train Loss: 0.68203\n",
      "Epoch: 00 [ 4644/27660 ( 17%)], Train Loss: 0.67868\n",
      "Epoch: 00 [ 4684/27660 ( 17%)], Train Loss: 0.67599\n",
      "Epoch: 00 [ 4724/27660 ( 17%)], Train Loss: 0.67470\n",
      "Epoch: 00 [ 4764/27660 ( 17%)], Train Loss: 0.67291\n",
      "Epoch: 00 [ 4804/27660 ( 17%)], Train Loss: 0.67210\n",
      "Epoch: 00 [ 4844/27660 ( 18%)], Train Loss: 0.67035\n",
      "Epoch: 00 [ 4884/27660 ( 18%)], Train Loss: 0.66774\n",
      "Epoch: 00 [ 4924/27660 ( 18%)], Train Loss: 0.66540\n",
      "Epoch: 00 [ 4964/27660 ( 18%)], Train Loss: 0.66386\n",
      "Epoch: 00 [ 5004/27660 ( 18%)], Train Loss: 0.66200\n",
      "Epoch: 00 [ 5044/27660 ( 18%)], Train Loss: 0.66002\n",
      "Epoch: 00 [ 5084/27660 ( 18%)], Train Loss: 0.65976\n",
      "Epoch: 00 [ 5124/27660 ( 19%)], Train Loss: 0.65782\n",
      "Epoch: 00 [ 5164/27660 ( 19%)], Train Loss: 0.65797\n",
      "Epoch: 00 [ 5204/27660 ( 19%)], Train Loss: 0.65544\n",
      "Epoch: 00 [ 5244/27660 ( 19%)], Train Loss: 0.65357\n",
      "Epoch: 00 [ 5284/27660 ( 19%)], Train Loss: 0.65093\n",
      "Epoch: 00 [ 5324/27660 ( 19%)], Train Loss: 0.64777\n",
      "Epoch: 00 [ 5364/27660 ( 19%)], Train Loss: 0.64769\n",
      "Epoch: 00 [ 5404/27660 ( 20%)], Train Loss: 0.64597\n",
      "Epoch: 00 [ 5444/27660 ( 20%)], Train Loss: 0.64378\n",
      "Epoch: 00 [ 5484/27660 ( 20%)], Train Loss: 0.64251\n",
      "Epoch: 00 [ 5524/27660 ( 20%)], Train Loss: 0.64075\n",
      "Epoch: 00 [ 5564/27660 ( 20%)], Train Loss: 0.64084\n",
      "Epoch: 00 [ 5604/27660 ( 20%)], Train Loss: 0.64023\n",
      "Epoch: 00 [ 5644/27660 ( 20%)], Train Loss: 0.63944\n",
      "Epoch: 00 [ 5684/27660 ( 21%)], Train Loss: 0.63773\n",
      "Epoch: 00 [ 5724/27660 ( 21%)], Train Loss: 0.63700\n",
      "Epoch: 00 [ 5764/27660 ( 21%)], Train Loss: 0.63463\n",
      "Epoch: 00 [ 5804/27660 ( 21%)], Train Loss: 0.63341\n",
      "Epoch: 00 [ 5844/27660 ( 21%)], Train Loss: 0.63190\n",
      "Epoch: 00 [ 5884/27660 ( 21%)], Train Loss: 0.63019\n",
      "Epoch: 00 [ 5924/27660 ( 21%)], Train Loss: 0.62832\n",
      "Epoch: 00 [ 5964/27660 ( 22%)], Train Loss: 0.62781\n",
      "Epoch: 00 [ 6004/27660 ( 22%)], Train Loss: 0.62670\n",
      "Epoch: 00 [ 6044/27660 ( 22%)], Train Loss: 0.62407\n",
      "Epoch: 00 [ 6084/27660 ( 22%)], Train Loss: 0.62217\n",
      "Epoch: 00 [ 6124/27660 ( 22%)], Train Loss: 0.62010\n",
      "Epoch: 00 [ 6164/27660 ( 22%)], Train Loss: 0.61877\n",
      "Epoch: 00 [ 6204/27660 ( 22%)], Train Loss: 0.61650\n",
      "Epoch: 00 [ 6244/27660 ( 23%)], Train Loss: 0.61636\n",
      "Epoch: 00 [ 6284/27660 ( 23%)], Train Loss: 0.61510\n",
      "Epoch: 00 [ 6324/27660 ( 23%)], Train Loss: 0.61362\n",
      "Epoch: 00 [ 6364/27660 ( 23%)], Train Loss: 0.61251\n",
      "Epoch: 00 [ 6404/27660 ( 23%)], Train Loss: 0.61039\n",
      "Epoch: 00 [ 6444/27660 ( 23%)], Train Loss: 0.60853\n",
      "Epoch: 00 [ 6484/27660 ( 23%)], Train Loss: 0.60694\n",
      "Epoch: 00 [ 6524/27660 ( 24%)], Train Loss: 0.60519\n",
      "Epoch: 00 [ 6564/27660 ( 24%)], Train Loss: 0.60423\n",
      "Epoch: 00 [ 6604/27660 ( 24%)], Train Loss: 0.60243\n",
      "Epoch: 00 [ 6644/27660 ( 24%)], Train Loss: 0.60039\n",
      "Epoch: 00 [ 6684/27660 ( 24%)], Train Loss: 0.59990\n",
      "Epoch: 00 [ 6724/27660 ( 24%)], Train Loss: 0.59842\n",
      "Epoch: 00 [ 6764/27660 ( 24%)], Train Loss: 0.59743\n",
      "Epoch: 00 [ 6804/27660 ( 25%)], Train Loss: 0.59626\n",
      "Epoch: 00 [ 6844/27660 ( 25%)], Train Loss: 0.59514\n",
      "Epoch: 00 [ 6884/27660 ( 25%)], Train Loss: 0.59448\n",
      "Epoch: 00 [ 6924/27660 ( 25%)], Train Loss: 0.59329\n",
      "Epoch: 00 [ 6964/27660 ( 25%)], Train Loss: 0.59181\n",
      "Epoch: 00 [ 7004/27660 ( 25%)], Train Loss: 0.59117\n",
      "Epoch: 00 [ 7044/27660 ( 25%)], Train Loss: 0.58991\n",
      "Epoch: 00 [ 7084/27660 ( 26%)], Train Loss: 0.58801\n",
      "Epoch: 00 [ 7124/27660 ( 26%)], Train Loss: 0.58665\n",
      "Epoch: 00 [ 7164/27660 ( 26%)], Train Loss: 0.58517\n",
      "Epoch: 00 [ 7204/27660 ( 26%)], Train Loss: 0.58353\n",
      "Epoch: 00 [ 7244/27660 ( 26%)], Train Loss: 0.58271\n",
      "Epoch: 00 [ 7284/27660 ( 26%)], Train Loss: 0.58182\n",
      "Epoch: 00 [ 7324/27660 ( 26%)], Train Loss: 0.58160\n",
      "Epoch: 00 [ 7364/27660 ( 27%)], Train Loss: 0.58073\n",
      "Epoch: 00 [ 7404/27660 ( 27%)], Train Loss: 0.57953\n",
      "Epoch: 00 [ 7444/27660 ( 27%)], Train Loss: 0.57955\n",
      "Epoch: 00 [ 7484/27660 ( 27%)], Train Loss: 0.58002\n",
      "Epoch: 00 [ 7524/27660 ( 27%)], Train Loss: 0.57896\n",
      "Epoch: 00 [ 7564/27660 ( 27%)], Train Loss: 0.57814\n",
      "Epoch: 00 [ 7604/27660 ( 27%)], Train Loss: 0.57678\n",
      "Epoch: 00 [ 7644/27660 ( 28%)], Train Loss: 0.57592\n",
      "Epoch: 00 [ 7684/27660 ( 28%)], Train Loss: 0.57447\n",
      "Epoch: 00 [ 7724/27660 ( 28%)], Train Loss: 0.57418\n",
      "Epoch: 00 [ 7764/27660 ( 28%)], Train Loss: 0.57328\n",
      "Epoch: 00 [ 7804/27660 ( 28%)], Train Loss: 0.57220\n",
      "Epoch: 00 [ 7844/27660 ( 28%)], Train Loss: 0.57132\n",
      "Epoch: 00 [ 7884/27660 ( 29%)], Train Loss: 0.56964\n",
      "Epoch: 00 [ 7924/27660 ( 29%)], Train Loss: 0.56824\n",
      "Epoch: 00 [ 7964/27660 ( 29%)], Train Loss: 0.56798\n",
      "Epoch: 00 [ 8004/27660 ( 29%)], Train Loss: 0.56703\n",
      "Epoch: 00 [ 8044/27660 ( 29%)], Train Loss: 0.56650\n",
      "Epoch: 00 [ 8084/27660 ( 29%)], Train Loss: 0.56582\n",
      "Epoch: 00 [ 8124/27660 ( 29%)], Train Loss: 0.56410\n",
      "Epoch: 00 [ 8164/27660 ( 30%)], Train Loss: 0.56365\n",
      "Epoch: 00 [ 8204/27660 ( 30%)], Train Loss: 0.56186\n",
      "Epoch: 00 [ 8244/27660 ( 30%)], Train Loss: 0.56110\n",
      "Epoch: 00 [ 8284/27660 ( 30%)], Train Loss: 0.55953\n",
      "Epoch: 00 [ 8324/27660 ( 30%)], Train Loss: 0.56007\n",
      "Epoch: 00 [ 8364/27660 ( 30%)], Train Loss: 0.55907\n",
      "Epoch: 00 [ 8404/27660 ( 30%)], Train Loss: 0.55835\n",
      "Epoch: 00 [ 8444/27660 ( 31%)], Train Loss: 0.55932\n",
      "Epoch: 00 [ 8484/27660 ( 31%)], Train Loss: 0.55975\n",
      "Epoch: 00 [ 8524/27660 ( 31%)], Train Loss: 0.55980\n",
      "Epoch: 00 [ 8564/27660 ( 31%)], Train Loss: 0.56025\n",
      "Epoch: 00 [ 8604/27660 ( 31%)], Train Loss: 0.55960\n",
      "Epoch: 00 [ 8644/27660 ( 31%)], Train Loss: 0.55853\n",
      "Epoch: 00 [ 8684/27660 ( 31%)], Train Loss: 0.55724\n",
      "Epoch: 00 [ 8724/27660 ( 32%)], Train Loss: 0.55637\n",
      "Epoch: 00 [ 8764/27660 ( 32%)], Train Loss: 0.55531\n",
      "Epoch: 00 [ 8804/27660 ( 32%)], Train Loss: 0.55535\n",
      "Epoch: 00 [ 8844/27660 ( 32%)], Train Loss: 0.55448\n",
      "Epoch: 00 [ 8884/27660 ( 32%)], Train Loss: 0.55333\n",
      "Epoch: 00 [ 8924/27660 ( 32%)], Train Loss: 0.55243\n",
      "Epoch: 00 [ 8964/27660 ( 32%)], Train Loss: 0.55157\n",
      "Epoch: 00 [ 9004/27660 ( 33%)], Train Loss: 0.55117\n",
      "Epoch: 00 [ 9044/27660 ( 33%)], Train Loss: 0.54999\n",
      "Epoch: 00 [ 9084/27660 ( 33%)], Train Loss: 0.54944\n",
      "Epoch: 00 [ 9124/27660 ( 33%)], Train Loss: 0.54887\n",
      "Epoch: 00 [ 9164/27660 ( 33%)], Train Loss: 0.54789\n",
      "Epoch: 00 [ 9204/27660 ( 33%)], Train Loss: 0.54663\n",
      "Epoch: 00 [ 9244/27660 ( 33%)], Train Loss: 0.54641\n",
      "Epoch: 00 [ 9284/27660 ( 34%)], Train Loss: 0.54602\n",
      "Epoch: 00 [ 9324/27660 ( 34%)], Train Loss: 0.54568\n",
      "Epoch: 00 [ 9364/27660 ( 34%)], Train Loss: 0.54524\n",
      "Epoch: 00 [ 9404/27660 ( 34%)], Train Loss: 0.54468\n",
      "Epoch: 00 [ 9444/27660 ( 34%)], Train Loss: 0.54360\n",
      "Epoch: 00 [ 9484/27660 ( 34%)], Train Loss: 0.54274\n",
      "Epoch: 00 [ 9524/27660 ( 34%)], Train Loss: 0.54203\n",
      "Epoch: 00 [ 9564/27660 ( 35%)], Train Loss: 0.54233\n",
      "Epoch: 00 [ 9604/27660 ( 35%)], Train Loss: 0.54166\n",
      "Epoch: 00 [ 9644/27660 ( 35%)], Train Loss: 0.54098\n",
      "Epoch: 00 [ 9684/27660 ( 35%)], Train Loss: 0.53956\n",
      "Epoch: 00 [ 9724/27660 ( 35%)], Train Loss: 0.53881\n",
      "Epoch: 00 [ 9764/27660 ( 35%)], Train Loss: 0.53827\n",
      "Epoch: 00 [ 9804/27660 ( 35%)], Train Loss: 0.53737\n",
      "Epoch: 00 [ 9844/27660 ( 36%)], Train Loss: 0.53744\n",
      "Epoch: 00 [ 9884/27660 ( 36%)], Train Loss: 0.53769\n",
      "Epoch: 00 [ 9924/27660 ( 36%)], Train Loss: 0.53790\n",
      "Epoch: 00 [ 9964/27660 ( 36%)], Train Loss: 0.53702\n",
      "Epoch: 00 [10004/27660 ( 36%)], Train Loss: 0.53699\n",
      "Epoch: 00 [10044/27660 ( 36%)], Train Loss: 0.53649\n",
      "Epoch: 00 [10084/27660 ( 36%)], Train Loss: 0.53581\n",
      "Epoch: 00 [10124/27660 ( 37%)], Train Loss: 0.53549\n",
      "Epoch: 00 [10164/27660 ( 37%)], Train Loss: 0.53479\n",
      "Epoch: 00 [10204/27660 ( 37%)], Train Loss: 0.53470\n",
      "Epoch: 00 [10244/27660 ( 37%)], Train Loss: 0.53478\n",
      "Epoch: 00 [10284/27660 ( 37%)], Train Loss: 0.53399\n",
      "Epoch: 00 [10324/27660 ( 37%)], Train Loss: 0.53359\n",
      "Epoch: 00 [10364/27660 ( 37%)], Train Loss: 0.53295\n",
      "Epoch: 00 [10404/27660 ( 38%)], Train Loss: 0.53185\n",
      "Epoch: 00 [10444/27660 ( 38%)], Train Loss: 0.53058\n",
      "Epoch: 00 [10484/27660 ( 38%)], Train Loss: 0.53021\n",
      "Epoch: 00 [10524/27660 ( 38%)], Train Loss: 0.52931\n",
      "Epoch: 00 [10564/27660 ( 38%)], Train Loss: 0.52886\n",
      "Epoch: 00 [10604/27660 ( 38%)], Train Loss: 0.52798\n",
      "Epoch: 00 [10644/27660 ( 38%)], Train Loss: 0.52696\n",
      "Epoch: 00 [10684/27660 ( 39%)], Train Loss: 0.52588\n",
      "Epoch: 00 [10724/27660 ( 39%)], Train Loss: 0.52534\n",
      "Epoch: 00 [10764/27660 ( 39%)], Train Loss: 0.52486\n",
      "Epoch: 00 [10804/27660 ( 39%)], Train Loss: 0.52461\n",
      "Epoch: 00 [10844/27660 ( 39%)], Train Loss: 0.52443\n",
      "Epoch: 00 [10884/27660 ( 39%)], Train Loss: 0.52405\n",
      "Epoch: 00 [10924/27660 ( 39%)], Train Loss: 0.52347\n",
      "Epoch: 00 [10964/27660 ( 40%)], Train Loss: 0.52278\n",
      "Epoch: 00 [11004/27660 ( 40%)], Train Loss: 0.52273\n",
      "Epoch: 00 [11044/27660 ( 40%)], Train Loss: 0.52339\n",
      "Epoch: 00 [11084/27660 ( 40%)], Train Loss: 0.52276\n",
      "Epoch: 00 [11124/27660 ( 40%)], Train Loss: 0.52208\n",
      "Epoch: 00 [11164/27660 ( 40%)], Train Loss: 0.52138\n",
      "Epoch: 00 [11204/27660 ( 41%)], Train Loss: 0.52083\n",
      "Epoch: 00 [11244/27660 ( 41%)], Train Loss: 0.52012\n",
      "Epoch: 00 [11284/27660 ( 41%)], Train Loss: 0.51962\n",
      "Epoch: 00 [11324/27660 ( 41%)], Train Loss: 0.51940\n",
      "Epoch: 00 [11364/27660 ( 41%)], Train Loss: 0.51914\n",
      "Epoch: 00 [11404/27660 ( 41%)], Train Loss: 0.51847\n",
      "Epoch: 00 [11444/27660 ( 41%)], Train Loss: 0.51763\n",
      "Epoch: 00 [11484/27660 ( 42%)], Train Loss: 0.51682\n",
      "Epoch: 00 [11524/27660 ( 42%)], Train Loss: 0.51641\n",
      "Epoch: 00 [11564/27660 ( 42%)], Train Loss: 0.51660\n",
      "Epoch: 00 [11604/27660 ( 42%)], Train Loss: 0.51642\n",
      "Epoch: 00 [11644/27660 ( 42%)], Train Loss: 0.51636\n",
      "Epoch: 00 [11684/27660 ( 42%)], Train Loss: 0.51594\n",
      "Epoch: 00 [11724/27660 ( 42%)], Train Loss: 0.51569\n",
      "Epoch: 00 [11764/27660 ( 43%)], Train Loss: 0.51493\n",
      "Epoch: 00 [11804/27660 ( 43%)], Train Loss: 0.51449\n",
      "Epoch: 00 [11844/27660 ( 43%)], Train Loss: 0.51414\n",
      "Epoch: 00 [11884/27660 ( 43%)], Train Loss: 0.51368\n",
      "Epoch: 00 [11924/27660 ( 43%)], Train Loss: 0.51313\n",
      "Epoch: 00 [11964/27660 ( 43%)], Train Loss: 0.51275\n",
      "Epoch: 00 [12004/27660 ( 43%)], Train Loss: 0.51216\n",
      "Epoch: 00 [12044/27660 ( 44%)], Train Loss: 0.51139\n",
      "Epoch: 00 [12084/27660 ( 44%)], Train Loss: 0.51091\n",
      "Epoch: 00 [12124/27660 ( 44%)], Train Loss: 0.51014\n",
      "Epoch: 00 [12164/27660 ( 44%)], Train Loss: 0.50942\n",
      "Epoch: 00 [12204/27660 ( 44%)], Train Loss: 0.50932\n",
      "Epoch: 00 [12244/27660 ( 44%)], Train Loss: 0.50885\n",
      "Epoch: 00 [12284/27660 ( 44%)], Train Loss: 0.50814\n",
      "Epoch: 00 [12324/27660 ( 45%)], Train Loss: 0.50769\n",
      "Epoch: 00 [12364/27660 ( 45%)], Train Loss: 0.50712\n",
      "Epoch: 00 [12404/27660 ( 45%)], Train Loss: 0.50688\n",
      "Epoch: 00 [12444/27660 ( 45%)], Train Loss: 0.50658\n",
      "Epoch: 00 [12484/27660 ( 45%)], Train Loss: 0.50586\n",
      "Epoch: 00 [12524/27660 ( 45%)], Train Loss: 0.50590\n",
      "Epoch: 00 [12564/27660 ( 45%)], Train Loss: 0.50555\n",
      "Epoch: 00 [12604/27660 ( 46%)], Train Loss: 0.50520\n",
      "Epoch: 00 [12644/27660 ( 46%)], Train Loss: 0.50510\n",
      "Epoch: 00 [12684/27660 ( 46%)], Train Loss: 0.50479\n",
      "Epoch: 00 [12724/27660 ( 46%)], Train Loss: 0.50389\n",
      "Epoch: 00 [12764/27660 ( 46%)], Train Loss: 0.50350\n",
      "Epoch: 00 [12804/27660 ( 46%)], Train Loss: 0.50286\n",
      "Epoch: 00 [12844/27660 ( 46%)], Train Loss: 0.50333\n",
      "Epoch: 00 [12884/27660 ( 47%)], Train Loss: 0.50289\n",
      "Epoch: 00 [12924/27660 ( 47%)], Train Loss: 0.50216\n",
      "Epoch: 00 [12964/27660 ( 47%)], Train Loss: 0.50168\n",
      "Epoch: 00 [13004/27660 ( 47%)], Train Loss: 0.50130\n",
      "Epoch: 00 [13044/27660 ( 47%)], Train Loss: 0.50080\n",
      "Epoch: 00 [13084/27660 ( 47%)], Train Loss: 0.50032\n",
      "Epoch: 00 [13124/27660 ( 47%)], Train Loss: 0.49980\n",
      "Epoch: 00 [13164/27660 ( 48%)], Train Loss: 0.49927\n",
      "Epoch: 00 [13204/27660 ( 48%)], Train Loss: 0.49916\n",
      "Epoch: 00 [13244/27660 ( 48%)], Train Loss: 0.49857\n",
      "Epoch: 00 [13284/27660 ( 48%)], Train Loss: 0.49796\n",
      "Epoch: 00 [13324/27660 ( 48%)], Train Loss: 0.49753\n",
      "Epoch: 00 [13364/27660 ( 48%)], Train Loss: 0.49717\n",
      "Epoch: 00 [13404/27660 ( 48%)], Train Loss: 0.49678\n",
      "Epoch: 00 [13444/27660 ( 49%)], Train Loss: 0.49617\n",
      "Epoch: 00 [13484/27660 ( 49%)], Train Loss: 0.49602\n",
      "Epoch: 00 [13524/27660 ( 49%)], Train Loss: 0.49583\n",
      "Epoch: 00 [13564/27660 ( 49%)], Train Loss: 0.49534\n",
      "Epoch: 00 [13604/27660 ( 49%)], Train Loss: 0.49469\n",
      "Epoch: 00 [13644/27660 ( 49%)], Train Loss: 0.49466\n",
      "Epoch: 00 [13684/27660 ( 49%)], Train Loss: 0.49453\n",
      "Epoch: 00 [13724/27660 ( 50%)], Train Loss: 0.49467\n",
      "Epoch: 00 [13764/27660 ( 50%)], Train Loss: 0.49393\n",
      "Epoch: 00 [13804/27660 ( 50%)], Train Loss: 0.49337\n",
      "Epoch: 00 [13844/27660 ( 50%)], Train Loss: 0.49279\n",
      "Epoch: 00 [13884/27660 ( 50%)], Train Loss: 0.49198\n",
      "Epoch: 00 [13924/27660 ( 50%)], Train Loss: 0.49137\n",
      "Epoch: 00 [13964/27660 ( 50%)], Train Loss: 0.49076\n",
      "Epoch: 00 [14004/27660 ( 51%)], Train Loss: 0.49021\n",
      "Epoch: 00 [14044/27660 ( 51%)], Train Loss: 0.48975\n",
      "Epoch: 00 [14084/27660 ( 51%)], Train Loss: 0.48939\n",
      "Epoch: 00 [14124/27660 ( 51%)], Train Loss: 0.48893\n",
      "Epoch: 00 [14164/27660 ( 51%)], Train Loss: 0.48868\n",
      "Epoch: 00 [14204/27660 ( 51%)], Train Loss: 0.48830\n",
      "Epoch: 00 [14244/27660 ( 51%)], Train Loss: 0.48760\n",
      "Epoch: 00 [14284/27660 ( 52%)], Train Loss: 0.48715\n",
      "Epoch: 00 [14324/27660 ( 52%)], Train Loss: 0.48658\n",
      "Epoch: 00 [14364/27660 ( 52%)], Train Loss: 0.48604\n",
      "Epoch: 00 [14404/27660 ( 52%)], Train Loss: 0.48595\n",
      "Epoch: 00 [14444/27660 ( 52%)], Train Loss: 0.48571\n",
      "Epoch: 00 [14484/27660 ( 52%)], Train Loss: 0.48537\n",
      "Epoch: 00 [14524/27660 ( 53%)], Train Loss: 0.48503\n",
      "Epoch: 00 [14564/27660 ( 53%)], Train Loss: 0.48468\n",
      "Epoch: 00 [14604/27660 ( 53%)], Train Loss: 0.48474\n",
      "Epoch: 00 [14644/27660 ( 53%)], Train Loss: 0.48407\n",
      "Epoch: 00 [14684/27660 ( 53%)], Train Loss: 0.48368\n",
      "Epoch: 00 [14724/27660 ( 53%)], Train Loss: 0.48307\n",
      "Epoch: 00 [14764/27660 ( 53%)], Train Loss: 0.48264\n",
      "Epoch: 00 [14804/27660 ( 54%)], Train Loss: 0.48232\n",
      "Epoch: 00 [14844/27660 ( 54%)], Train Loss: 0.48200\n",
      "Epoch: 00 [14884/27660 ( 54%)], Train Loss: 0.48157\n",
      "Epoch: 00 [14924/27660 ( 54%)], Train Loss: 0.48134\n",
      "Epoch: 00 [14964/27660 ( 54%)], Train Loss: 0.48057\n",
      "Epoch: 00 [15004/27660 ( 54%)], Train Loss: 0.47990\n",
      "Epoch: 00 [15044/27660 ( 54%)], Train Loss: 0.47928\n",
      "Epoch: 00 [15084/27660 ( 55%)], Train Loss: 0.47862\n",
      "Epoch: 00 [15124/27660 ( 55%)], Train Loss: 0.47798\n",
      "Epoch: 00 [15164/27660 ( 55%)], Train Loss: 0.47816\n",
      "Epoch: 00 [15204/27660 ( 55%)], Train Loss: 0.47786\n",
      "Epoch: 00 [15244/27660 ( 55%)], Train Loss: 0.47785\n",
      "Epoch: 00 [15284/27660 ( 55%)], Train Loss: 0.47758\n",
      "Epoch: 00 [15324/27660 ( 55%)], Train Loss: 0.47728\n",
      "Epoch: 00 [15364/27660 ( 56%)], Train Loss: 0.47672\n",
      "Epoch: 00 [15404/27660 ( 56%)], Train Loss: 0.47621\n",
      "Epoch: 00 [15444/27660 ( 56%)], Train Loss: 0.47557\n",
      "Epoch: 00 [15484/27660 ( 56%)], Train Loss: 0.47528\n",
      "Epoch: 00 [15524/27660 ( 56%)], Train Loss: 0.47536\n",
      "Epoch: 00 [15564/27660 ( 56%)], Train Loss: 0.47482\n",
      "Epoch: 00 [15604/27660 ( 56%)], Train Loss: 0.47410\n",
      "Epoch: 00 [15644/27660 ( 57%)], Train Loss: 0.47354\n",
      "Epoch: 00 [15684/27660 ( 57%)], Train Loss: 0.47289\n",
      "Epoch: 00 [15724/27660 ( 57%)], Train Loss: 0.47252\n",
      "Epoch: 00 [15764/27660 ( 57%)], Train Loss: 0.47214\n",
      "Epoch: 00 [15804/27660 ( 57%)], Train Loss: 0.47147\n",
      "Epoch: 00 [15844/27660 ( 57%)], Train Loss: 0.47085\n",
      "Epoch: 00 [15884/27660 ( 57%)], Train Loss: 0.47065\n",
      "Epoch: 00 [15924/27660 ( 58%)], Train Loss: 0.47020\n",
      "Epoch: 00 [15964/27660 ( 58%)], Train Loss: 0.47008\n",
      "Epoch: 00 [16004/27660 ( 58%)], Train Loss: 0.46955\n",
      "Epoch: 00 [16044/27660 ( 58%)], Train Loss: 0.46905\n",
      "Epoch: 00 [16084/27660 ( 58%)], Train Loss: 0.46913\n",
      "Epoch: 00 [16124/27660 ( 58%)], Train Loss: 0.46923\n",
      "Epoch: 00 [16164/27660 ( 58%)], Train Loss: 0.46887\n",
      "Epoch: 00 [16204/27660 ( 59%)], Train Loss: 0.46823\n",
      "Epoch: 00 [16244/27660 ( 59%)], Train Loss: 0.46761\n",
      "Epoch: 00 [16284/27660 ( 59%)], Train Loss: 0.46712\n",
      "Epoch: 00 [16324/27660 ( 59%)], Train Loss: 0.46639\n",
      "Epoch: 00 [16364/27660 ( 59%)], Train Loss: 0.46590\n",
      "Epoch: 00 [16404/27660 ( 59%)], Train Loss: 0.46538\n",
      "Epoch: 00 [16444/27660 ( 59%)], Train Loss: 0.46481\n",
      "Epoch: 00 [16484/27660 ( 60%)], Train Loss: 0.46420\n",
      "Epoch: 00 [16524/27660 ( 60%)], Train Loss: 0.46396\n",
      "Epoch: 00 [16564/27660 ( 60%)], Train Loss: 0.46408\n",
      "Epoch: 00 [16604/27660 ( 60%)], Train Loss: 0.46367\n",
      "Epoch: 00 [16644/27660 ( 60%)], Train Loss: 0.46304\n",
      "Epoch: 00 [16684/27660 ( 60%)], Train Loss: 0.46260\n",
      "Epoch: 00 [16724/27660 ( 60%)], Train Loss: 0.46218\n",
      "Epoch: 00 [16764/27660 ( 61%)], Train Loss: 0.46167\n",
      "Epoch: 00 [16804/27660 ( 61%)], Train Loss: 0.46134\n",
      "Epoch: 00 [16844/27660 ( 61%)], Train Loss: 0.46097\n",
      "Epoch: 00 [16884/27660 ( 61%)], Train Loss: 0.46108\n",
      "Epoch: 00 [16924/27660 ( 61%)], Train Loss: 0.46057\n",
      "Epoch: 00 [16964/27660 ( 61%)], Train Loss: 0.46069\n",
      "Epoch: 00 [17004/27660 ( 61%)], Train Loss: 0.46025\n",
      "Epoch: 00 [17044/27660 ( 62%)], Train Loss: 0.46000\n",
      "Epoch: 00 [17084/27660 ( 62%)], Train Loss: 0.45982\n",
      "Epoch: 00 [17124/27660 ( 62%)], Train Loss: 0.45976\n",
      "Epoch: 00 [17164/27660 ( 62%)], Train Loss: 0.46000\n",
      "Epoch: 00 [17204/27660 ( 62%)], Train Loss: 0.45965\n",
      "Epoch: 00 [17244/27660 ( 62%)], Train Loss: 0.45932\n",
      "Epoch: 00 [17284/27660 ( 62%)], Train Loss: 0.45888\n",
      "Epoch: 00 [17324/27660 ( 63%)], Train Loss: 0.45852\n",
      "Epoch: 00 [17364/27660 ( 63%)], Train Loss: 0.45811\n",
      "Epoch: 00 [17404/27660 ( 63%)], Train Loss: 0.45772\n",
      "Epoch: 00 [17444/27660 ( 63%)], Train Loss: 0.45758\n",
      "Epoch: 00 [17484/27660 ( 63%)], Train Loss: 0.45736\n",
      "Epoch: 00 [17524/27660 ( 63%)], Train Loss: 0.45687\n",
      "Epoch: 00 [17564/27660 ( 63%)], Train Loss: 0.45649\n",
      "Epoch: 00 [17604/27660 ( 64%)], Train Loss: 0.45674\n",
      "Epoch: 00 [17644/27660 ( 64%)], Train Loss: 0.45623\n",
      "Epoch: 00 [17684/27660 ( 64%)], Train Loss: 0.45586\n",
      "Epoch: 00 [17724/27660 ( 64%)], Train Loss: 0.45529\n",
      "Epoch: 00 [17764/27660 ( 64%)], Train Loss: 0.45514\n",
      "Epoch: 00 [17804/27660 ( 64%)], Train Loss: 0.45503\n",
      "Epoch: 00 [17844/27660 ( 65%)], Train Loss: 0.45464\n",
      "Epoch: 00 [17884/27660 ( 65%)], Train Loss: 0.45440\n",
      "Epoch: 00 [17924/27660 ( 65%)], Train Loss: 0.45400\n",
      "Epoch: 00 [17964/27660 ( 65%)], Train Loss: 0.45376\n",
      "Epoch: 00 [18004/27660 ( 65%)], Train Loss: 0.45345\n",
      "Epoch: 00 [18044/27660 ( 65%)], Train Loss: 0.45328\n",
      "Epoch: 00 [18084/27660 ( 65%)], Train Loss: 0.45324\n",
      "Epoch: 00 [18124/27660 ( 66%)], Train Loss: 0.45294\n",
      "Epoch: 00 [18164/27660 ( 66%)], Train Loss: 0.45303\n",
      "Epoch: 00 [18204/27660 ( 66%)], Train Loss: 0.45292\n",
      "Epoch: 00 [18244/27660 ( 66%)], Train Loss: 0.45268\n",
      "Epoch: 00 [18284/27660 ( 66%)], Train Loss: 0.45236\n",
      "Epoch: 00 [18324/27660 ( 66%)], Train Loss: 0.45217\n",
      "Epoch: 00 [18364/27660 ( 66%)], Train Loss: 0.45172\n",
      "Epoch: 00 [18404/27660 ( 67%)], Train Loss: 0.45118\n",
      "Epoch: 00 [18444/27660 ( 67%)], Train Loss: 0.45104\n",
      "Epoch: 00 [18484/27660 ( 67%)], Train Loss: 0.45090\n",
      "Epoch: 00 [18524/27660 ( 67%)], Train Loss: 0.45021\n",
      "Epoch: 00 [18564/27660 ( 67%)], Train Loss: 0.44983\n",
      "Epoch: 00 [18604/27660 ( 67%)], Train Loss: 0.44977\n",
      "Epoch: 00 [18644/27660 ( 67%)], Train Loss: 0.44957\n",
      "Epoch: 00 [18684/27660 ( 68%)], Train Loss: 0.44903\n",
      "Epoch: 00 [18724/27660 ( 68%)], Train Loss: 0.44894\n",
      "Epoch: 00 [18764/27660 ( 68%)], Train Loss: 0.44857\n",
      "Epoch: 00 [18804/27660 ( 68%)], Train Loss: 0.44814\n",
      "Epoch: 00 [18844/27660 ( 68%)], Train Loss: 0.44782\n",
      "Epoch: 00 [18884/27660 ( 68%)], Train Loss: 0.44753\n",
      "Epoch: 00 [18924/27660 ( 68%)], Train Loss: 0.44718\n",
      "Epoch: 00 [18964/27660 ( 69%)], Train Loss: 0.44706\n",
      "Epoch: 00 [19004/27660 ( 69%)], Train Loss: 0.44667\n",
      "Epoch: 00 [19044/27660 ( 69%)], Train Loss: 0.44627\n",
      "Epoch: 00 [19084/27660 ( 69%)], Train Loss: 0.44589\n",
      "Epoch: 00 [19124/27660 ( 69%)], Train Loss: 0.44554\n",
      "Epoch: 00 [19164/27660 ( 69%)], Train Loss: 0.44534\n",
      "Epoch: 00 [19204/27660 ( 69%)], Train Loss: 0.44512\n",
      "Epoch: 00 [19244/27660 ( 70%)], Train Loss: 0.44491\n",
      "Epoch: 00 [19284/27660 ( 70%)], Train Loss: 0.44493\n",
      "Epoch: 00 [19324/27660 ( 70%)], Train Loss: 0.44441\n",
      "Epoch: 00 [19364/27660 ( 70%)], Train Loss: 0.44415\n",
      "Epoch: 00 [19404/27660 ( 70%)], Train Loss: 0.44373\n",
      "Epoch: 00 [19444/27660 ( 70%)], Train Loss: 0.44320\n",
      "Epoch: 00 [19484/27660 ( 70%)], Train Loss: 0.44279\n",
      "Epoch: 00 [19524/27660 ( 71%)], Train Loss: 0.44253\n",
      "Epoch: 00 [19564/27660 ( 71%)], Train Loss: 0.44236\n",
      "Epoch: 00 [19604/27660 ( 71%)], Train Loss: 0.44188\n",
      "Epoch: 00 [19644/27660 ( 71%)], Train Loss: 0.44162\n",
      "Epoch: 00 [19684/27660 ( 71%)], Train Loss: 0.44104\n",
      "Epoch: 00 [19724/27660 ( 71%)], Train Loss: 0.44102\n",
      "Epoch: 00 [19764/27660 ( 71%)], Train Loss: 0.44061\n",
      "Epoch: 00 [19804/27660 ( 72%)], Train Loss: 0.44053\n",
      "Epoch: 00 [19844/27660 ( 72%)], Train Loss: 0.44018\n",
      "Epoch: 00 [19884/27660 ( 72%)], Train Loss: 0.43965\n",
      "Epoch: 00 [19924/27660 ( 72%)], Train Loss: 0.43917\n",
      "Epoch: 00 [19964/27660 ( 72%)], Train Loss: 0.43873\n",
      "Epoch: 00 [20004/27660 ( 72%)], Train Loss: 0.43891\n",
      "Epoch: 00 [20044/27660 ( 72%)], Train Loss: 0.43851\n",
      "Epoch: 00 [20084/27660 ( 73%)], Train Loss: 0.43824\n",
      "Epoch: 00 [20124/27660 ( 73%)], Train Loss: 0.43800\n",
      "Epoch: 00 [20164/27660 ( 73%)], Train Loss: 0.43763\n",
      "Epoch: 00 [20204/27660 ( 73%)], Train Loss: 0.43738\n",
      "Epoch: 00 [20244/27660 ( 73%)], Train Loss: 0.43731\n",
      "Epoch: 00 [20284/27660 ( 73%)], Train Loss: 0.43692\n",
      "Epoch: 00 [20324/27660 ( 73%)], Train Loss: 0.43651\n",
      "Epoch: 00 [20364/27660 ( 74%)], Train Loss: 0.43622\n",
      "Epoch: 00 [20404/27660 ( 74%)], Train Loss: 0.43584\n",
      "Epoch: 00 [20444/27660 ( 74%)], Train Loss: 0.43572\n",
      "Epoch: 00 [20484/27660 ( 74%)], Train Loss: 0.43538\n",
      "Epoch: 00 [20524/27660 ( 74%)], Train Loss: 0.43540\n",
      "Epoch: 00 [20564/27660 ( 74%)], Train Loss: 0.43522\n",
      "Epoch: 00 [20604/27660 ( 74%)], Train Loss: 0.43482\n",
      "Epoch: 00 [20644/27660 ( 75%)], Train Loss: 0.43450\n",
      "Epoch: 00 [20684/27660 ( 75%)], Train Loss: 0.43432\n",
      "Epoch: 00 [20724/27660 ( 75%)], Train Loss: 0.43399\n",
      "Epoch: 00 [20764/27660 ( 75%)], Train Loss: 0.43381\n",
      "Epoch: 00 [20804/27660 ( 75%)], Train Loss: 0.43341\n",
      "Epoch: 00 [20844/27660 ( 75%)], Train Loss: 0.43291\n",
      "Epoch: 00 [20884/27660 ( 76%)], Train Loss: 0.43298\n",
      "Epoch: 00 [20924/27660 ( 76%)], Train Loss: 0.43289\n",
      "Epoch: 00 [20964/27660 ( 76%)], Train Loss: 0.43288\n",
      "Epoch: 00 [21004/27660 ( 76%)], Train Loss: 0.43283\n",
      "Epoch: 00 [21044/27660 ( 76%)], Train Loss: 0.43244\n",
      "Epoch: 00 [21084/27660 ( 76%)], Train Loss: 0.43223\n",
      "Epoch: 00 [21124/27660 ( 76%)], Train Loss: 0.43210\n",
      "Epoch: 00 [21164/27660 ( 77%)], Train Loss: 0.43184\n",
      "Epoch: 00 [21204/27660 ( 77%)], Train Loss: 0.43171\n",
      "Epoch: 00 [21244/27660 ( 77%)], Train Loss: 0.43173\n",
      "Epoch: 00 [21284/27660 ( 77%)], Train Loss: 0.43150\n",
      "Epoch: 00 [21324/27660 ( 77%)], Train Loss: 0.43131\n",
      "Epoch: 00 [21364/27660 ( 77%)], Train Loss: 0.43083\n",
      "Epoch: 00 [21404/27660 ( 77%)], Train Loss: 0.43066\n",
      "Epoch: 00 [21444/27660 ( 78%)], Train Loss: 0.43028\n",
      "Epoch: 00 [21484/27660 ( 78%)], Train Loss: 0.42990\n",
      "Epoch: 00 [21524/27660 ( 78%)], Train Loss: 0.42966\n",
      "Epoch: 00 [21564/27660 ( 78%)], Train Loss: 0.42928\n",
      "Epoch: 00 [21604/27660 ( 78%)], Train Loss: 0.42889\n",
      "Epoch: 00 [21644/27660 ( 78%)], Train Loss: 0.42903\n",
      "Epoch: 00 [21684/27660 ( 78%)], Train Loss: 0.42873\n",
      "Epoch: 00 [21724/27660 ( 79%)], Train Loss: 0.42855\n",
      "Epoch: 00 [21764/27660 ( 79%)], Train Loss: 0.42824\n",
      "Epoch: 00 [21804/27660 ( 79%)], Train Loss: 0.42792\n",
      "Epoch: 00 [21844/27660 ( 79%)], Train Loss: 0.42766\n",
      "Epoch: 00 [21884/27660 ( 79%)], Train Loss: 0.42756\n",
      "Epoch: 00 [21924/27660 ( 79%)], Train Loss: 0.42731\n",
      "Epoch: 00 [21964/27660 ( 79%)], Train Loss: 0.42704\n",
      "Epoch: 00 [22004/27660 ( 80%)], Train Loss: 0.42675\n",
      "Epoch: 00 [22044/27660 ( 80%)], Train Loss: 0.42647\n",
      "Epoch: 00 [22084/27660 ( 80%)], Train Loss: 0.42638\n",
      "Epoch: 00 [22124/27660 ( 80%)], Train Loss: 0.42613\n",
      "Epoch: 00 [22164/27660 ( 80%)], Train Loss: 0.42595\n",
      "Epoch: 00 [22204/27660 ( 80%)], Train Loss: 0.42573\n",
      "Epoch: 00 [22244/27660 ( 80%)], Train Loss: 0.42546\n",
      "Epoch: 00 [22284/27660 ( 81%)], Train Loss: 0.42507\n",
      "Epoch: 00 [22324/27660 ( 81%)], Train Loss: 0.42479\n",
      "Epoch: 00 [22364/27660 ( 81%)], Train Loss: 0.42456\n",
      "Epoch: 00 [22404/27660 ( 81%)], Train Loss: 0.42441\n",
      "Epoch: 00 [22444/27660 ( 81%)], Train Loss: 0.42412\n",
      "Epoch: 00 [22484/27660 ( 81%)], Train Loss: 0.42389\n",
      "Epoch: 00 [22524/27660 ( 81%)], Train Loss: 0.42367\n",
      "Epoch: 00 [22564/27660 ( 82%)], Train Loss: 0.42353\n",
      "Epoch: 00 [22604/27660 ( 82%)], Train Loss: 0.42345\n",
      "Epoch: 00 [22644/27660 ( 82%)], Train Loss: 0.42324\n",
      "Epoch: 00 [22684/27660 ( 82%)], Train Loss: 0.42298\n",
      "Epoch: 00 [22724/27660 ( 82%)], Train Loss: 0.42282\n",
      "Epoch: 00 [22764/27660 ( 82%)], Train Loss: 0.42248\n",
      "Epoch: 00 [22804/27660 ( 82%)], Train Loss: 0.42215\n",
      "Epoch: 00 [22844/27660 ( 83%)], Train Loss: 0.42207\n",
      "Epoch: 00 [22884/27660 ( 83%)], Train Loss: 0.42174\n",
      "Epoch: 00 [22924/27660 ( 83%)], Train Loss: 0.42153\n",
      "Epoch: 00 [22964/27660 ( 83%)], Train Loss: 0.42113\n",
      "Epoch: 00 [23004/27660 ( 83%)], Train Loss: 0.42085\n",
      "Epoch: 00 [23044/27660 ( 83%)], Train Loss: 0.42051\n",
      "Epoch: 00 [23084/27660 ( 83%)], Train Loss: 0.42020\n",
      "Epoch: 00 [23124/27660 ( 84%)], Train Loss: 0.41980\n",
      "Epoch: 00 [23164/27660 ( 84%)], Train Loss: 0.41949\n",
      "Epoch: 00 [23204/27660 ( 84%)], Train Loss: 0.41913\n",
      "Epoch: 00 [23244/27660 ( 84%)], Train Loss: 0.41904\n",
      "Epoch: 00 [23284/27660 ( 84%)], Train Loss: 0.41894\n",
      "Epoch: 00 [23324/27660 ( 84%)], Train Loss: 0.41896\n",
      "Epoch: 00 [23364/27660 ( 84%)], Train Loss: 0.41886\n",
      "Epoch: 00 [23404/27660 ( 85%)], Train Loss: 0.41869\n",
      "Epoch: 00 [23444/27660 ( 85%)], Train Loss: 0.41861\n",
      "Epoch: 00 [23484/27660 ( 85%)], Train Loss: 0.41831\n",
      "Epoch: 00 [23524/27660 ( 85%)], Train Loss: 0.41824\n",
      "Epoch: 00 [23564/27660 ( 85%)], Train Loss: 0.41843\n",
      "Epoch: 00 [23604/27660 ( 85%)], Train Loss: 0.41825\n",
      "Epoch: 00 [23644/27660 ( 85%)], Train Loss: 0.41820\n",
      "Epoch: 00 [23684/27660 ( 86%)], Train Loss: 0.41802\n",
      "Epoch: 00 [23724/27660 ( 86%)], Train Loss: 0.41789\n",
      "Epoch: 00 [23764/27660 ( 86%)], Train Loss: 0.41776\n",
      "Epoch: 00 [23804/27660 ( 86%)], Train Loss: 0.41750\n",
      "Epoch: 00 [23844/27660 ( 86%)], Train Loss: 0.41733\n",
      "Epoch: 00 [23884/27660 ( 86%)], Train Loss: 0.41751\n",
      "Epoch: 00 [23924/27660 ( 86%)], Train Loss: 0.41707\n",
      "Epoch: 00 [23964/27660 ( 87%)], Train Loss: 0.41680\n",
      "Epoch: 00 [24004/27660 ( 87%)], Train Loss: 0.41685\n",
      "Epoch: 00 [24044/27660 ( 87%)], Train Loss: 0.41652\n",
      "Epoch: 00 [24084/27660 ( 87%)], Train Loss: 0.41625\n",
      "Epoch: 00 [24124/27660 ( 87%)], Train Loss: 0.41614\n",
      "Epoch: 00 [24164/27660 ( 87%)], Train Loss: 0.41580\n",
      "Epoch: 00 [24204/27660 ( 88%)], Train Loss: 0.41547\n",
      "Epoch: 00 [24244/27660 ( 88%)], Train Loss: 0.41505\n",
      "Epoch: 00 [24284/27660 ( 88%)], Train Loss: 0.41485\n",
      "Epoch: 00 [24324/27660 ( 88%)], Train Loss: 0.41463\n",
      "Epoch: 00 [24364/27660 ( 88%)], Train Loss: 0.41427\n",
      "Epoch: 00 [24404/27660 ( 88%)], Train Loss: 0.41405\n",
      "Epoch: 00 [24444/27660 ( 88%)], Train Loss: 0.41369\n",
      "Epoch: 00 [24484/27660 ( 89%)], Train Loss: 0.41345\n",
      "Epoch: 00 [24524/27660 ( 89%)], Train Loss: 0.41311\n",
      "Epoch: 00 [24564/27660 ( 89%)], Train Loss: 0.41281\n",
      "Epoch: 00 [24604/27660 ( 89%)], Train Loss: 0.41263\n",
      "Epoch: 00 [24644/27660 ( 89%)], Train Loss: 0.41255\n",
      "Epoch: 00 [24684/27660 ( 89%)], Train Loss: 0.41222\n",
      "Epoch: 00 [24724/27660 ( 89%)], Train Loss: 0.41204\n",
      "Epoch: 00 [24764/27660 ( 90%)], Train Loss: 0.41184\n",
      "Epoch: 00 [24804/27660 ( 90%)], Train Loss: 0.41154\n",
      "Epoch: 00 [24844/27660 ( 90%)], Train Loss: 0.41123\n",
      "Epoch: 00 [24884/27660 ( 90%)], Train Loss: 0.41113\n",
      "Epoch: 00 [24924/27660 ( 90%)], Train Loss: 0.41105\n",
      "Epoch: 00 [24964/27660 ( 90%)], Train Loss: 0.41083\n",
      "Epoch: 00 [25004/27660 ( 90%)], Train Loss: 0.41057\n",
      "Epoch: 00 [25044/27660 ( 91%)], Train Loss: 0.41044\n",
      "Epoch: 00 [25084/27660 ( 91%)], Train Loss: 0.41027\n",
      "Epoch: 00 [25124/27660 ( 91%)], Train Loss: 0.41002\n",
      "Epoch: 00 [25164/27660 ( 91%)], Train Loss: 0.40975\n",
      "Epoch: 00 [25204/27660 ( 91%)], Train Loss: 0.40953\n",
      "Epoch: 00 [25244/27660 ( 91%)], Train Loss: 0.40988\n",
      "Epoch: 00 [25284/27660 ( 91%)], Train Loss: 0.40972\n",
      "Epoch: 00 [25324/27660 ( 92%)], Train Loss: 0.40941\n",
      "Epoch: 00 [25364/27660 ( 92%)], Train Loss: 0.40927\n",
      "Epoch: 00 [25404/27660 ( 92%)], Train Loss: 0.40884\n",
      "Epoch: 00 [25444/27660 ( 92%)], Train Loss: 0.40878\n",
      "Epoch: 00 [25484/27660 ( 92%)], Train Loss: 0.40871\n",
      "Epoch: 00 [25524/27660 ( 92%)], Train Loss: 0.40866\n",
      "Epoch: 00 [25564/27660 ( 92%)], Train Loss: 0.40858\n",
      "Epoch: 00 [25604/27660 ( 93%)], Train Loss: 0.40855\n",
      "Epoch: 00 [25644/27660 ( 93%)], Train Loss: 0.40860\n",
      "Epoch: 00 [25684/27660 ( 93%)], Train Loss: 0.40849\n",
      "Epoch: 00 [25724/27660 ( 93%)], Train Loss: 0.40848\n",
      "Epoch: 00 [25764/27660 ( 93%)], Train Loss: 0.40837\n",
      "Epoch: 00 [25804/27660 ( 93%)], Train Loss: 0.40825\n",
      "Epoch: 00 [25844/27660 ( 93%)], Train Loss: 0.40801\n",
      "Epoch: 00 [25884/27660 ( 94%)], Train Loss: 0.40773\n",
      "Epoch: 00 [25924/27660 ( 94%)], Train Loss: 0.40764\n",
      "Epoch: 00 [25964/27660 ( 94%)], Train Loss: 0.40751\n",
      "Epoch: 00 [26004/27660 ( 94%)], Train Loss: 0.40764\n",
      "Epoch: 00 [26044/27660 ( 94%)], Train Loss: 0.40752\n",
      "Epoch: 00 [26084/27660 ( 94%)], Train Loss: 0.40730\n",
      "Epoch: 00 [26124/27660 ( 94%)], Train Loss: 0.40716\n",
      "Epoch: 00 [26164/27660 ( 95%)], Train Loss: 0.40711\n",
      "Epoch: 00 [26204/27660 ( 95%)], Train Loss: 0.40688\n",
      "Epoch: 00 [26244/27660 ( 95%)], Train Loss: 0.40673\n",
      "Epoch: 00 [26284/27660 ( 95%)], Train Loss: 0.40646\n",
      "Epoch: 00 [26324/27660 ( 95%)], Train Loss: 0.40631\n",
      "Epoch: 00 [26364/27660 ( 95%)], Train Loss: 0.40611\n",
      "Epoch: 00 [26404/27660 ( 95%)], Train Loss: 0.40602\n",
      "Epoch: 00 [26444/27660 ( 96%)], Train Loss: 0.40596\n",
      "Epoch: 00 [26484/27660 ( 96%)], Train Loss: 0.40569\n",
      "Epoch: 00 [26524/27660 ( 96%)], Train Loss: 0.40561\n",
      "Epoch: 00 [26564/27660 ( 96%)], Train Loss: 0.40528\n",
      "Epoch: 00 [26604/27660 ( 96%)], Train Loss: 0.40500\n",
      "Epoch: 00 [26644/27660 ( 96%)], Train Loss: 0.40491\n",
      "Epoch: 00 [26684/27660 ( 96%)], Train Loss: 0.40479\n",
      "Epoch: 00 [26724/27660 ( 97%)], Train Loss: 0.40461\n",
      "Epoch: 00 [26764/27660 ( 97%)], Train Loss: 0.40437\n",
      "Epoch: 00 [26804/27660 ( 97%)], Train Loss: 0.40410\n",
      "Epoch: 00 [26844/27660 ( 97%)], Train Loss: 0.40389\n",
      "Epoch: 00 [26884/27660 ( 97%)], Train Loss: 0.40371\n",
      "Epoch: 00 [26924/27660 ( 97%)], Train Loss: 0.40353\n",
      "Epoch: 00 [26964/27660 ( 97%)], Train Loss: 0.40324\n",
      "Epoch: 00 [27004/27660 ( 98%)], Train Loss: 0.40301\n",
      "Epoch: 00 [27044/27660 ( 98%)], Train Loss: 0.40280\n",
      "Epoch: 00 [27084/27660 ( 98%)], Train Loss: 0.40284\n",
      "Epoch: 00 [27124/27660 ( 98%)], Train Loss: 0.40305\n",
      "Epoch: 00 [27164/27660 ( 98%)], Train Loss: 0.40272\n",
      "Epoch: 00 [27204/27660 ( 98%)], Train Loss: 0.40283\n",
      "Epoch: 00 [27244/27660 ( 98%)], Train Loss: 0.40272\n",
      "Epoch: 00 [27284/27660 ( 99%)], Train Loss: 0.40285\n",
      "Epoch: 00 [27324/27660 ( 99%)], Train Loss: 0.40277\n",
      "Epoch: 00 [27364/27660 ( 99%)], Train Loss: 0.40246\n",
      "Epoch: 00 [27404/27660 ( 99%)], Train Loss: 0.40230\n",
      "Epoch: 00 [27444/27660 ( 99%)], Train Loss: 0.40216\n",
      "Epoch: 00 [27484/27660 ( 99%)], Train Loss: 0.40192\n",
      "Epoch: 00 [27524/27660 (100%)], Train Loss: 0.40184\n",
      "Epoch: 00 [27564/27660 (100%)], Train Loss: 0.40162\n",
      "Epoch: 00 [27604/27660 (100%)], Train Loss: 0.40138\n",
      "Epoch: 00 [27644/27660 (100%)], Train Loss: 0.40128\n",
      "Epoch: 00 [27660/27660 (100%)], Train Loss: 0.40128\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.61113\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.61113\n",
      "Saving model checkpoint to output/checkpoint-fold-3.\n",
      "\n",
      "Total Training Time: 3219.7496457099915secs, Average Training Time per Epoch: 3219.7496457099915secs.\n",
      "Total Validation Time: 145.3993420600891secs, Average Validation Time per Epoch: 145.3993420600891secs.\n",
      "--------------------------------------------------\n",
      "FOLD: 4\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27778, Num examples Valid=3944\n",
      "Total Training Steps: 3473, Total Warmup Steps: 347\n",
      "Epoch: 00 [    4/27778 (  0%)], Train Loss: 2.67932\n",
      "Epoch: 00 [   44/27778 (  0%)], Train Loss: 2.78495\n",
      "Epoch: 00 [   84/27778 (  0%)], Train Loss: 2.75761\n",
      "Epoch: 00 [  124/27778 (  0%)], Train Loss: 2.73842\n",
      "Epoch: 00 [  164/27778 (  1%)], Train Loss: 2.70424\n",
      "Epoch: 00 [  204/27778 (  1%)], Train Loss: 2.67170\n",
      "Epoch: 00 [  244/27778 (  1%)], Train Loss: 2.63200\n",
      "Epoch: 00 [  284/27778 (  1%)], Train Loss: 2.58670\n",
      "Epoch: 00 [  324/27778 (  1%)], Train Loss: 2.52552\n",
      "Epoch: 00 [  364/27778 (  1%)], Train Loss: 2.46106\n",
      "Epoch: 00 [  404/27778 (  1%)], Train Loss: 2.39716\n",
      "Epoch: 00 [  444/27778 (  2%)], Train Loss: 2.33712\n",
      "Epoch: 00 [  484/27778 (  2%)], Train Loss: 2.25031\n",
      "Epoch: 00 [  524/27778 (  2%)], Train Loss: 2.17263\n",
      "Epoch: 00 [  564/27778 (  2%)], Train Loss: 2.10321\n",
      "Epoch: 00 [  604/27778 (  2%)], Train Loss: 2.01971\n",
      "Epoch: 00 [  644/27778 (  2%)], Train Loss: 1.94206\n",
      "Epoch: 00 [  684/27778 (  2%)], Train Loss: 1.87120\n",
      "Epoch: 00 [  724/27778 (  3%)], Train Loss: 1.80091\n",
      "Epoch: 00 [  764/27778 (  3%)], Train Loss: 1.74350\n",
      "Epoch: 00 [  804/27778 (  3%)], Train Loss: 1.69222\n",
      "Epoch: 00 [  844/27778 (  3%)], Train Loss: 1.64482\n",
      "Epoch: 00 [  884/27778 (  3%)], Train Loss: 1.60623\n",
      "Epoch: 00 [  924/27778 (  3%)], Train Loss: 1.56199\n",
      "Epoch: 00 [  964/27778 (  3%)], Train Loss: 1.52351\n",
      "Epoch: 00 [ 1004/27778 (  4%)], Train Loss: 1.48570\n",
      "Epoch: 00 [ 1044/27778 (  4%)], Train Loss: 1.44389\n",
      "Epoch: 00 [ 1084/27778 (  4%)], Train Loss: 1.41041\n",
      "Epoch: 00 [ 1124/27778 (  4%)], Train Loss: 1.37990\n",
      "Epoch: 00 [ 1164/27778 (  4%)], Train Loss: 1.35189\n",
      "Epoch: 00 [ 1204/27778 (  4%)], Train Loss: 1.32463\n",
      "Epoch: 00 [ 1244/27778 (  4%)], Train Loss: 1.29569\n",
      "Epoch: 00 [ 1284/27778 (  5%)], Train Loss: 1.27335\n",
      "Epoch: 00 [ 1324/27778 (  5%)], Train Loss: 1.24902\n",
      "Epoch: 00 [ 1364/27778 (  5%)], Train Loss: 1.22733\n",
      "Epoch: 00 [ 1404/27778 (  5%)], Train Loss: 1.20945\n",
      "Epoch: 00 [ 1444/27778 (  5%)], Train Loss: 1.19333\n",
      "Epoch: 00 [ 1484/27778 (  5%)], Train Loss: 1.17021\n",
      "Epoch: 00 [ 1524/27778 (  5%)], Train Loss: 1.15496\n",
      "Epoch: 00 [ 1564/27778 (  6%)], Train Loss: 1.13682\n",
      "Epoch: 00 [ 1604/27778 (  6%)], Train Loss: 1.12506\n",
      "Epoch: 00 [ 1644/27778 (  6%)], Train Loss: 1.11162\n",
      "Epoch: 00 [ 1684/27778 (  6%)], Train Loss: 1.09936\n",
      "Epoch: 00 [ 1724/27778 (  6%)], Train Loss: 1.08422\n",
      "Epoch: 00 [ 1764/27778 (  6%)], Train Loss: 1.07126\n",
      "Epoch: 00 [ 1804/27778 (  6%)], Train Loss: 1.05637\n",
      "Epoch: 00 [ 1844/27778 (  7%)], Train Loss: 1.04419\n",
      "Epoch: 00 [ 1884/27778 (  7%)], Train Loss: 1.02936\n",
      "Epoch: 00 [ 1924/27778 (  7%)], Train Loss: 1.01483\n",
      "Epoch: 00 [ 1964/27778 (  7%)], Train Loss: 0.99937\n",
      "Epoch: 00 [ 2004/27778 (  7%)], Train Loss: 0.98896\n",
      "Epoch: 00 [ 2044/27778 (  7%)], Train Loss: 0.98346\n",
      "Epoch: 00 [ 2084/27778 (  8%)], Train Loss: 0.97788\n",
      "Epoch: 00 [ 2124/27778 (  8%)], Train Loss: 0.96870\n",
      "Epoch: 00 [ 2164/27778 (  8%)], Train Loss: 0.95871\n",
      "Epoch: 00 [ 2204/27778 (  8%)], Train Loss: 0.95125\n",
      "Epoch: 00 [ 2244/27778 (  8%)], Train Loss: 0.94157\n",
      "Epoch: 00 [ 2284/27778 (  8%)], Train Loss: 0.93324\n",
      "Epoch: 00 [ 2324/27778 (  8%)], Train Loss: 0.92715\n",
      "Epoch: 00 [ 2364/27778 (  9%)], Train Loss: 0.91760\n",
      "Epoch: 00 [ 2404/27778 (  9%)], Train Loss: 0.90949\n",
      "Epoch: 00 [ 2444/27778 (  9%)], Train Loss: 0.90408\n",
      "Epoch: 00 [ 2484/27778 (  9%)], Train Loss: 0.89809\n",
      "Epoch: 00 [ 2524/27778 (  9%)], Train Loss: 0.89001\n",
      "Epoch: 00 [ 2564/27778 (  9%)], Train Loss: 0.88248\n",
      "Epoch: 00 [ 2604/27778 (  9%)], Train Loss: 0.87556\n",
      "Epoch: 00 [ 2644/27778 ( 10%)], Train Loss: 0.87209\n",
      "Epoch: 00 [ 2684/27778 ( 10%)], Train Loss: 0.86518\n",
      "Epoch: 00 [ 2724/27778 ( 10%)], Train Loss: 0.85672\n",
      "Epoch: 00 [ 2764/27778 ( 10%)], Train Loss: 0.85230\n",
      "Epoch: 00 [ 2804/27778 ( 10%)], Train Loss: 0.84760\n",
      "Epoch: 00 [ 2844/27778 ( 10%)], Train Loss: 0.84308\n",
      "Epoch: 00 [ 2884/27778 ( 10%)], Train Loss: 0.83839\n",
      "Epoch: 00 [ 2924/27778 ( 11%)], Train Loss: 0.83503\n",
      "Epoch: 00 [ 2964/27778 ( 11%)], Train Loss: 0.82903\n",
      "Epoch: 00 [ 3004/27778 ( 11%)], Train Loss: 0.82481\n",
      "Epoch: 00 [ 3044/27778 ( 11%)], Train Loss: 0.82246\n",
      "Epoch: 00 [ 3084/27778 ( 11%)], Train Loss: 0.81676\n",
      "Epoch: 00 [ 3124/27778 ( 11%)], Train Loss: 0.81064\n",
      "Epoch: 00 [ 3164/27778 ( 11%)], Train Loss: 0.80667\n",
      "Epoch: 00 [ 3204/27778 ( 12%)], Train Loss: 0.80070\n",
      "Epoch: 00 [ 3244/27778 ( 12%)], Train Loss: 0.79640\n",
      "Epoch: 00 [ 3284/27778 ( 12%)], Train Loss: 0.79099\n",
      "Epoch: 00 [ 3324/27778 ( 12%)], Train Loss: 0.78745\n",
      "Epoch: 00 [ 3364/27778 ( 12%)], Train Loss: 0.78387\n",
      "Epoch: 00 [ 3404/27778 ( 12%)], Train Loss: 0.78148\n",
      "Epoch: 00 [ 3444/27778 ( 12%)], Train Loss: 0.77864\n",
      "Epoch: 00 [ 3484/27778 ( 13%)], Train Loss: 0.77690\n",
      "Epoch: 00 [ 3524/27778 ( 13%)], Train Loss: 0.77348\n",
      "Epoch: 00 [ 3564/27778 ( 13%)], Train Loss: 0.76968\n",
      "Epoch: 00 [ 3604/27778 ( 13%)], Train Loss: 0.76771\n",
      "Epoch: 00 [ 3644/27778 ( 13%)], Train Loss: 0.76666\n",
      "Epoch: 00 [ 3684/27778 ( 13%)], Train Loss: 0.76343\n",
      "Epoch: 00 [ 3724/27778 ( 13%)], Train Loss: 0.76079\n",
      "Epoch: 00 [ 3764/27778 ( 14%)], Train Loss: 0.75723\n",
      "Epoch: 00 [ 3804/27778 ( 14%)], Train Loss: 0.75393\n",
      "Epoch: 00 [ 3844/27778 ( 14%)], Train Loss: 0.75067\n",
      "Epoch: 00 [ 3884/27778 ( 14%)], Train Loss: 0.75173\n",
      "Epoch: 00 [ 3924/27778 ( 14%)], Train Loss: 0.74939\n",
      "Epoch: 00 [ 3964/27778 ( 14%)], Train Loss: 0.74707\n",
      "Epoch: 00 [ 4004/27778 ( 14%)], Train Loss: 0.74327\n",
      "Epoch: 00 [ 4044/27778 ( 15%)], Train Loss: 0.73925\n",
      "Epoch: 00 [ 4084/27778 ( 15%)], Train Loss: 0.73559\n",
      "Epoch: 00 [ 4124/27778 ( 15%)], Train Loss: 0.73413\n",
      "Epoch: 00 [ 4164/27778 ( 15%)], Train Loss: 0.73068\n",
      "Epoch: 00 [ 4204/27778 ( 15%)], Train Loss: 0.72745\n",
      "Epoch: 00 [ 4244/27778 ( 15%)], Train Loss: 0.72221\n",
      "Epoch: 00 [ 4284/27778 ( 15%)], Train Loss: 0.72051\n",
      "Epoch: 00 [ 4324/27778 ( 16%)], Train Loss: 0.71777\n",
      "Epoch: 00 [ 4364/27778 ( 16%)], Train Loss: 0.71540\n",
      "Epoch: 00 [ 4404/27778 ( 16%)], Train Loss: 0.71337\n",
      "Epoch: 00 [ 4444/27778 ( 16%)], Train Loss: 0.71076\n",
      "Epoch: 00 [ 4484/27778 ( 16%)], Train Loss: 0.70758\n",
      "Epoch: 00 [ 4524/27778 ( 16%)], Train Loss: 0.70498\n",
      "Epoch: 00 [ 4564/27778 ( 16%)], Train Loss: 0.70411\n",
      "Epoch: 00 [ 4604/27778 ( 17%)], Train Loss: 0.70255\n",
      "Epoch: 00 [ 4644/27778 ( 17%)], Train Loss: 0.70088\n",
      "Epoch: 00 [ 4684/27778 ( 17%)], Train Loss: 0.69850\n",
      "Epoch: 00 [ 4724/27778 ( 17%)], Train Loss: 0.69686\n",
      "Epoch: 00 [ 4764/27778 ( 17%)], Train Loss: 0.69384\n",
      "Epoch: 00 [ 4804/27778 ( 17%)], Train Loss: 0.69327\n",
      "Epoch: 00 [ 4844/27778 ( 17%)], Train Loss: 0.69097\n",
      "Epoch: 00 [ 4884/27778 ( 18%)], Train Loss: 0.68936\n",
      "Epoch: 00 [ 4924/27778 ( 18%)], Train Loss: 0.68686\n",
      "Epoch: 00 [ 4964/27778 ( 18%)], Train Loss: 0.68428\n",
      "Epoch: 00 [ 5004/27778 ( 18%)], Train Loss: 0.68386\n",
      "Epoch: 00 [ 5044/27778 ( 18%)], Train Loss: 0.68094\n",
      "Epoch: 00 [ 5084/27778 ( 18%)], Train Loss: 0.67789\n",
      "Epoch: 00 [ 5124/27778 ( 18%)], Train Loss: 0.67630\n",
      "Epoch: 00 [ 5164/27778 ( 19%)], Train Loss: 0.67331\n",
      "Epoch: 00 [ 5204/27778 ( 19%)], Train Loss: 0.67035\n",
      "Epoch: 00 [ 5244/27778 ( 19%)], Train Loss: 0.66854\n",
      "Epoch: 00 [ 5284/27778 ( 19%)], Train Loss: 0.66710\n",
      "Epoch: 00 [ 5324/27778 ( 19%)], Train Loss: 0.66629\n",
      "Epoch: 00 [ 5364/27778 ( 19%)], Train Loss: 0.66357\n",
      "Epoch: 00 [ 5404/27778 ( 19%)], Train Loss: 0.66092\n",
      "Epoch: 00 [ 5444/27778 ( 20%)], Train Loss: 0.65859\n",
      "Epoch: 00 [ 5484/27778 ( 20%)], Train Loss: 0.65770\n",
      "Epoch: 00 [ 5524/27778 ( 20%)], Train Loss: 0.65690\n",
      "Epoch: 00 [ 5564/27778 ( 20%)], Train Loss: 0.65499\n",
      "Epoch: 00 [ 5604/27778 ( 20%)], Train Loss: 0.65374\n",
      "Epoch: 00 [ 5644/27778 ( 20%)], Train Loss: 0.65255\n",
      "Epoch: 00 [ 5684/27778 ( 20%)], Train Loss: 0.65116\n",
      "Epoch: 00 [ 5724/27778 ( 21%)], Train Loss: 0.64991\n",
      "Epoch: 00 [ 5764/27778 ( 21%)], Train Loss: 0.64900\n",
      "Epoch: 00 [ 5804/27778 ( 21%)], Train Loss: 0.64785\n",
      "Epoch: 00 [ 5844/27778 ( 21%)], Train Loss: 0.64620\n",
      "Epoch: 00 [ 5884/27778 ( 21%)], Train Loss: 0.64471\n",
      "Epoch: 00 [ 5924/27778 ( 21%)], Train Loss: 0.64372\n",
      "Epoch: 00 [ 5964/27778 ( 21%)], Train Loss: 0.64145\n",
      "Epoch: 00 [ 6004/27778 ( 22%)], Train Loss: 0.64014\n",
      "Epoch: 00 [ 6044/27778 ( 22%)], Train Loss: 0.63858\n",
      "Epoch: 00 [ 6084/27778 ( 22%)], Train Loss: 0.63753\n",
      "Epoch: 00 [ 6124/27778 ( 22%)], Train Loss: 0.63537\n",
      "Epoch: 00 [ 6164/27778 ( 22%)], Train Loss: 0.63419\n",
      "Epoch: 00 [ 6204/27778 ( 22%)], Train Loss: 0.63303\n",
      "Epoch: 00 [ 6244/27778 ( 22%)], Train Loss: 0.63191\n",
      "Epoch: 00 [ 6284/27778 ( 23%)], Train Loss: 0.62987\n",
      "Epoch: 00 [ 6324/27778 ( 23%)], Train Loss: 0.62831\n",
      "Epoch: 00 [ 6364/27778 ( 23%)], Train Loss: 0.62649\n",
      "Epoch: 00 [ 6404/27778 ( 23%)], Train Loss: 0.62481\n",
      "Epoch: 00 [ 6444/27778 ( 23%)], Train Loss: 0.62396\n",
      "Epoch: 00 [ 6484/27778 ( 23%)], Train Loss: 0.62187\n",
      "Epoch: 00 [ 6524/27778 ( 23%)], Train Loss: 0.61987\n",
      "Epoch: 00 [ 6564/27778 ( 24%)], Train Loss: 0.61939\n",
      "Epoch: 00 [ 6604/27778 ( 24%)], Train Loss: 0.61774\n",
      "Epoch: 00 [ 6644/27778 ( 24%)], Train Loss: 0.61871\n",
      "Epoch: 00 [ 6684/27778 ( 24%)], Train Loss: 0.61716\n",
      "Epoch: 00 [ 6724/27778 ( 24%)], Train Loss: 0.61629\n",
      "Epoch: 00 [ 6764/27778 ( 24%)], Train Loss: 0.61535\n",
      "Epoch: 00 [ 6804/27778 ( 24%)], Train Loss: 0.61395\n",
      "Epoch: 00 [ 6844/27778 ( 25%)], Train Loss: 0.61262\n",
      "Epoch: 00 [ 6884/27778 ( 25%)], Train Loss: 0.61115\n",
      "Epoch: 00 [ 6924/27778 ( 25%)], Train Loss: 0.61056\n",
      "Epoch: 00 [ 6964/27778 ( 25%)], Train Loss: 0.60866\n",
      "Epoch: 00 [ 7004/27778 ( 25%)], Train Loss: 0.60764\n",
      "Epoch: 00 [ 7044/27778 ( 25%)], Train Loss: 0.60599\n",
      "Epoch: 00 [ 7084/27778 ( 26%)], Train Loss: 0.60430\n",
      "Epoch: 00 [ 7124/27778 ( 26%)], Train Loss: 0.60325\n",
      "Epoch: 00 [ 7164/27778 ( 26%)], Train Loss: 0.60219\n",
      "Epoch: 00 [ 7204/27778 ( 26%)], Train Loss: 0.60121\n",
      "Epoch: 00 [ 7244/27778 ( 26%)], Train Loss: 0.59952\n",
      "Epoch: 00 [ 7284/27778 ( 26%)], Train Loss: 0.59920\n",
      "Epoch: 00 [ 7324/27778 ( 26%)], Train Loss: 0.59934\n",
      "Epoch: 00 [ 7364/27778 ( 27%)], Train Loss: 0.59850\n",
      "Epoch: 00 [ 7404/27778 ( 27%)], Train Loss: 0.59768\n",
      "Epoch: 00 [ 7444/27778 ( 27%)], Train Loss: 0.59712\n",
      "Epoch: 00 [ 7484/27778 ( 27%)], Train Loss: 0.59636\n",
      "Epoch: 00 [ 7524/27778 ( 27%)], Train Loss: 0.59491\n",
      "Epoch: 00 [ 7564/27778 ( 27%)], Train Loss: 0.59363\n",
      "Epoch: 00 [ 7604/27778 ( 27%)], Train Loss: 0.59279\n",
      "Epoch: 00 [ 7644/27778 ( 28%)], Train Loss: 0.59329\n",
      "Epoch: 00 [ 7684/27778 ( 28%)], Train Loss: 0.59217\n",
      "Epoch: 00 [ 7724/27778 ( 28%)], Train Loss: 0.59100\n",
      "Epoch: 00 [ 7764/27778 ( 28%)], Train Loss: 0.58995\n",
      "Epoch: 00 [ 7804/27778 ( 28%)], Train Loss: 0.58891\n",
      "Epoch: 00 [ 7844/27778 ( 28%)], Train Loss: 0.58795\n",
      "Epoch: 00 [ 7884/27778 ( 28%)], Train Loss: 0.58688\n",
      "Epoch: 00 [ 7924/27778 ( 29%)], Train Loss: 0.58559\n",
      "Epoch: 00 [ 7964/27778 ( 29%)], Train Loss: 0.58466\n",
      "Epoch: 00 [ 8004/27778 ( 29%)], Train Loss: 0.58324\n",
      "Epoch: 00 [ 8044/27778 ( 29%)], Train Loss: 0.58243\n",
      "Epoch: 00 [ 8084/27778 ( 29%)], Train Loss: 0.58172\n",
      "Epoch: 00 [ 8124/27778 ( 29%)], Train Loss: 0.58076\n",
      "Epoch: 00 [ 8164/27778 ( 29%)], Train Loss: 0.57973\n",
      "Epoch: 00 [ 8204/27778 ( 30%)], Train Loss: 0.57838\n",
      "Epoch: 00 [ 8244/27778 ( 30%)], Train Loss: 0.57676\n",
      "Epoch: 00 [ 8284/27778 ( 30%)], Train Loss: 0.57672\n",
      "Epoch: 00 [ 8324/27778 ( 30%)], Train Loss: 0.57573\n",
      "Epoch: 00 [ 8364/27778 ( 30%)], Train Loss: 0.57483\n",
      "Epoch: 00 [ 8404/27778 ( 30%)], Train Loss: 0.57421\n",
      "Epoch: 00 [ 8444/27778 ( 30%)], Train Loss: 0.57303\n",
      "Epoch: 00 [ 8484/27778 ( 31%)], Train Loss: 0.57318\n",
      "Epoch: 00 [ 8524/27778 ( 31%)], Train Loss: 0.57295\n",
      "Epoch: 00 [ 8564/27778 ( 31%)], Train Loss: 0.57141\n",
      "Epoch: 00 [ 8604/27778 ( 31%)], Train Loss: 0.57107\n",
      "Epoch: 00 [ 8644/27778 ( 31%)], Train Loss: 0.57025\n",
      "Epoch: 00 [ 8684/27778 ( 31%)], Train Loss: 0.57024\n",
      "Epoch: 00 [ 8724/27778 ( 31%)], Train Loss: 0.56935\n",
      "Epoch: 00 [ 8764/27778 ( 32%)], Train Loss: 0.56881\n",
      "Epoch: 00 [ 8804/27778 ( 32%)], Train Loss: 0.56800\n",
      "Epoch: 00 [ 8844/27778 ( 32%)], Train Loss: 0.56632\n",
      "Epoch: 00 [ 8884/27778 ( 32%)], Train Loss: 0.56623\n",
      "Epoch: 00 [ 8924/27778 ( 32%)], Train Loss: 0.56528\n",
      "Epoch: 00 [ 8964/27778 ( 32%)], Train Loss: 0.56464\n",
      "Epoch: 00 [ 9004/27778 ( 32%)], Train Loss: 0.56389\n",
      "Epoch: 00 [ 9044/27778 ( 33%)], Train Loss: 0.56289\n",
      "Epoch: 00 [ 9084/27778 ( 33%)], Train Loss: 0.56301\n",
      "Epoch: 00 [ 9124/27778 ( 33%)], Train Loss: 0.56200\n",
      "Epoch: 00 [ 9164/27778 ( 33%)], Train Loss: 0.56155\n",
      "Epoch: 00 [ 9204/27778 ( 33%)], Train Loss: 0.56016\n",
      "Epoch: 00 [ 9244/27778 ( 33%)], Train Loss: 0.55944\n",
      "Epoch: 00 [ 9284/27778 ( 33%)], Train Loss: 0.55854\n",
      "Epoch: 00 [ 9324/27778 ( 34%)], Train Loss: 0.55852\n",
      "Epoch: 00 [ 9364/27778 ( 34%)], Train Loss: 0.55757\n",
      "Epoch: 00 [ 9404/27778 ( 34%)], Train Loss: 0.55634\n",
      "Epoch: 00 [ 9444/27778 ( 34%)], Train Loss: 0.55546\n",
      "Epoch: 00 [ 9484/27778 ( 34%)], Train Loss: 0.55461\n",
      "Epoch: 00 [ 9524/27778 ( 34%)], Train Loss: 0.55399\n",
      "Epoch: 00 [ 9564/27778 ( 34%)], Train Loss: 0.55357\n",
      "Epoch: 00 [ 9604/27778 ( 35%)], Train Loss: 0.55242\n",
      "Epoch: 00 [ 9644/27778 ( 35%)], Train Loss: 0.55239\n",
      "Epoch: 00 [ 9684/27778 ( 35%)], Train Loss: 0.55213\n",
      "Epoch: 00 [ 9724/27778 ( 35%)], Train Loss: 0.55114\n",
      "Epoch: 00 [ 9764/27778 ( 35%)], Train Loss: 0.54972\n",
      "Epoch: 00 [ 9804/27778 ( 35%)], Train Loss: 0.54900\n",
      "Epoch: 00 [ 9844/27778 ( 35%)], Train Loss: 0.54856\n",
      "Epoch: 00 [ 9884/27778 ( 36%)], Train Loss: 0.54808\n",
      "Epoch: 00 [ 9924/27778 ( 36%)], Train Loss: 0.54713\n",
      "Epoch: 00 [ 9964/27778 ( 36%)], Train Loss: 0.54579\n",
      "Epoch: 00 [10004/27778 ( 36%)], Train Loss: 0.54566\n",
      "Epoch: 00 [10044/27778 ( 36%)], Train Loss: 0.54457\n",
      "Epoch: 00 [10084/27778 ( 36%)], Train Loss: 0.54373\n",
      "Epoch: 00 [10124/27778 ( 36%)], Train Loss: 0.54316\n",
      "Epoch: 00 [10164/27778 ( 37%)], Train Loss: 0.54213\n",
      "Epoch: 00 [10204/27778 ( 37%)], Train Loss: 0.54157\n",
      "Epoch: 00 [10244/27778 ( 37%)], Train Loss: 0.54031\n",
      "Epoch: 00 [10284/27778 ( 37%)], Train Loss: 0.53944\n",
      "Epoch: 00 [10324/27778 ( 37%)], Train Loss: 0.53923\n",
      "Epoch: 00 [10364/27778 ( 37%)], Train Loss: 0.53888\n",
      "Epoch: 00 [10404/27778 ( 37%)], Train Loss: 0.53807\n",
      "Epoch: 00 [10444/27778 ( 38%)], Train Loss: 0.53703\n",
      "Epoch: 00 [10484/27778 ( 38%)], Train Loss: 0.53718\n",
      "Epoch: 00 [10524/27778 ( 38%)], Train Loss: 0.53642\n",
      "Epoch: 00 [10564/27778 ( 38%)], Train Loss: 0.53578\n",
      "Epoch: 00 [10604/27778 ( 38%)], Train Loss: 0.53488\n",
      "Epoch: 00 [10644/27778 ( 38%)], Train Loss: 0.53366\n",
      "Epoch: 00 [10684/27778 ( 38%)], Train Loss: 0.53349\n",
      "Epoch: 00 [10724/27778 ( 39%)], Train Loss: 0.53303\n",
      "Epoch: 00 [10764/27778 ( 39%)], Train Loss: 0.53267\n",
      "Epoch: 00 [10804/27778 ( 39%)], Train Loss: 0.53203\n",
      "Epoch: 00 [10844/27778 ( 39%)], Train Loss: 0.53140\n",
      "Epoch: 00 [10884/27778 ( 39%)], Train Loss: 0.53070\n",
      "Epoch: 00 [10924/27778 ( 39%)], Train Loss: 0.52977\n",
      "Epoch: 00 [10964/27778 ( 39%)], Train Loss: 0.52894\n",
      "Epoch: 00 [11004/27778 ( 40%)], Train Loss: 0.52851\n",
      "Epoch: 00 [11044/27778 ( 40%)], Train Loss: 0.52797\n",
      "Epoch: 00 [11084/27778 ( 40%)], Train Loss: 0.52665\n",
      "Epoch: 00 [11124/27778 ( 40%)], Train Loss: 0.52588\n",
      "Epoch: 00 [11164/27778 ( 40%)], Train Loss: 0.52499\n",
      "Epoch: 00 [11204/27778 ( 40%)], Train Loss: 0.52433\n",
      "Epoch: 00 [11244/27778 ( 40%)], Train Loss: 0.52385\n",
      "Epoch: 00 [11284/27778 ( 41%)], Train Loss: 0.52297\n",
      "Epoch: 00 [11324/27778 ( 41%)], Train Loss: 0.52207\n",
      "Epoch: 00 [11364/27778 ( 41%)], Train Loss: 0.52143\n",
      "Epoch: 00 [11404/27778 ( 41%)], Train Loss: 0.52044\n",
      "Epoch: 00 [11444/27778 ( 41%)], Train Loss: 0.51942\n",
      "Epoch: 00 [11484/27778 ( 41%)], Train Loss: 0.51872\n",
      "Epoch: 00 [11524/27778 ( 41%)], Train Loss: 0.51783\n",
      "Epoch: 00 [11564/27778 ( 42%)], Train Loss: 0.51728\n",
      "Epoch: 00 [11604/27778 ( 42%)], Train Loss: 0.51645\n",
      "Epoch: 00 [11644/27778 ( 42%)], Train Loss: 0.51598\n",
      "Epoch: 00 [11684/27778 ( 42%)], Train Loss: 0.51533\n",
      "Epoch: 00 [11724/27778 ( 42%)], Train Loss: 0.51526\n",
      "Epoch: 00 [11764/27778 ( 42%)], Train Loss: 0.51500\n",
      "Epoch: 00 [11804/27778 ( 42%)], Train Loss: 0.51462\n",
      "Epoch: 00 [11844/27778 ( 43%)], Train Loss: 0.51440\n",
      "Epoch: 00 [11884/27778 ( 43%)], Train Loss: 0.51390\n",
      "Epoch: 00 [11924/27778 ( 43%)], Train Loss: 0.51353\n",
      "Epoch: 00 [11964/27778 ( 43%)], Train Loss: 0.51257\n",
      "Epoch: 00 [12004/27778 ( 43%)], Train Loss: 0.51159\n",
      "Epoch: 00 [12044/27778 ( 43%)], Train Loss: 0.51091\n",
      "Epoch: 00 [12084/27778 ( 44%)], Train Loss: 0.51012\n",
      "Epoch: 00 [12124/27778 ( 44%)], Train Loss: 0.50958\n",
      "Epoch: 00 [12164/27778 ( 44%)], Train Loss: 0.50926\n",
      "Epoch: 00 [12204/27778 ( 44%)], Train Loss: 0.50856\n",
      "Epoch: 00 [12244/27778 ( 44%)], Train Loss: 0.50792\n",
      "Epoch: 00 [12284/27778 ( 44%)], Train Loss: 0.50713\n",
      "Epoch: 00 [12324/27778 ( 44%)], Train Loss: 0.50633\n",
      "Epoch: 00 [12364/27778 ( 45%)], Train Loss: 0.50605\n",
      "Epoch: 00 [12404/27778 ( 45%)], Train Loss: 0.50569\n",
      "Epoch: 00 [12444/27778 ( 45%)], Train Loss: 0.50501\n",
      "Epoch: 00 [12484/27778 ( 45%)], Train Loss: 0.50408\n",
      "Epoch: 00 [12524/27778 ( 45%)], Train Loss: 0.50362\n",
      "Epoch: 00 [12564/27778 ( 45%)], Train Loss: 0.50379\n",
      "Epoch: 00 [12604/27778 ( 45%)], Train Loss: 0.50310\n",
      "Epoch: 00 [12644/27778 ( 46%)], Train Loss: 0.50277\n",
      "Epoch: 00 [12684/27778 ( 46%)], Train Loss: 0.50199\n",
      "Epoch: 00 [12724/27778 ( 46%)], Train Loss: 0.50189\n",
      "Epoch: 00 [12764/27778 ( 46%)], Train Loss: 0.50137\n",
      "Epoch: 00 [12804/27778 ( 46%)], Train Loss: 0.50071\n",
      "Epoch: 00 [12844/27778 ( 46%)], Train Loss: 0.49983\n",
      "Epoch: 00 [12884/27778 ( 46%)], Train Loss: 0.49903\n",
      "Epoch: 00 [12924/27778 ( 47%)], Train Loss: 0.49831\n",
      "Epoch: 00 [12964/27778 ( 47%)], Train Loss: 0.49791\n",
      "Epoch: 00 [13004/27778 ( 47%)], Train Loss: 0.49749\n",
      "Epoch: 00 [13044/27778 ( 47%)], Train Loss: 0.49730\n",
      "Epoch: 00 [13084/27778 ( 47%)], Train Loss: 0.49654\n",
      "Epoch: 00 [13124/27778 ( 47%)], Train Loss: 0.49582\n",
      "Epoch: 00 [13164/27778 ( 47%)], Train Loss: 0.49597\n",
      "Epoch: 00 [13204/27778 ( 48%)], Train Loss: 0.49555\n",
      "Epoch: 00 [13244/27778 ( 48%)], Train Loss: 0.49504\n",
      "Epoch: 00 [13284/27778 ( 48%)], Train Loss: 0.49421\n",
      "Epoch: 00 [13324/27778 ( 48%)], Train Loss: 0.49346\n",
      "Epoch: 00 [13364/27778 ( 48%)], Train Loss: 0.49261\n",
      "Epoch: 00 [13404/27778 ( 48%)], Train Loss: 0.49194\n",
      "Epoch: 00 [13444/27778 ( 48%)], Train Loss: 0.49137\n",
      "Epoch: 00 [13484/27778 ( 49%)], Train Loss: 0.49110\n",
      "Epoch: 00 [13524/27778 ( 49%)], Train Loss: 0.49110\n",
      "Epoch: 00 [13564/27778 ( 49%)], Train Loss: 0.49070\n",
      "Epoch: 00 [13604/27778 ( 49%)], Train Loss: 0.49018\n",
      "Epoch: 00 [13644/27778 ( 49%)], Train Loss: 0.48930\n",
      "Epoch: 00 [13684/27778 ( 49%)], Train Loss: 0.48957\n",
      "Epoch: 00 [13724/27778 ( 49%)], Train Loss: 0.48942\n",
      "Epoch: 00 [13764/27778 ( 50%)], Train Loss: 0.48881\n",
      "Epoch: 00 [13804/27778 ( 50%)], Train Loss: 0.48847\n",
      "Epoch: 00 [13844/27778 ( 50%)], Train Loss: 0.48815\n",
      "Epoch: 00 [13884/27778 ( 50%)], Train Loss: 0.48737\n",
      "Epoch: 00 [13924/27778 ( 50%)], Train Loss: 0.48724\n",
      "Epoch: 00 [13964/27778 ( 50%)], Train Loss: 0.48682\n",
      "Epoch: 00 [14004/27778 ( 50%)], Train Loss: 0.48643\n",
      "Epoch: 00 [14044/27778 ( 51%)], Train Loss: 0.48612\n",
      "Epoch: 00 [14084/27778 ( 51%)], Train Loss: 0.48573\n",
      "Epoch: 00 [14124/27778 ( 51%)], Train Loss: 0.48570\n",
      "Epoch: 00 [14164/27778 ( 51%)], Train Loss: 0.48563\n",
      "Epoch: 00 [14204/27778 ( 51%)], Train Loss: 0.48482\n",
      "Epoch: 00 [14244/27778 ( 51%)], Train Loss: 0.48456\n",
      "Epoch: 00 [14284/27778 ( 51%)], Train Loss: 0.48404\n",
      "Epoch: 00 [14324/27778 ( 52%)], Train Loss: 0.48385\n",
      "Epoch: 00 [14364/27778 ( 52%)], Train Loss: 0.48321\n",
      "Epoch: 00 [14404/27778 ( 52%)], Train Loss: 0.48299\n",
      "Epoch: 00 [14444/27778 ( 52%)], Train Loss: 0.48218\n",
      "Epoch: 00 [14484/27778 ( 52%)], Train Loss: 0.48137\n",
      "Epoch: 00 [14524/27778 ( 52%)], Train Loss: 0.48080\n",
      "Epoch: 00 [14564/27778 ( 52%)], Train Loss: 0.48065\n",
      "Epoch: 00 [14604/27778 ( 53%)], Train Loss: 0.48006\n",
      "Epoch: 00 [14644/27778 ( 53%)], Train Loss: 0.47960\n",
      "Epoch: 00 [14684/27778 ( 53%)], Train Loss: 0.47881\n",
      "Epoch: 00 [14724/27778 ( 53%)], Train Loss: 0.47880\n",
      "Epoch: 00 [14764/27778 ( 53%)], Train Loss: 0.47920\n",
      "Epoch: 00 [14804/27778 ( 53%)], Train Loss: 0.47891\n",
      "Epoch: 00 [14844/27778 ( 53%)], Train Loss: 0.47922\n",
      "Epoch: 00 [14884/27778 ( 54%)], Train Loss: 0.47878\n",
      "Epoch: 00 [14924/27778 ( 54%)], Train Loss: 0.47879\n",
      "Epoch: 00 [14964/27778 ( 54%)], Train Loss: 0.47853\n",
      "Epoch: 00 [15004/27778 ( 54%)], Train Loss: 0.47821\n",
      "Epoch: 00 [15044/27778 ( 54%)], Train Loss: 0.47792\n",
      "Epoch: 00 [15084/27778 ( 54%)], Train Loss: 0.47796\n",
      "Epoch: 00 [15124/27778 ( 54%)], Train Loss: 0.47740\n",
      "Epoch: 00 [15164/27778 ( 55%)], Train Loss: 0.47715\n",
      "Epoch: 00 [15204/27778 ( 55%)], Train Loss: 0.47676\n",
      "Epoch: 00 [15244/27778 ( 55%)], Train Loss: 0.47628\n",
      "Epoch: 00 [15284/27778 ( 55%)], Train Loss: 0.47618\n",
      "Epoch: 00 [15324/27778 ( 55%)], Train Loss: 0.47558\n",
      "Epoch: 00 [15364/27778 ( 55%)], Train Loss: 0.47496\n",
      "Epoch: 00 [15404/27778 ( 55%)], Train Loss: 0.47500\n",
      "Epoch: 00 [15444/27778 ( 56%)], Train Loss: 0.47439\n",
      "Epoch: 00 [15484/27778 ( 56%)], Train Loss: 0.47389\n",
      "Epoch: 00 [15524/27778 ( 56%)], Train Loss: 0.47397\n",
      "Epoch: 00 [15564/27778 ( 56%)], Train Loss: 0.47361\n",
      "Epoch: 00 [15604/27778 ( 56%)], Train Loss: 0.47299\n",
      "Epoch: 00 [15644/27778 ( 56%)], Train Loss: 0.47219\n",
      "Epoch: 00 [15684/27778 ( 56%)], Train Loss: 0.47209\n",
      "Epoch: 00 [15724/27778 ( 57%)], Train Loss: 0.47162\n",
      "Epoch: 00 [15764/27778 ( 57%)], Train Loss: 0.47116\n",
      "Epoch: 00 [15804/27778 ( 57%)], Train Loss: 0.47096\n",
      "Epoch: 00 [15844/27778 ( 57%)], Train Loss: 0.47037\n",
      "Epoch: 00 [15884/27778 ( 57%)], Train Loss: 0.47003\n",
      "Epoch: 00 [15924/27778 ( 57%)], Train Loss: 0.46972\n",
      "Epoch: 00 [15964/27778 ( 57%)], Train Loss: 0.46941\n",
      "Epoch: 00 [16004/27778 ( 58%)], Train Loss: 0.46886\n",
      "Epoch: 00 [16044/27778 ( 58%)], Train Loss: 0.46877\n",
      "Epoch: 00 [16084/27778 ( 58%)], Train Loss: 0.46820\n",
      "Epoch: 00 [16124/27778 ( 58%)], Train Loss: 0.46782\n",
      "Epoch: 00 [16164/27778 ( 58%)], Train Loss: 0.46728\n",
      "Epoch: 00 [16204/27778 ( 58%)], Train Loss: 0.46730\n",
      "Epoch: 00 [16244/27778 ( 58%)], Train Loss: 0.46693\n",
      "Epoch: 00 [16284/27778 ( 59%)], Train Loss: 0.46650\n",
      "Epoch: 00 [16324/27778 ( 59%)], Train Loss: 0.46636\n",
      "Epoch: 00 [16364/27778 ( 59%)], Train Loss: 0.46599\n",
      "Epoch: 00 [16404/27778 ( 59%)], Train Loss: 0.46533\n",
      "Epoch: 00 [16444/27778 ( 59%)], Train Loss: 0.46542\n",
      "Epoch: 00 [16484/27778 ( 59%)], Train Loss: 0.46508\n",
      "Epoch: 00 [16524/27778 ( 59%)], Train Loss: 0.46460\n",
      "Epoch: 00 [16564/27778 ( 60%)], Train Loss: 0.46455\n",
      "Epoch: 00 [16604/27778 ( 60%)], Train Loss: 0.46409\n",
      "Epoch: 00 [16644/27778 ( 60%)], Train Loss: 0.46348\n",
      "Epoch: 00 [16684/27778 ( 60%)], Train Loss: 0.46342\n",
      "Epoch: 00 [16724/27778 ( 60%)], Train Loss: 0.46349\n",
      "Epoch: 00 [16764/27778 ( 60%)], Train Loss: 0.46295\n",
      "Epoch: 00 [16804/27778 ( 60%)], Train Loss: 0.46275\n",
      "Epoch: 00 [16844/27778 ( 61%)], Train Loss: 0.46311\n",
      "Epoch: 00 [16884/27778 ( 61%)], Train Loss: 0.46281\n",
      "Epoch: 00 [16924/27778 ( 61%)], Train Loss: 0.46252\n",
      "Epoch: 00 [16964/27778 ( 61%)], Train Loss: 0.46238\n",
      "Epoch: 00 [17004/27778 ( 61%)], Train Loss: 0.46198\n",
      "Epoch: 00 [17044/27778 ( 61%)], Train Loss: 0.46162\n",
      "Epoch: 00 [17084/27778 ( 62%)], Train Loss: 0.46116\n",
      "Epoch: 00 [17124/27778 ( 62%)], Train Loss: 0.46071\n",
      "Epoch: 00 [17164/27778 ( 62%)], Train Loss: 0.46000\n",
      "Epoch: 00 [17204/27778 ( 62%)], Train Loss: 0.45951\n",
      "Epoch: 00 [17244/27778 ( 62%)], Train Loss: 0.45909\n",
      "Epoch: 00 [17284/27778 ( 62%)], Train Loss: 0.45842\n",
      "Epoch: 00 [17324/27778 ( 62%)], Train Loss: 0.45804\n",
      "Epoch: 00 [17364/27778 ( 63%)], Train Loss: 0.45777\n",
      "Epoch: 00 [17404/27778 ( 63%)], Train Loss: 0.45743\n",
      "Epoch: 00 [17444/27778 ( 63%)], Train Loss: 0.45707\n",
      "Epoch: 00 [17484/27778 ( 63%)], Train Loss: 0.45684\n",
      "Epoch: 00 [17524/27778 ( 63%)], Train Loss: 0.45664\n",
      "Epoch: 00 [17564/27778 ( 63%)], Train Loss: 0.45621\n",
      "Epoch: 00 [17604/27778 ( 63%)], Train Loss: 0.45595\n",
      "Epoch: 00 [17644/27778 ( 64%)], Train Loss: 0.45567\n",
      "Epoch: 00 [17684/27778 ( 64%)], Train Loss: 0.45544\n",
      "Epoch: 00 [17724/27778 ( 64%)], Train Loss: 0.45546\n",
      "Epoch: 00 [17764/27778 ( 64%)], Train Loss: 0.45499\n",
      "Epoch: 00 [17804/27778 ( 64%)], Train Loss: 0.45474\n",
      "Epoch: 00 [17844/27778 ( 64%)], Train Loss: 0.45447\n",
      "Epoch: 00 [17884/27778 ( 64%)], Train Loss: 0.45399\n",
      "Epoch: 00 [17924/27778 ( 65%)], Train Loss: 0.45363\n",
      "Epoch: 00 [17964/27778 ( 65%)], Train Loss: 0.45334\n",
      "Epoch: 00 [18004/27778 ( 65%)], Train Loss: 0.45327\n",
      "Epoch: 00 [18044/27778 ( 65%)], Train Loss: 0.45272\n",
      "Epoch: 00 [18084/27778 ( 65%)], Train Loss: 0.45253\n",
      "Epoch: 00 [18124/27778 ( 65%)], Train Loss: 0.45234\n",
      "Epoch: 00 [18164/27778 ( 65%)], Train Loss: 0.45203\n",
      "Epoch: 00 [18204/27778 ( 66%)], Train Loss: 0.45159\n",
      "Epoch: 00 [18244/27778 ( 66%)], Train Loss: 0.45114\n",
      "Epoch: 00 [18284/27778 ( 66%)], Train Loss: 0.45070\n",
      "Epoch: 00 [18324/27778 ( 66%)], Train Loss: 0.45048\n",
      "Epoch: 00 [18364/27778 ( 66%)], Train Loss: 0.44996\n",
      "Epoch: 00 [18404/27778 ( 66%)], Train Loss: 0.44987\n",
      "Epoch: 00 [18444/27778 ( 66%)], Train Loss: 0.44966\n",
      "Epoch: 00 [18484/27778 ( 67%)], Train Loss: 0.44940\n",
      "Epoch: 00 [18524/27778 ( 67%)], Train Loss: 0.44945\n",
      "Epoch: 00 [18564/27778 ( 67%)], Train Loss: 0.44909\n",
      "Epoch: 00 [18604/27778 ( 67%)], Train Loss: 0.44868\n",
      "Epoch: 00 [18644/27778 ( 67%)], Train Loss: 0.44822\n",
      "Epoch: 00 [18684/27778 ( 67%)], Train Loss: 0.44820\n",
      "Epoch: 00 [18724/27778 ( 67%)], Train Loss: 0.44809\n",
      "Epoch: 00 [18764/27778 ( 68%)], Train Loss: 0.44778\n",
      "Epoch: 00 [18804/27778 ( 68%)], Train Loss: 0.44728\n",
      "Epoch: 00 [18844/27778 ( 68%)], Train Loss: 0.44706\n",
      "Epoch: 00 [18884/27778 ( 68%)], Train Loss: 0.44661\n",
      "Epoch: 00 [18924/27778 ( 68%)], Train Loss: 0.44634\n",
      "Epoch: 00 [18964/27778 ( 68%)], Train Loss: 0.44602\n",
      "Epoch: 00 [19004/27778 ( 68%)], Train Loss: 0.44588\n",
      "Epoch: 00 [19044/27778 ( 69%)], Train Loss: 0.44552\n",
      "Epoch: 00 [19084/27778 ( 69%)], Train Loss: 0.44512\n",
      "Epoch: 00 [19124/27778 ( 69%)], Train Loss: 0.44480\n",
      "Epoch: 00 [19164/27778 ( 69%)], Train Loss: 0.44445\n",
      "Epoch: 00 [19204/27778 ( 69%)], Train Loss: 0.44437\n",
      "Epoch: 00 [19244/27778 ( 69%)], Train Loss: 0.44389\n",
      "Epoch: 00 [19284/27778 ( 69%)], Train Loss: 0.44344\n",
      "Epoch: 00 [19324/27778 ( 70%)], Train Loss: 0.44292\n",
      "Epoch: 00 [19364/27778 ( 70%)], Train Loss: 0.44265\n",
      "Epoch: 00 [19404/27778 ( 70%)], Train Loss: 0.44264\n",
      "Epoch: 00 [19444/27778 ( 70%)], Train Loss: 0.44231\n",
      "Epoch: 00 [19484/27778 ( 70%)], Train Loss: 0.44220\n",
      "Epoch: 00 [19524/27778 ( 70%)], Train Loss: 0.44215\n",
      "Epoch: 00 [19564/27778 ( 70%)], Train Loss: 0.44185\n",
      "Epoch: 00 [19604/27778 ( 71%)], Train Loss: 0.44160\n",
      "Epoch: 00 [19644/27778 ( 71%)], Train Loss: 0.44141\n",
      "Epoch: 00 [19684/27778 ( 71%)], Train Loss: 0.44111\n",
      "Epoch: 00 [19724/27778 ( 71%)], Train Loss: 0.44077\n",
      "Epoch: 00 [19764/27778 ( 71%)], Train Loss: 0.44060\n",
      "Epoch: 00 [19804/27778 ( 71%)], Train Loss: 0.44034\n",
      "Epoch: 00 [19844/27778 ( 71%)], Train Loss: 0.44022\n",
      "Epoch: 00 [19884/27778 ( 72%)], Train Loss: 0.43999\n",
      "Epoch: 00 [19924/27778 ( 72%)], Train Loss: 0.43963\n",
      "Epoch: 00 [19964/27778 ( 72%)], Train Loss: 0.43918\n",
      "Epoch: 00 [20004/27778 ( 72%)], Train Loss: 0.43917\n",
      "Epoch: 00 [20044/27778 ( 72%)], Train Loss: 0.43896\n",
      "Epoch: 00 [20084/27778 ( 72%)], Train Loss: 0.43898\n",
      "Epoch: 00 [20124/27778 ( 72%)], Train Loss: 0.43854\n",
      "Epoch: 00 [20164/27778 ( 73%)], Train Loss: 0.43822\n",
      "Epoch: 00 [20204/27778 ( 73%)], Train Loss: 0.43755\n",
      "Epoch: 00 [20244/27778 ( 73%)], Train Loss: 0.43699\n",
      "Epoch: 00 [20284/27778 ( 73%)], Train Loss: 0.43687\n",
      "Epoch: 00 [20324/27778 ( 73%)], Train Loss: 0.43677\n",
      "Epoch: 00 [20364/27778 ( 73%)], Train Loss: 0.43685\n",
      "Epoch: 00 [20404/27778 ( 73%)], Train Loss: 0.43656\n",
      "Epoch: 00 [20444/27778 ( 74%)], Train Loss: 0.43624\n",
      "Epoch: 00 [20484/27778 ( 74%)], Train Loss: 0.43618\n",
      "Epoch: 00 [20524/27778 ( 74%)], Train Loss: 0.43590\n",
      "Epoch: 00 [20564/27778 ( 74%)], Train Loss: 0.43570\n",
      "Epoch: 00 [20604/27778 ( 74%)], Train Loss: 0.43532\n",
      "Epoch: 00 [20644/27778 ( 74%)], Train Loss: 0.43526\n",
      "Epoch: 00 [20684/27778 ( 74%)], Train Loss: 0.43523\n",
      "Epoch: 00 [20724/27778 ( 75%)], Train Loss: 0.43475\n",
      "Epoch: 00 [20764/27778 ( 75%)], Train Loss: 0.43469\n",
      "Epoch: 00 [20804/27778 ( 75%)], Train Loss: 0.43436\n",
      "Epoch: 00 [20844/27778 ( 75%)], Train Loss: 0.43395\n",
      "Epoch: 00 [20884/27778 ( 75%)], Train Loss: 0.43353\n",
      "Epoch: 00 [20924/27778 ( 75%)], Train Loss: 0.43317\n",
      "Epoch: 00 [20964/27778 ( 75%)], Train Loss: 0.43306\n",
      "Epoch: 00 [21004/27778 ( 76%)], Train Loss: 0.43263\n",
      "Epoch: 00 [21044/27778 ( 76%)], Train Loss: 0.43249\n",
      "Epoch: 00 [21084/27778 ( 76%)], Train Loss: 0.43221\n",
      "Epoch: 00 [21124/27778 ( 76%)], Train Loss: 0.43178\n",
      "Epoch: 00 [21164/27778 ( 76%)], Train Loss: 0.43144\n",
      "Epoch: 00 [21204/27778 ( 76%)], Train Loss: 0.43122\n",
      "Epoch: 00 [21244/27778 ( 76%)], Train Loss: 0.43087\n",
      "Epoch: 00 [21284/27778 ( 77%)], Train Loss: 0.43047\n",
      "Epoch: 00 [21324/27778 ( 77%)], Train Loss: 0.43043\n",
      "Epoch: 00 [21364/27778 ( 77%)], Train Loss: 0.43032\n",
      "Epoch: 00 [21404/27778 ( 77%)], Train Loss: 0.43009\n",
      "Epoch: 00 [21444/27778 ( 77%)], Train Loss: 0.42974\n",
      "Epoch: 00 [21484/27778 ( 77%)], Train Loss: 0.42922\n",
      "Epoch: 00 [21524/27778 ( 77%)], Train Loss: 0.42896\n",
      "Epoch: 00 [21564/27778 ( 78%)], Train Loss: 0.42875\n",
      "Epoch: 00 [21604/27778 ( 78%)], Train Loss: 0.42828\n",
      "Epoch: 00 [21644/27778 ( 78%)], Train Loss: 0.42802\n",
      "Epoch: 00 [21684/27778 ( 78%)], Train Loss: 0.42805\n",
      "Epoch: 00 [21724/27778 ( 78%)], Train Loss: 0.42792\n",
      "Epoch: 00 [21764/27778 ( 78%)], Train Loss: 0.42756\n",
      "Epoch: 00 [21804/27778 ( 78%)], Train Loss: 0.42730\n",
      "Epoch: 00 [21844/27778 ( 79%)], Train Loss: 0.42715\n",
      "Epoch: 00 [21884/27778 ( 79%)], Train Loss: 0.42700\n",
      "Epoch: 00 [21924/27778 ( 79%)], Train Loss: 0.42717\n",
      "Epoch: 00 [21964/27778 ( 79%)], Train Loss: 0.42698\n",
      "Epoch: 00 [22004/27778 ( 79%)], Train Loss: 0.42694\n",
      "Epoch: 00 [22044/27778 ( 79%)], Train Loss: 0.42664\n",
      "Epoch: 00 [22084/27778 ( 80%)], Train Loss: 0.42648\n",
      "Epoch: 00 [22124/27778 ( 80%)], Train Loss: 0.42620\n",
      "Epoch: 00 [22164/27778 ( 80%)], Train Loss: 0.42618\n",
      "Epoch: 00 [22204/27778 ( 80%)], Train Loss: 0.42601\n",
      "Epoch: 00 [22244/27778 ( 80%)], Train Loss: 0.42601\n",
      "Epoch: 00 [22284/27778 ( 80%)], Train Loss: 0.42572\n",
      "Epoch: 00 [22324/27778 ( 80%)], Train Loss: 0.42571\n",
      "Epoch: 00 [22364/27778 ( 81%)], Train Loss: 0.42577\n",
      "Epoch: 00 [22404/27778 ( 81%)], Train Loss: 0.42563\n",
      "Epoch: 00 [22444/27778 ( 81%)], Train Loss: 0.42529\n",
      "Epoch: 00 [22484/27778 ( 81%)], Train Loss: 0.42493\n",
      "Epoch: 00 [22524/27778 ( 81%)], Train Loss: 0.42450\n",
      "Epoch: 00 [22564/27778 ( 81%)], Train Loss: 0.42428\n",
      "Epoch: 00 [22604/27778 ( 81%)], Train Loss: 0.42394\n",
      "Epoch: 00 [22644/27778 ( 82%)], Train Loss: 0.42404\n",
      "Epoch: 00 [22684/27778 ( 82%)], Train Loss: 0.42363\n",
      "Epoch: 00 [22724/27778 ( 82%)], Train Loss: 0.42399\n",
      "Epoch: 00 [22764/27778 ( 82%)], Train Loss: 0.42393\n",
      "Epoch: 00 [22804/27778 ( 82%)], Train Loss: 0.42394\n",
      "Epoch: 00 [22844/27778 ( 82%)], Train Loss: 0.42370\n",
      "Epoch: 00 [22884/27778 ( 82%)], Train Loss: 0.42336\n",
      "Epoch: 00 [22924/27778 ( 83%)], Train Loss: 0.42319\n",
      "Epoch: 00 [22964/27778 ( 83%)], Train Loss: 0.42298\n",
      "Epoch: 00 [23004/27778 ( 83%)], Train Loss: 0.42268\n",
      "Epoch: 00 [23044/27778 ( 83%)], Train Loss: 0.42234\n",
      "Epoch: 00 [23084/27778 ( 83%)], Train Loss: 0.42203\n",
      "Epoch: 00 [23124/27778 ( 83%)], Train Loss: 0.42166\n",
      "Epoch: 00 [23164/27778 ( 83%)], Train Loss: 0.42139\n",
      "Epoch: 00 [23204/27778 ( 84%)], Train Loss: 0.42146\n",
      "Epoch: 00 [23244/27778 ( 84%)], Train Loss: 0.42137\n",
      "Epoch: 00 [23284/27778 ( 84%)], Train Loss: 0.42123\n",
      "Epoch: 00 [23324/27778 ( 84%)], Train Loss: 0.42091\n",
      "Epoch: 00 [23364/27778 ( 84%)], Train Loss: 0.42090\n",
      "Epoch: 00 [23404/27778 ( 84%)], Train Loss: 0.42097\n",
      "Epoch: 00 [23444/27778 ( 84%)], Train Loss: 0.42064\n",
      "Epoch: 00 [23484/27778 ( 85%)], Train Loss: 0.42065\n",
      "Epoch: 00 [23524/27778 ( 85%)], Train Loss: 0.42035\n",
      "Epoch: 00 [23564/27778 ( 85%)], Train Loss: 0.42020\n",
      "Epoch: 00 [23604/27778 ( 85%)], Train Loss: 0.41994\n",
      "Epoch: 00 [23644/27778 ( 85%)], Train Loss: 0.42009\n",
      "Epoch: 00 [23684/27778 ( 85%)], Train Loss: 0.41977\n",
      "Epoch: 00 [23724/27778 ( 85%)], Train Loss: 0.41952\n",
      "Epoch: 00 [23764/27778 ( 86%)], Train Loss: 0.41941\n",
      "Epoch: 00 [23804/27778 ( 86%)], Train Loss: 0.41927\n",
      "Epoch: 00 [23844/27778 ( 86%)], Train Loss: 0.41906\n",
      "Epoch: 00 [23884/27778 ( 86%)], Train Loss: 0.41878\n",
      "Epoch: 00 [23924/27778 ( 86%)], Train Loss: 0.41857\n",
      "Epoch: 00 [23964/27778 ( 86%)], Train Loss: 0.41834\n",
      "Epoch: 00 [24004/27778 ( 86%)], Train Loss: 0.41801\n",
      "Epoch: 00 [24044/27778 ( 87%)], Train Loss: 0.41773\n",
      "Epoch: 00 [24084/27778 ( 87%)], Train Loss: 0.41731\n",
      "Epoch: 00 [24124/27778 ( 87%)], Train Loss: 0.41702\n",
      "Epoch: 00 [24164/27778 ( 87%)], Train Loss: 0.41685\n",
      "Epoch: 00 [24204/27778 ( 87%)], Train Loss: 0.41653\n",
      "Epoch: 00 [24244/27778 ( 87%)], Train Loss: 0.41620\n",
      "Epoch: 00 [24284/27778 ( 87%)], Train Loss: 0.41620\n",
      "Epoch: 00 [24324/27778 ( 88%)], Train Loss: 0.41595\n",
      "Epoch: 00 [24364/27778 ( 88%)], Train Loss: 0.41581\n",
      "Epoch: 00 [24404/27778 ( 88%)], Train Loss: 0.41537\n",
      "Epoch: 00 [24444/27778 ( 88%)], Train Loss: 0.41509\n",
      "Epoch: 00 [24484/27778 ( 88%)], Train Loss: 0.41476\n",
      "Epoch: 00 [24524/27778 ( 88%)], Train Loss: 0.41460\n",
      "Epoch: 00 [24564/27778 ( 88%)], Train Loss: 0.41429\n",
      "Epoch: 00 [24604/27778 ( 89%)], Train Loss: 0.41416\n",
      "Epoch: 00 [24644/27778 ( 89%)], Train Loss: 0.41390\n",
      "Epoch: 00 [24684/27778 ( 89%)], Train Loss: 0.41356\n",
      "Epoch: 00 [24724/27778 ( 89%)], Train Loss: 0.41371\n",
      "Epoch: 00 [24764/27778 ( 89%)], Train Loss: 0.41370\n",
      "Epoch: 00 [24804/27778 ( 89%)], Train Loss: 0.41355\n",
      "Epoch: 00 [24844/27778 ( 89%)], Train Loss: 0.41337\n",
      "Epoch: 00 [24884/27778 ( 90%)], Train Loss: 0.41309\n",
      "Epoch: 00 [24924/27778 ( 90%)], Train Loss: 0.41271\n",
      "Epoch: 00 [24964/27778 ( 90%)], Train Loss: 0.41242\n",
      "Epoch: 00 [25004/27778 ( 90%)], Train Loss: 0.41235\n",
      "Epoch: 00 [25044/27778 ( 90%)], Train Loss: 0.41209\n",
      "Epoch: 00 [25084/27778 ( 90%)], Train Loss: 0.41195\n",
      "Epoch: 00 [25124/27778 ( 90%)], Train Loss: 0.41198\n",
      "Epoch: 00 [25164/27778 ( 91%)], Train Loss: 0.41168\n",
      "Epoch: 00 [25204/27778 ( 91%)], Train Loss: 0.41129\n",
      "Epoch: 00 [25244/27778 ( 91%)], Train Loss: 0.41110\n",
      "Epoch: 00 [25284/27778 ( 91%)], Train Loss: 0.41103\n",
      "Epoch: 00 [25324/27778 ( 91%)], Train Loss: 0.41092\n",
      "Epoch: 00 [25364/27778 ( 91%)], Train Loss: 0.41072\n",
      "Epoch: 00 [25404/27778 ( 91%)], Train Loss: 0.41059\n",
      "Epoch: 00 [25444/27778 ( 92%)], Train Loss: 0.41044\n",
      "Epoch: 00 [25484/27778 ( 92%)], Train Loss: 0.41043\n",
      "Epoch: 00 [25524/27778 ( 92%)], Train Loss: 0.41025\n",
      "Epoch: 00 [25564/27778 ( 92%)], Train Loss: 0.41002\n",
      "Epoch: 00 [25604/27778 ( 92%)], Train Loss: 0.40968\n",
      "Epoch: 00 [25644/27778 ( 92%)], Train Loss: 0.40940\n",
      "Epoch: 00 [25684/27778 ( 92%)], Train Loss: 0.40921\n",
      "Epoch: 00 [25724/27778 ( 93%)], Train Loss: 0.40894\n",
      "Epoch: 00 [25764/27778 ( 93%)], Train Loss: 0.40883\n",
      "Epoch: 00 [25804/27778 ( 93%)], Train Loss: 0.40849\n",
      "Epoch: 00 [25844/27778 ( 93%)], Train Loss: 0.40830\n",
      "Epoch: 00 [25884/27778 ( 93%)], Train Loss: 0.40796\n",
      "Epoch: 00 [25924/27778 ( 93%)], Train Loss: 0.40765\n",
      "Epoch: 00 [25964/27778 ( 93%)], Train Loss: 0.40749\n",
      "Epoch: 00 [26004/27778 ( 94%)], Train Loss: 0.40743\n",
      "Epoch: 00 [26044/27778 ( 94%)], Train Loss: 0.40708\n",
      "Epoch: 00 [26084/27778 ( 94%)], Train Loss: 0.40694\n",
      "Epoch: 00 [26124/27778 ( 94%)], Train Loss: 0.40687\n",
      "Epoch: 00 [26164/27778 ( 94%)], Train Loss: 0.40672\n",
      "Epoch: 00 [26204/27778 ( 94%)], Train Loss: 0.40649\n",
      "Epoch: 00 [26244/27778 ( 94%)], Train Loss: 0.40617\n",
      "Epoch: 00 [26284/27778 ( 95%)], Train Loss: 0.40594\n",
      "Epoch: 00 [26324/27778 ( 95%)], Train Loss: 0.40569\n",
      "Epoch: 00 [26364/27778 ( 95%)], Train Loss: 0.40533\n",
      "Epoch: 00 [26404/27778 ( 95%)], Train Loss: 0.40499\n",
      "Epoch: 00 [26444/27778 ( 95%)], Train Loss: 0.40496\n",
      "Epoch: 00 [26484/27778 ( 95%)], Train Loss: 0.40472\n",
      "Epoch: 00 [26524/27778 ( 95%)], Train Loss: 0.40445\n",
      "Epoch: 00 [26564/27778 ( 96%)], Train Loss: 0.40414\n",
      "Epoch: 00 [26604/27778 ( 96%)], Train Loss: 0.40381\n",
      "Epoch: 00 [26644/27778 ( 96%)], Train Loss: 0.40369\n",
      "Epoch: 00 [26684/27778 ( 96%)], Train Loss: 0.40366\n",
      "Epoch: 00 [26724/27778 ( 96%)], Train Loss: 0.40339\n",
      "Epoch: 00 [26764/27778 ( 96%)], Train Loss: 0.40313\n",
      "Epoch: 00 [26804/27778 ( 96%)], Train Loss: 0.40288\n",
      "Epoch: 00 [26844/27778 ( 97%)], Train Loss: 0.40259\n",
      "Epoch: 00 [26884/27778 ( 97%)], Train Loss: 0.40254\n",
      "Epoch: 00 [26924/27778 ( 97%)], Train Loss: 0.40217\n",
      "Epoch: 00 [26964/27778 ( 97%)], Train Loss: 0.40195\n",
      "Epoch: 00 [27004/27778 ( 97%)], Train Loss: 0.40171\n",
      "Epoch: 00 [27044/27778 ( 97%)], Train Loss: 0.40152\n",
      "Epoch: 00 [27084/27778 ( 98%)], Train Loss: 0.40148\n",
      "Epoch: 00 [27124/27778 ( 98%)], Train Loss: 0.40139\n",
      "Epoch: 00 [27164/27778 ( 98%)], Train Loss: 0.40132\n",
      "Epoch: 00 [27204/27778 ( 98%)], Train Loss: 0.40114\n",
      "Epoch: 00 [27244/27778 ( 98%)], Train Loss: 0.40096\n",
      "Epoch: 00 [27284/27778 ( 98%)], Train Loss: 0.40074\n",
      "Epoch: 00 [27324/27778 ( 98%)], Train Loss: 0.40055\n",
      "Epoch: 00 [27364/27778 ( 99%)], Train Loss: 0.40023\n",
      "Epoch: 00 [27404/27778 ( 99%)], Train Loss: 0.40023\n",
      "Epoch: 00 [27444/27778 ( 99%)], Train Loss: 0.40020\n",
      "Epoch: 00 [27484/27778 ( 99%)], Train Loss: 0.40033\n",
      "Epoch: 00 [27524/27778 ( 99%)], Train Loss: 0.40030\n",
      "Epoch: 00 [27564/27778 ( 99%)], Train Loss: 0.40014\n",
      "Epoch: 00 [27604/27778 ( 99%)], Train Loss: 0.39987\n",
      "Epoch: 00 [27644/27778 (100%)], Train Loss: 0.39964\n",
      "Epoch: 00 [27684/27778 (100%)], Train Loss: 0.39952\n",
      "Epoch: 00 [27724/27778 (100%)], Train Loss: 0.39927\n",
      "Epoch: 00 [27764/27778 (100%)], Train Loss: 0.39915\n",
      "Epoch: 00 [27778/27778 (100%)], Train Loss: 0.39921\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.56394\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.56394\n",
      "Saving model checkpoint to output/checkpoint-fold-4.\n",
      "\n",
      "Total Training Time: 3234.297482728958secs, Average Training Time per Epoch: 3234.297482728958secs.\n",
      "Total Validation Time: 141.1761655807495secs, Average Validation Time per Epoch: 141.1761655807495secs.\n",
      "--------------------------------------------------\n",
      "FOLD: 5\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27778, Num examples Valid=3944\n",
      "Total Training Steps: 3473, Total Warmup Steps: 347\n",
      "Epoch: 00 [    4/27778 (  0%)], Train Loss: 2.71668\n",
      "Epoch: 00 [   44/27778 (  0%)], Train Loss: 2.83723\n",
      "Epoch: 00 [   84/27778 (  0%)], Train Loss: 2.78486\n",
      "Epoch: 00 [  124/27778 (  0%)], Train Loss: 2.75730\n",
      "Epoch: 00 [  164/27778 (  1%)], Train Loss: 2.71460\n",
      "Epoch: 00 [  204/27778 (  1%)], Train Loss: 2.67738\n",
      "Epoch: 00 [  244/27778 (  1%)], Train Loss: 2.64058\n",
      "Epoch: 00 [  284/27778 (  1%)], Train Loss: 2.59144\n",
      "Epoch: 00 [  324/27778 (  1%)], Train Loss: 2.52259\n",
      "Epoch: 00 [  364/27778 (  1%)], Train Loss: 2.46340\n",
      "Epoch: 00 [  404/27778 (  1%)], Train Loss: 2.40012\n",
      "Epoch: 00 [  444/27778 (  2%)], Train Loss: 2.33539\n",
      "Epoch: 00 [  484/27778 (  2%)], Train Loss: 2.24742\n",
      "Epoch: 00 [  524/27778 (  2%)], Train Loss: 2.16431\n",
      "Epoch: 00 [  564/27778 (  2%)], Train Loss: 2.08688\n",
      "Epoch: 00 [  604/27778 (  2%)], Train Loss: 2.00338\n",
      "Epoch: 00 [  644/27778 (  2%)], Train Loss: 1.93819\n",
      "Epoch: 00 [  684/27778 (  2%)], Train Loss: 1.86947\n",
      "Epoch: 00 [  724/27778 (  3%)], Train Loss: 1.80688\n",
      "Epoch: 00 [  764/27778 (  3%)], Train Loss: 1.75122\n",
      "Epoch: 00 [  804/27778 (  3%)], Train Loss: 1.69129\n",
      "Epoch: 00 [  844/27778 (  3%)], Train Loss: 1.64532\n",
      "Epoch: 00 [  884/27778 (  3%)], Train Loss: 1.59466\n",
      "Epoch: 00 [  924/27778 (  3%)], Train Loss: 1.54370\n",
      "Epoch: 00 [  964/27778 (  3%)], Train Loss: 1.50134\n",
      "Epoch: 00 [ 1004/27778 (  4%)], Train Loss: 1.46879\n",
      "Epoch: 00 [ 1044/27778 (  4%)], Train Loss: 1.43146\n",
      "Epoch: 00 [ 1084/27778 (  4%)], Train Loss: 1.39243\n",
      "Epoch: 00 [ 1124/27778 (  4%)], Train Loss: 1.36395\n",
      "Epoch: 00 [ 1164/27778 (  4%)], Train Loss: 1.33494\n",
      "Epoch: 00 [ 1204/27778 (  4%)], Train Loss: 1.32039\n",
      "Epoch: 00 [ 1244/27778 (  4%)], Train Loss: 1.29531\n",
      "Epoch: 00 [ 1284/27778 (  5%)], Train Loss: 1.27153\n",
      "Epoch: 00 [ 1324/27778 (  5%)], Train Loss: 1.25480\n",
      "Epoch: 00 [ 1364/27778 (  5%)], Train Loss: 1.23219\n",
      "Epoch: 00 [ 1404/27778 (  5%)], Train Loss: 1.20996\n",
      "Epoch: 00 [ 1444/27778 (  5%)], Train Loss: 1.19417\n",
      "Epoch: 00 [ 1484/27778 (  5%)], Train Loss: 1.17338\n",
      "Epoch: 00 [ 1524/27778 (  5%)], Train Loss: 1.15963\n",
      "Epoch: 00 [ 1564/27778 (  6%)], Train Loss: 1.14588\n",
      "Epoch: 00 [ 1604/27778 (  6%)], Train Loss: 1.13243\n",
      "Epoch: 00 [ 1644/27778 (  6%)], Train Loss: 1.11491\n",
      "Epoch: 00 [ 1684/27778 (  6%)], Train Loss: 1.09985\n",
      "Epoch: 00 [ 1724/27778 (  6%)], Train Loss: 1.08294\n",
      "Epoch: 00 [ 1764/27778 (  6%)], Train Loss: 1.06907\n",
      "Epoch: 00 [ 1804/27778 (  6%)], Train Loss: 1.05921\n",
      "Epoch: 00 [ 1844/27778 (  7%)], Train Loss: 1.05249\n",
      "Epoch: 00 [ 1884/27778 (  7%)], Train Loss: 1.03938\n",
      "Epoch: 00 [ 1924/27778 (  7%)], Train Loss: 1.02631\n",
      "Epoch: 00 [ 1964/27778 (  7%)], Train Loss: 1.01655\n",
      "Epoch: 00 [ 2004/27778 (  7%)], Train Loss: 1.00487\n",
      "Epoch: 00 [ 2044/27778 (  7%)], Train Loss: 0.99263\n",
      "Epoch: 00 [ 2084/27778 (  8%)], Train Loss: 0.98517\n",
      "Epoch: 00 [ 2124/27778 (  8%)], Train Loss: 0.97331\n",
      "Epoch: 00 [ 2164/27778 (  8%)], Train Loss: 0.96118\n",
      "Epoch: 00 [ 2204/27778 (  8%)], Train Loss: 0.95436\n",
      "Epoch: 00 [ 2244/27778 (  8%)], Train Loss: 0.94247\n",
      "Epoch: 00 [ 2284/27778 (  8%)], Train Loss: 0.93413\n",
      "Epoch: 00 [ 2324/27778 (  8%)], Train Loss: 0.92594\n",
      "Epoch: 00 [ 2364/27778 (  9%)], Train Loss: 0.91749\n",
      "Epoch: 00 [ 2404/27778 (  9%)], Train Loss: 0.91139\n",
      "Epoch: 00 [ 2444/27778 (  9%)], Train Loss: 0.90377\n",
      "Epoch: 00 [ 2484/27778 (  9%)], Train Loss: 0.90020\n",
      "Epoch: 00 [ 2524/27778 (  9%)], Train Loss: 0.89344\n",
      "Epoch: 00 [ 2564/27778 (  9%)], Train Loss: 0.88525\n",
      "Epoch: 00 [ 2604/27778 (  9%)], Train Loss: 0.87936\n",
      "Epoch: 00 [ 2644/27778 ( 10%)], Train Loss: 0.87124\n",
      "Epoch: 00 [ 2684/27778 ( 10%)], Train Loss: 0.86702\n",
      "Epoch: 00 [ 2724/27778 ( 10%)], Train Loss: 0.85982\n",
      "Epoch: 00 [ 2764/27778 ( 10%)], Train Loss: 0.85247\n",
      "Epoch: 00 [ 2804/27778 ( 10%)], Train Loss: 0.84910\n",
      "Epoch: 00 [ 2844/27778 ( 10%)], Train Loss: 0.84401\n",
      "Epoch: 00 [ 2884/27778 ( 10%)], Train Loss: 0.83635\n",
      "Epoch: 00 [ 2924/27778 ( 11%)], Train Loss: 0.83349\n",
      "Epoch: 00 [ 2964/27778 ( 11%)], Train Loss: 0.82710\n",
      "Epoch: 00 [ 3004/27778 ( 11%)], Train Loss: 0.82411\n",
      "Epoch: 00 [ 3044/27778 ( 11%)], Train Loss: 0.82256\n",
      "Epoch: 00 [ 3084/27778 ( 11%)], Train Loss: 0.81917\n",
      "Epoch: 00 [ 3124/27778 ( 11%)], Train Loss: 0.81373\n",
      "Epoch: 00 [ 3164/27778 ( 11%)], Train Loss: 0.80968\n",
      "Epoch: 00 [ 3204/27778 ( 12%)], Train Loss: 0.80567\n",
      "Epoch: 00 [ 3244/27778 ( 12%)], Train Loss: 0.80203\n",
      "Epoch: 00 [ 3284/27778 ( 12%)], Train Loss: 0.79831\n",
      "Epoch: 00 [ 3324/27778 ( 12%)], Train Loss: 0.79395\n",
      "Epoch: 00 [ 3364/27778 ( 12%)], Train Loss: 0.78909\n",
      "Epoch: 00 [ 3404/27778 ( 12%)], Train Loss: 0.78391\n",
      "Epoch: 00 [ 3444/27778 ( 12%)], Train Loss: 0.78297\n",
      "Epoch: 00 [ 3484/27778 ( 13%)], Train Loss: 0.77985\n",
      "Epoch: 00 [ 3524/27778 ( 13%)], Train Loss: 0.77521\n",
      "Epoch: 00 [ 3564/27778 ( 13%)], Train Loss: 0.77083\n",
      "Epoch: 00 [ 3604/27778 ( 13%)], Train Loss: 0.76705\n",
      "Epoch: 00 [ 3644/27778 ( 13%)], Train Loss: 0.76240\n",
      "Epoch: 00 [ 3684/27778 ( 13%)], Train Loss: 0.75781\n",
      "Epoch: 00 [ 3724/27778 ( 13%)], Train Loss: 0.75451\n",
      "Epoch: 00 [ 3764/27778 ( 14%)], Train Loss: 0.75005\n",
      "Epoch: 00 [ 3804/27778 ( 14%)], Train Loss: 0.74508\n",
      "Epoch: 00 [ 3844/27778 ( 14%)], Train Loss: 0.73931\n",
      "Epoch: 00 [ 3884/27778 ( 14%)], Train Loss: 0.73758\n",
      "Epoch: 00 [ 3924/27778 ( 14%)], Train Loss: 0.73525\n",
      "Epoch: 00 [ 3964/27778 ( 14%)], Train Loss: 0.73308\n",
      "Epoch: 00 [ 4004/27778 ( 14%)], Train Loss: 0.72864\n",
      "Epoch: 00 [ 4044/27778 ( 15%)], Train Loss: 0.72533\n",
      "Epoch: 00 [ 4084/27778 ( 15%)], Train Loss: 0.72156\n",
      "Epoch: 00 [ 4124/27778 ( 15%)], Train Loss: 0.71647\n",
      "Epoch: 00 [ 4164/27778 ( 15%)], Train Loss: 0.71455\n",
      "Epoch: 00 [ 4204/27778 ( 15%)], Train Loss: 0.71201\n",
      "Epoch: 00 [ 4244/27778 ( 15%)], Train Loss: 0.70887\n",
      "Epoch: 00 [ 4284/27778 ( 15%)], Train Loss: 0.70620\n",
      "Epoch: 00 [ 4324/27778 ( 16%)], Train Loss: 0.70295\n",
      "Epoch: 00 [ 4364/27778 ( 16%)], Train Loss: 0.69993\n",
      "Epoch: 00 [ 4404/27778 ( 16%)], Train Loss: 0.69723\n",
      "Epoch: 00 [ 4444/27778 ( 16%)], Train Loss: 0.69515\n",
      "Epoch: 00 [ 4484/27778 ( 16%)], Train Loss: 0.69387\n",
      "Epoch: 00 [ 4524/27778 ( 16%)], Train Loss: 0.69149\n",
      "Epoch: 00 [ 4564/27778 ( 16%)], Train Loss: 0.69137\n",
      "Epoch: 00 [ 4604/27778 ( 17%)], Train Loss: 0.68984\n",
      "Epoch: 00 [ 4644/27778 ( 17%)], Train Loss: 0.68821\n",
      "Epoch: 00 [ 4684/27778 ( 17%)], Train Loss: 0.68633\n",
      "Epoch: 00 [ 4724/27778 ( 17%)], Train Loss: 0.68440\n",
      "Epoch: 00 [ 4764/27778 ( 17%)], Train Loss: 0.68257\n",
      "Epoch: 00 [ 4804/27778 ( 17%)], Train Loss: 0.68067\n",
      "Epoch: 00 [ 4844/27778 ( 17%)], Train Loss: 0.67789\n",
      "Epoch: 00 [ 4884/27778 ( 18%)], Train Loss: 0.67687\n",
      "Epoch: 00 [ 4924/27778 ( 18%)], Train Loss: 0.67425\n",
      "Epoch: 00 [ 4964/27778 ( 18%)], Train Loss: 0.67439\n",
      "Epoch: 00 [ 5004/27778 ( 18%)], Train Loss: 0.67424\n",
      "Epoch: 00 [ 5044/27778 ( 18%)], Train Loss: 0.67144\n",
      "Epoch: 00 [ 5084/27778 ( 18%)], Train Loss: 0.66959\n",
      "Epoch: 00 [ 5124/27778 ( 18%)], Train Loss: 0.67022\n",
      "Epoch: 00 [ 5164/27778 ( 19%)], Train Loss: 0.66837\n",
      "Epoch: 00 [ 5204/27778 ( 19%)], Train Loss: 0.66745\n",
      "Epoch: 00 [ 5244/27778 ( 19%)], Train Loss: 0.66616\n",
      "Epoch: 00 [ 5284/27778 ( 19%)], Train Loss: 0.66371\n",
      "Epoch: 00 [ 5324/27778 ( 19%)], Train Loss: 0.66443\n",
      "Epoch: 00 [ 5364/27778 ( 19%)], Train Loss: 0.66201\n",
      "Epoch: 00 [ 5404/27778 ( 19%)], Train Loss: 0.66099\n",
      "Epoch: 00 [ 5444/27778 ( 20%)], Train Loss: 0.65964\n",
      "Epoch: 00 [ 5484/27778 ( 20%)], Train Loss: 0.65814\n",
      "Epoch: 00 [ 5524/27778 ( 20%)], Train Loss: 0.65726\n",
      "Epoch: 00 [ 5564/27778 ( 20%)], Train Loss: 0.65449\n",
      "Epoch: 00 [ 5604/27778 ( 20%)], Train Loss: 0.65260\n",
      "Epoch: 00 [ 5644/27778 ( 20%)], Train Loss: 0.64979\n",
      "Epoch: 00 [ 5684/27778 ( 20%)], Train Loss: 0.64722\n",
      "Epoch: 00 [ 5724/27778 ( 21%)], Train Loss: 0.64569\n",
      "Epoch: 00 [ 5764/27778 ( 21%)], Train Loss: 0.64395\n",
      "Epoch: 00 [ 5804/27778 ( 21%)], Train Loss: 0.64075\n",
      "Epoch: 00 [ 5844/27778 ( 21%)], Train Loss: 0.63852\n",
      "Epoch: 00 [ 5884/27778 ( 21%)], Train Loss: 0.63566\n",
      "Epoch: 00 [ 5924/27778 ( 21%)], Train Loss: 0.63632\n",
      "Epoch: 00 [ 5964/27778 ( 21%)], Train Loss: 0.63503\n",
      "Epoch: 00 [ 6004/27778 ( 22%)], Train Loss: 0.63434\n",
      "Epoch: 00 [ 6044/27778 ( 22%)], Train Loss: 0.63269\n",
      "Epoch: 00 [ 6084/27778 ( 22%)], Train Loss: 0.63053\n",
      "Epoch: 00 [ 6124/27778 ( 22%)], Train Loss: 0.62836\n",
      "Epoch: 00 [ 6164/27778 ( 22%)], Train Loss: 0.62768\n",
      "Epoch: 00 [ 6204/27778 ( 22%)], Train Loss: 0.62620\n",
      "Epoch: 00 [ 6244/27778 ( 22%)], Train Loss: 0.62475\n",
      "Epoch: 00 [ 6284/27778 ( 23%)], Train Loss: 0.62248\n",
      "Epoch: 00 [ 6324/27778 ( 23%)], Train Loss: 0.62114\n",
      "Epoch: 00 [ 6364/27778 ( 23%)], Train Loss: 0.62003\n",
      "Epoch: 00 [ 6404/27778 ( 23%)], Train Loss: 0.61922\n",
      "Epoch: 00 [ 6444/27778 ( 23%)], Train Loss: 0.61757\n",
      "Epoch: 00 [ 6484/27778 ( 23%)], Train Loss: 0.61552\n",
      "Epoch: 00 [ 6524/27778 ( 23%)], Train Loss: 0.61451\n",
      "Epoch: 00 [ 6564/27778 ( 24%)], Train Loss: 0.61340\n",
      "Epoch: 00 [ 6604/27778 ( 24%)], Train Loss: 0.61164\n",
      "Epoch: 00 [ 6644/27778 ( 24%)], Train Loss: 0.60977\n",
      "Epoch: 00 [ 6684/27778 ( 24%)], Train Loss: 0.60719\n",
      "Epoch: 00 [ 6724/27778 ( 24%)], Train Loss: 0.60713\n",
      "Epoch: 00 [ 6764/27778 ( 24%)], Train Loss: 0.60558\n",
      "Epoch: 00 [ 6804/27778 ( 24%)], Train Loss: 0.60382\n",
      "Epoch: 00 [ 6844/27778 ( 25%)], Train Loss: 0.60310\n",
      "Epoch: 00 [ 6884/27778 ( 25%)], Train Loss: 0.60190\n",
      "Epoch: 00 [ 6924/27778 ( 25%)], Train Loss: 0.60252\n",
      "Epoch: 00 [ 6964/27778 ( 25%)], Train Loss: 0.60156\n",
      "Epoch: 00 [ 7004/27778 ( 25%)], Train Loss: 0.60014\n",
      "Epoch: 00 [ 7044/27778 ( 25%)], Train Loss: 0.59885\n",
      "Epoch: 00 [ 7084/27778 ( 26%)], Train Loss: 0.59749\n",
      "Epoch: 00 [ 7124/27778 ( 26%)], Train Loss: 0.59694\n",
      "Epoch: 00 [ 7164/27778 ( 26%)], Train Loss: 0.59659\n",
      "Epoch: 00 [ 7204/27778 ( 26%)], Train Loss: 0.59530\n",
      "Epoch: 00 [ 7244/27778 ( 26%)], Train Loss: 0.59457\n",
      "Epoch: 00 [ 7284/27778 ( 26%)], Train Loss: 0.59374\n",
      "Epoch: 00 [ 7324/27778 ( 26%)], Train Loss: 0.59284\n",
      "Epoch: 00 [ 7364/27778 ( 27%)], Train Loss: 0.59215\n",
      "Epoch: 00 [ 7404/27778 ( 27%)], Train Loss: 0.59184\n",
      "Epoch: 00 [ 7444/27778 ( 27%)], Train Loss: 0.59008\n",
      "Epoch: 00 [ 7484/27778 ( 27%)], Train Loss: 0.58860\n",
      "Epoch: 00 [ 7524/27778 ( 27%)], Train Loss: 0.58780\n",
      "Epoch: 00 [ 7564/27778 ( 27%)], Train Loss: 0.58704\n",
      "Epoch: 00 [ 7604/27778 ( 27%)], Train Loss: 0.58543\n",
      "Epoch: 00 [ 7644/27778 ( 28%)], Train Loss: 0.58447\n",
      "Epoch: 00 [ 7684/27778 ( 28%)], Train Loss: 0.58328\n",
      "Epoch: 00 [ 7724/27778 ( 28%)], Train Loss: 0.58248\n",
      "Epoch: 00 [ 7764/27778 ( 28%)], Train Loss: 0.58235\n",
      "Epoch: 00 [ 7804/27778 ( 28%)], Train Loss: 0.58204\n",
      "Epoch: 00 [ 7844/27778 ( 28%)], Train Loss: 0.58197\n",
      "Epoch: 00 [ 7884/27778 ( 28%)], Train Loss: 0.58081\n",
      "Epoch: 00 [ 7924/27778 ( 29%)], Train Loss: 0.57901\n",
      "Epoch: 00 [ 7964/27778 ( 29%)], Train Loss: 0.57766\n",
      "Epoch: 00 [ 8004/27778 ( 29%)], Train Loss: 0.57717\n",
      "Epoch: 00 [ 8044/27778 ( 29%)], Train Loss: 0.57547\n",
      "Epoch: 00 [ 8084/27778 ( 29%)], Train Loss: 0.57450\n",
      "Epoch: 00 [ 8124/27778 ( 29%)], Train Loss: 0.57341\n",
      "Epoch: 00 [ 8164/27778 ( 29%)], Train Loss: 0.57255\n",
      "Epoch: 00 [ 8204/27778 ( 30%)], Train Loss: 0.57207\n",
      "Epoch: 00 [ 8244/27778 ( 30%)], Train Loss: 0.57058\n",
      "Epoch: 00 [ 8284/27778 ( 30%)], Train Loss: 0.57022\n",
      "Epoch: 00 [ 8324/27778 ( 30%)], Train Loss: 0.56934\n",
      "Epoch: 00 [ 8364/27778 ( 30%)], Train Loss: 0.56894\n",
      "Epoch: 00 [ 8404/27778 ( 30%)], Train Loss: 0.56850\n",
      "Epoch: 00 [ 8444/27778 ( 30%)], Train Loss: 0.56742\n",
      "Epoch: 00 [ 8484/27778 ( 31%)], Train Loss: 0.56631\n",
      "Epoch: 00 [ 8524/27778 ( 31%)], Train Loss: 0.56525\n",
      "Epoch: 00 [ 8564/27778 ( 31%)], Train Loss: 0.56386\n",
      "Epoch: 00 [ 8604/27778 ( 31%)], Train Loss: 0.56354\n",
      "Epoch: 00 [ 8644/27778 ( 31%)], Train Loss: 0.56263\n",
      "Epoch: 00 [ 8684/27778 ( 31%)], Train Loss: 0.56195\n",
      "Epoch: 00 [ 8724/27778 ( 31%)], Train Loss: 0.56089\n",
      "Epoch: 00 [ 8764/27778 ( 32%)], Train Loss: 0.56018\n",
      "Epoch: 00 [ 8804/27778 ( 32%)], Train Loss: 0.55892\n",
      "Epoch: 00 [ 8844/27778 ( 32%)], Train Loss: 0.55772\n",
      "Epoch: 00 [ 8884/27778 ( 32%)], Train Loss: 0.55738\n",
      "Epoch: 00 [ 8924/27778 ( 32%)], Train Loss: 0.55700\n",
      "Epoch: 00 [ 8964/27778 ( 32%)], Train Loss: 0.55625\n",
      "Epoch: 00 [ 9004/27778 ( 32%)], Train Loss: 0.55490\n",
      "Epoch: 00 [ 9044/27778 ( 33%)], Train Loss: 0.55438\n",
      "Epoch: 00 [ 9084/27778 ( 33%)], Train Loss: 0.55354\n",
      "Epoch: 00 [ 9124/27778 ( 33%)], Train Loss: 0.55214\n",
      "Epoch: 00 [ 9164/27778 ( 33%)], Train Loss: 0.55197\n",
      "Epoch: 00 [ 9204/27778 ( 33%)], Train Loss: 0.55071\n",
      "Epoch: 00 [ 9244/27778 ( 33%)], Train Loss: 0.54937\n",
      "Epoch: 00 [ 9284/27778 ( 33%)], Train Loss: 0.54865\n",
      "Epoch: 00 [ 9324/27778 ( 34%)], Train Loss: 0.54760\n",
      "Epoch: 00 [ 9364/27778 ( 34%)], Train Loss: 0.54709\n",
      "Epoch: 00 [ 9404/27778 ( 34%)], Train Loss: 0.54717\n",
      "Epoch: 00 [ 9444/27778 ( 34%)], Train Loss: 0.54585\n",
      "Epoch: 00 [ 9484/27778 ( 34%)], Train Loss: 0.54496\n",
      "Epoch: 00 [ 9524/27778 ( 34%)], Train Loss: 0.54459\n",
      "Epoch: 00 [ 9564/27778 ( 34%)], Train Loss: 0.54382\n",
      "Epoch: 00 [ 9604/27778 ( 35%)], Train Loss: 0.54276\n",
      "Epoch: 00 [ 9644/27778 ( 35%)], Train Loss: 0.54239\n",
      "Epoch: 00 [ 9684/27778 ( 35%)], Train Loss: 0.54158\n",
      "Epoch: 00 [ 9724/27778 ( 35%)], Train Loss: 0.54146\n",
      "Epoch: 00 [ 9764/27778 ( 35%)], Train Loss: 0.54067\n",
      "Epoch: 00 [ 9804/27778 ( 35%)], Train Loss: 0.54048\n",
      "Epoch: 00 [ 9844/27778 ( 35%)], Train Loss: 0.54024\n",
      "Epoch: 00 [ 9884/27778 ( 36%)], Train Loss: 0.53957\n",
      "Epoch: 00 [ 9924/27778 ( 36%)], Train Loss: 0.53932\n",
      "Epoch: 00 [ 9964/27778 ( 36%)], Train Loss: 0.53897\n",
      "Epoch: 00 [10004/27778 ( 36%)], Train Loss: 0.53801\n",
      "Epoch: 00 [10044/27778 ( 36%)], Train Loss: 0.53649\n",
      "Epoch: 00 [10084/27778 ( 36%)], Train Loss: 0.53576\n",
      "Epoch: 00 [10124/27778 ( 36%)], Train Loss: 0.53537\n",
      "Epoch: 00 [10164/27778 ( 37%)], Train Loss: 0.53454\n",
      "Epoch: 00 [10204/27778 ( 37%)], Train Loss: 0.53363\n",
      "Epoch: 00 [10244/27778 ( 37%)], Train Loss: 0.53249\n",
      "Epoch: 00 [10284/27778 ( 37%)], Train Loss: 0.53191\n",
      "Epoch: 00 [10324/27778 ( 37%)], Train Loss: 0.53144\n",
      "Epoch: 00 [10364/27778 ( 37%)], Train Loss: 0.53150\n",
      "Epoch: 00 [10404/27778 ( 37%)], Train Loss: 0.53074\n",
      "Epoch: 00 [10444/27778 ( 38%)], Train Loss: 0.53007\n",
      "Epoch: 00 [10484/27778 ( 38%)], Train Loss: 0.52980\n",
      "Epoch: 00 [10524/27778 ( 38%)], Train Loss: 0.52865\n",
      "Epoch: 00 [10564/27778 ( 38%)], Train Loss: 0.52732\n",
      "Epoch: 00 [10604/27778 ( 38%)], Train Loss: 0.52684\n",
      "Epoch: 00 [10644/27778 ( 38%)], Train Loss: 0.52635\n",
      "Epoch: 00 [10684/27778 ( 38%)], Train Loss: 0.52572\n",
      "Epoch: 00 [10724/27778 ( 39%)], Train Loss: 0.52531\n",
      "Epoch: 00 [10764/27778 ( 39%)], Train Loss: 0.52500\n",
      "Epoch: 00 [10804/27778 ( 39%)], Train Loss: 0.52418\n",
      "Epoch: 00 [10844/27778 ( 39%)], Train Loss: 0.52324\n",
      "Epoch: 00 [10884/27778 ( 39%)], Train Loss: 0.52251\n",
      "Epoch: 00 [10924/27778 ( 39%)], Train Loss: 0.52187\n",
      "Epoch: 00 [10964/27778 ( 39%)], Train Loss: 0.52118\n",
      "Epoch: 00 [11004/27778 ( 40%)], Train Loss: 0.52085\n",
      "Epoch: 00 [11044/27778 ( 40%)], Train Loss: 0.52049\n",
      "Epoch: 00 [11084/27778 ( 40%)], Train Loss: 0.52056\n",
      "Epoch: 00 [11124/27778 ( 40%)], Train Loss: 0.52039\n",
      "Epoch: 00 [11164/27778 ( 40%)], Train Loss: 0.51951\n",
      "Epoch: 00 [11204/27778 ( 40%)], Train Loss: 0.51893\n",
      "Epoch: 00 [11244/27778 ( 40%)], Train Loss: 0.51803\n",
      "Epoch: 00 [11284/27778 ( 41%)], Train Loss: 0.51684\n",
      "Epoch: 00 [11324/27778 ( 41%)], Train Loss: 0.51586\n",
      "Epoch: 00 [11364/27778 ( 41%)], Train Loss: 0.51487\n",
      "Epoch: 00 [11404/27778 ( 41%)], Train Loss: 0.51393\n",
      "Epoch: 00 [11444/27778 ( 41%)], Train Loss: 0.51334\n",
      "Epoch: 00 [11484/27778 ( 41%)], Train Loss: 0.51286\n",
      "Epoch: 00 [11524/27778 ( 41%)], Train Loss: 0.51207\n",
      "Epoch: 00 [11564/27778 ( 42%)], Train Loss: 0.51152\n",
      "Epoch: 00 [11604/27778 ( 42%)], Train Loss: 0.51093\n",
      "Epoch: 00 [11644/27778 ( 42%)], Train Loss: 0.51012\n",
      "Epoch: 00 [11684/27778 ( 42%)], Train Loss: 0.50960\n",
      "Epoch: 00 [11724/27778 ( 42%)], Train Loss: 0.50880\n",
      "Epoch: 00 [11764/27778 ( 42%)], Train Loss: 0.50853\n",
      "Epoch: 00 [11804/27778 ( 42%)], Train Loss: 0.50770\n",
      "Epoch: 00 [11844/27778 ( 43%)], Train Loss: 0.50699\n",
      "Epoch: 00 [11884/27778 ( 43%)], Train Loss: 0.50649\n",
      "Epoch: 00 [11924/27778 ( 43%)], Train Loss: 0.50585\n",
      "Epoch: 00 [11964/27778 ( 43%)], Train Loss: 0.50519\n",
      "Epoch: 00 [12004/27778 ( 43%)], Train Loss: 0.50448\n",
      "Epoch: 00 [12044/27778 ( 43%)], Train Loss: 0.50418\n",
      "Epoch: 00 [12084/27778 ( 44%)], Train Loss: 0.50374\n",
      "Epoch: 00 [12124/27778 ( 44%)], Train Loss: 0.50344\n",
      "Epoch: 00 [12164/27778 ( 44%)], Train Loss: 0.50309\n",
      "Epoch: 00 [12204/27778 ( 44%)], Train Loss: 0.50255\n",
      "Epoch: 00 [12244/27778 ( 44%)], Train Loss: 0.50186\n",
      "Epoch: 00 [12284/27778 ( 44%)], Train Loss: 0.50137\n",
      "Epoch: 00 [12324/27778 ( 44%)], Train Loss: 0.50060\n",
      "Epoch: 00 [12364/27778 ( 45%)], Train Loss: 0.50037\n",
      "Epoch: 00 [12404/27778 ( 45%)], Train Loss: 0.49940\n",
      "Epoch: 00 [12444/27778 ( 45%)], Train Loss: 0.49897\n",
      "Epoch: 00 [12484/27778 ( 45%)], Train Loss: 0.49817\n",
      "Epoch: 00 [12524/27778 ( 45%)], Train Loss: 0.49789\n",
      "Epoch: 00 [12564/27778 ( 45%)], Train Loss: 0.49758\n",
      "Epoch: 00 [12604/27778 ( 45%)], Train Loss: 0.49694\n",
      "Epoch: 00 [12644/27778 ( 46%)], Train Loss: 0.49601\n",
      "Epoch: 00 [12684/27778 ( 46%)], Train Loss: 0.49550\n",
      "Epoch: 00 [12724/27778 ( 46%)], Train Loss: 0.49514\n",
      "Epoch: 00 [12764/27778 ( 46%)], Train Loss: 0.49467\n",
      "Epoch: 00 [12804/27778 ( 46%)], Train Loss: 0.49384\n",
      "Epoch: 00 [12844/27778 ( 46%)], Train Loss: 0.49370\n",
      "Epoch: 00 [12884/27778 ( 46%)], Train Loss: 0.49299\n",
      "Epoch: 00 [12924/27778 ( 47%)], Train Loss: 0.49293\n",
      "Epoch: 00 [12964/27778 ( 47%)], Train Loss: 0.49220\n",
      "Epoch: 00 [13004/27778 ( 47%)], Train Loss: 0.49187\n",
      "Epoch: 00 [13044/27778 ( 47%)], Train Loss: 0.49126\n",
      "Epoch: 00 [13084/27778 ( 47%)], Train Loss: 0.49146\n",
      "Epoch: 00 [13124/27778 ( 47%)], Train Loss: 0.49103\n",
      "Epoch: 00 [13164/27778 ( 47%)], Train Loss: 0.49078\n",
      "Epoch: 00 [13204/27778 ( 48%)], Train Loss: 0.49003\n",
      "Epoch: 00 [13244/27778 ( 48%)], Train Loss: 0.48994\n",
      "Epoch: 00 [13284/27778 ( 48%)], Train Loss: 0.48925\n",
      "Epoch: 00 [13324/27778 ( 48%)], Train Loss: 0.48894\n",
      "Epoch: 00 [13364/27778 ( 48%)], Train Loss: 0.48880\n",
      "Epoch: 00 [13404/27778 ( 48%)], Train Loss: 0.48843\n",
      "Epoch: 00 [13444/27778 ( 48%)], Train Loss: 0.48822\n",
      "Epoch: 00 [13484/27778 ( 49%)], Train Loss: 0.48818\n",
      "Epoch: 00 [13524/27778 ( 49%)], Train Loss: 0.48844\n",
      "Epoch: 00 [13564/27778 ( 49%)], Train Loss: 0.48778\n",
      "Epoch: 00 [13604/27778 ( 49%)], Train Loss: 0.48707\n",
      "Epoch: 00 [13644/27778 ( 49%)], Train Loss: 0.48655\n",
      "Epoch: 00 [13684/27778 ( 49%)], Train Loss: 0.48646\n",
      "Epoch: 00 [13724/27778 ( 49%)], Train Loss: 0.48653\n",
      "Epoch: 00 [13764/27778 ( 50%)], Train Loss: 0.48620\n",
      "Epoch: 00 [13804/27778 ( 50%)], Train Loss: 0.48591\n",
      "Epoch: 00 [13844/27778 ( 50%)], Train Loss: 0.48529\n",
      "Epoch: 00 [13884/27778 ( 50%)], Train Loss: 0.48487\n",
      "Epoch: 00 [13924/27778 ( 50%)], Train Loss: 0.48437\n",
      "Epoch: 00 [13964/27778 ( 50%)], Train Loss: 0.48430\n",
      "Epoch: 00 [14004/27778 ( 50%)], Train Loss: 0.48358\n",
      "Epoch: 00 [14044/27778 ( 51%)], Train Loss: 0.48343\n",
      "Epoch: 00 [14084/27778 ( 51%)], Train Loss: 0.48284\n",
      "Epoch: 00 [14124/27778 ( 51%)], Train Loss: 0.48219\n",
      "Epoch: 00 [14164/27778 ( 51%)], Train Loss: 0.48220\n",
      "Epoch: 00 [14204/27778 ( 51%)], Train Loss: 0.48118\n",
      "Epoch: 00 [14244/27778 ( 51%)], Train Loss: 0.48109\n",
      "Epoch: 00 [14284/27778 ( 51%)], Train Loss: 0.48041\n",
      "Epoch: 00 [14324/27778 ( 52%)], Train Loss: 0.47980\n",
      "Epoch: 00 [14364/27778 ( 52%)], Train Loss: 0.47952\n",
      "Epoch: 00 [14404/27778 ( 52%)], Train Loss: 0.47943\n",
      "Epoch: 00 [14444/27778 ( 52%)], Train Loss: 0.47880\n",
      "Epoch: 00 [14484/27778 ( 52%)], Train Loss: 0.47806\n",
      "Epoch: 00 [14524/27778 ( 52%)], Train Loss: 0.47751\n",
      "Epoch: 00 [14564/27778 ( 52%)], Train Loss: 0.47732\n",
      "Epoch: 00 [14604/27778 ( 53%)], Train Loss: 0.47691\n",
      "Epoch: 00 [14644/27778 ( 53%)], Train Loss: 0.47646\n",
      "Epoch: 00 [14684/27778 ( 53%)], Train Loss: 0.47609\n",
      "Epoch: 00 [14724/27778 ( 53%)], Train Loss: 0.47530\n",
      "Epoch: 00 [14764/27778 ( 53%)], Train Loss: 0.47473\n",
      "Epoch: 00 [14804/27778 ( 53%)], Train Loss: 0.47449\n",
      "Epoch: 00 [14844/27778 ( 53%)], Train Loss: 0.47429\n",
      "Epoch: 00 [14884/27778 ( 54%)], Train Loss: 0.47410\n",
      "Epoch: 00 [14924/27778 ( 54%)], Train Loss: 0.47359\n",
      "Epoch: 00 [14964/27778 ( 54%)], Train Loss: 0.47323\n",
      "Epoch: 00 [15004/27778 ( 54%)], Train Loss: 0.47290\n",
      "Epoch: 00 [15044/27778 ( 54%)], Train Loss: 0.47286\n",
      "Epoch: 00 [15084/27778 ( 54%)], Train Loss: 0.47232\n",
      "Epoch: 00 [15124/27778 ( 54%)], Train Loss: 0.47203\n",
      "Epoch: 00 [15164/27778 ( 55%)], Train Loss: 0.47217\n",
      "Epoch: 00 [15204/27778 ( 55%)], Train Loss: 0.47196\n",
      "Epoch: 00 [15244/27778 ( 55%)], Train Loss: 0.47154\n",
      "Epoch: 00 [15284/27778 ( 55%)], Train Loss: 0.47172\n",
      "Epoch: 00 [15324/27778 ( 55%)], Train Loss: 0.47151\n",
      "Epoch: 00 [15364/27778 ( 55%)], Train Loss: 0.47095\n",
      "Epoch: 00 [15404/27778 ( 55%)], Train Loss: 0.47051\n",
      "Epoch: 00 [15444/27778 ( 56%)], Train Loss: 0.47013\n",
      "Epoch: 00 [15484/27778 ( 56%)], Train Loss: 0.46961\n",
      "Epoch: 00 [15524/27778 ( 56%)], Train Loss: 0.46945\n",
      "Epoch: 00 [15564/27778 ( 56%)], Train Loss: 0.46873\n",
      "Epoch: 00 [15604/27778 ( 56%)], Train Loss: 0.46844\n",
      "Epoch: 00 [15644/27778 ( 56%)], Train Loss: 0.46797\n",
      "Epoch: 00 [15684/27778 ( 56%)], Train Loss: 0.46824\n",
      "Epoch: 00 [15724/27778 ( 57%)], Train Loss: 0.46784\n",
      "Epoch: 00 [15764/27778 ( 57%)], Train Loss: 0.46761\n",
      "Epoch: 00 [15804/27778 ( 57%)], Train Loss: 0.46726\n",
      "Epoch: 00 [15844/27778 ( 57%)], Train Loss: 0.46694\n",
      "Epoch: 00 [15884/27778 ( 57%)], Train Loss: 0.46666\n",
      "Epoch: 00 [15924/27778 ( 57%)], Train Loss: 0.46659\n",
      "Epoch: 00 [15964/27778 ( 57%)], Train Loss: 0.46642\n",
      "Epoch: 00 [16004/27778 ( 58%)], Train Loss: 0.46605\n",
      "Epoch: 00 [16044/27778 ( 58%)], Train Loss: 0.46552\n",
      "Epoch: 00 [16084/27778 ( 58%)], Train Loss: 0.46496\n",
      "Epoch: 00 [16124/27778 ( 58%)], Train Loss: 0.46487\n",
      "Epoch: 00 [16164/27778 ( 58%)], Train Loss: 0.46427\n",
      "Epoch: 00 [16204/27778 ( 58%)], Train Loss: 0.46370\n",
      "Epoch: 00 [16244/27778 ( 58%)], Train Loss: 0.46345\n",
      "Epoch: 00 [16284/27778 ( 59%)], Train Loss: 0.46296\n",
      "Epoch: 00 [16324/27778 ( 59%)], Train Loss: 0.46296\n",
      "Epoch: 00 [16364/27778 ( 59%)], Train Loss: 0.46279\n",
      "Epoch: 00 [16404/27778 ( 59%)], Train Loss: 0.46214\n",
      "Epoch: 00 [16444/27778 ( 59%)], Train Loss: 0.46170\n",
      "Epoch: 00 [16484/27778 ( 59%)], Train Loss: 0.46189\n",
      "Epoch: 00 [16524/27778 ( 59%)], Train Loss: 0.46182\n",
      "Epoch: 00 [16564/27778 ( 60%)], Train Loss: 0.46185\n",
      "Epoch: 00 [16604/27778 ( 60%)], Train Loss: 0.46140\n",
      "Epoch: 00 [16644/27778 ( 60%)], Train Loss: 0.46117\n",
      "Epoch: 00 [16684/27778 ( 60%)], Train Loss: 0.46118\n",
      "Epoch: 00 [16724/27778 ( 60%)], Train Loss: 0.46072\n",
      "Epoch: 00 [16764/27778 ( 60%)], Train Loss: 0.46010\n",
      "Epoch: 00 [16804/27778 ( 60%)], Train Loss: 0.45975\n",
      "Epoch: 00 [16844/27778 ( 61%)], Train Loss: 0.45961\n",
      "Epoch: 00 [16884/27778 ( 61%)], Train Loss: 0.45945\n",
      "Epoch: 00 [16924/27778 ( 61%)], Train Loss: 0.45926\n",
      "Epoch: 00 [16964/27778 ( 61%)], Train Loss: 0.45873\n",
      "Epoch: 00 [17004/27778 ( 61%)], Train Loss: 0.45811\n",
      "Epoch: 00 [17044/27778 ( 61%)], Train Loss: 0.45787\n",
      "Epoch: 00 [17084/27778 ( 62%)], Train Loss: 0.45743\n",
      "Epoch: 00 [17124/27778 ( 62%)], Train Loss: 0.45705\n",
      "Epoch: 00 [17164/27778 ( 62%)], Train Loss: 0.45640\n",
      "Epoch: 00 [17204/27778 ( 62%)], Train Loss: 0.45620\n",
      "Epoch: 00 [17244/27778 ( 62%)], Train Loss: 0.45590\n",
      "Epoch: 00 [17284/27778 ( 62%)], Train Loss: 0.45557\n",
      "Epoch: 00 [17324/27778 ( 62%)], Train Loss: 0.45519\n",
      "Epoch: 00 [17364/27778 ( 63%)], Train Loss: 0.45511\n",
      "Epoch: 00 [17404/27778 ( 63%)], Train Loss: 0.45538\n",
      "Epoch: 00 [17444/27778 ( 63%)], Train Loss: 0.45509\n",
      "Epoch: 00 [17484/27778 ( 63%)], Train Loss: 0.45485\n",
      "Epoch: 00 [17524/27778 ( 63%)], Train Loss: 0.45431\n",
      "Epoch: 00 [17564/27778 ( 63%)], Train Loss: 0.45372\n",
      "Epoch: 00 [17604/27778 ( 63%)], Train Loss: 0.45339\n",
      "Epoch: 00 [17644/27778 ( 64%)], Train Loss: 0.45324\n",
      "Epoch: 00 [17684/27778 ( 64%)], Train Loss: 0.45346\n",
      "Epoch: 00 [17724/27778 ( 64%)], Train Loss: 0.45313\n",
      "Epoch: 00 [17764/27778 ( 64%)], Train Loss: 0.45280\n",
      "Epoch: 00 [17804/27778 ( 64%)], Train Loss: 0.45289\n",
      "Epoch: 00 [17844/27778 ( 64%)], Train Loss: 0.45270\n",
      "Epoch: 00 [17884/27778 ( 64%)], Train Loss: 0.45228\n",
      "Epoch: 00 [17924/27778 ( 65%)], Train Loss: 0.45173\n",
      "Epoch: 00 [17964/27778 ( 65%)], Train Loss: 0.45130\n",
      "Epoch: 00 [18004/27778 ( 65%)], Train Loss: 0.45088\n",
      "Epoch: 00 [18044/27778 ( 65%)], Train Loss: 0.45052\n",
      "Epoch: 00 [18084/27778 ( 65%)], Train Loss: 0.45034\n",
      "Epoch: 00 [18124/27778 ( 65%)], Train Loss: 0.44999\n",
      "Epoch: 00 [18164/27778 ( 65%)], Train Loss: 0.44952\n",
      "Epoch: 00 [18204/27778 ( 66%)], Train Loss: 0.44938\n",
      "Epoch: 00 [18244/27778 ( 66%)], Train Loss: 0.44937\n",
      "Epoch: 00 [18284/27778 ( 66%)], Train Loss: 0.44877\n",
      "Epoch: 00 [18324/27778 ( 66%)], Train Loss: 0.44878\n",
      "Epoch: 00 [18364/27778 ( 66%)], Train Loss: 0.44892\n",
      "Epoch: 00 [18404/27778 ( 66%)], Train Loss: 0.44859\n",
      "Epoch: 00 [18444/27778 ( 66%)], Train Loss: 0.44824\n",
      "Epoch: 00 [18484/27778 ( 67%)], Train Loss: 0.44804\n",
      "Epoch: 00 [18524/27778 ( 67%)], Train Loss: 0.44778\n",
      "Epoch: 00 [18564/27778 ( 67%)], Train Loss: 0.44757\n",
      "Epoch: 00 [18604/27778 ( 67%)], Train Loss: 0.44759\n",
      "Epoch: 00 [18644/27778 ( 67%)], Train Loss: 0.44740\n",
      "Epoch: 00 [18684/27778 ( 67%)], Train Loss: 0.44691\n",
      "Epoch: 00 [18724/27778 ( 67%)], Train Loss: 0.44662\n",
      "Epoch: 00 [18764/27778 ( 68%)], Train Loss: 0.44601\n",
      "Epoch: 00 [18804/27778 ( 68%)], Train Loss: 0.44555\n",
      "Epoch: 00 [18844/27778 ( 68%)], Train Loss: 0.44567\n",
      "Epoch: 00 [18884/27778 ( 68%)], Train Loss: 0.44548\n",
      "Epoch: 00 [18924/27778 ( 68%)], Train Loss: 0.44562\n",
      "Epoch: 00 [18964/27778 ( 68%)], Train Loss: 0.44549\n",
      "Epoch: 00 [19004/27778 ( 68%)], Train Loss: 0.44525\n",
      "Epoch: 00 [19044/27778 ( 69%)], Train Loss: 0.44468\n",
      "Epoch: 00 [19084/27778 ( 69%)], Train Loss: 0.44441\n",
      "Epoch: 00 [19124/27778 ( 69%)], Train Loss: 0.44475\n",
      "Epoch: 00 [19164/27778 ( 69%)], Train Loss: 0.44496\n",
      "Epoch: 00 [19204/27778 ( 69%)], Train Loss: 0.44470\n",
      "Epoch: 00 [19244/27778 ( 69%)], Train Loss: 0.44431\n",
      "Epoch: 00 [19284/27778 ( 69%)], Train Loss: 0.44404\n",
      "Epoch: 00 [19324/27778 ( 70%)], Train Loss: 0.44391\n",
      "Epoch: 00 [19364/27778 ( 70%)], Train Loss: 0.44349\n",
      "Epoch: 00 [19404/27778 ( 70%)], Train Loss: 0.44328\n",
      "Epoch: 00 [19444/27778 ( 70%)], Train Loss: 0.44283\n",
      "Epoch: 00 [19484/27778 ( 70%)], Train Loss: 0.44245\n",
      "Epoch: 00 [19524/27778 ( 70%)], Train Loss: 0.44218\n",
      "Epoch: 00 [19564/27778 ( 70%)], Train Loss: 0.44203\n",
      "Epoch: 00 [19604/27778 ( 71%)], Train Loss: 0.44184\n",
      "Epoch: 00 [19644/27778 ( 71%)], Train Loss: 0.44134\n",
      "Epoch: 00 [19684/27778 ( 71%)], Train Loss: 0.44093\n",
      "Epoch: 00 [19724/27778 ( 71%)], Train Loss: 0.44062\n",
      "Epoch: 00 [19764/27778 ( 71%)], Train Loss: 0.44071\n",
      "Epoch: 00 [19804/27778 ( 71%)], Train Loss: 0.44028\n",
      "Epoch: 00 [19844/27778 ( 71%)], Train Loss: 0.43979\n",
      "Epoch: 00 [19884/27778 ( 72%)], Train Loss: 0.43961\n",
      "Epoch: 00 [19924/27778 ( 72%)], Train Loss: 0.43965\n",
      "Epoch: 00 [19964/27778 ( 72%)], Train Loss: 0.43941\n",
      "Epoch: 00 [20004/27778 ( 72%)], Train Loss: 0.43933\n",
      "Epoch: 00 [20044/27778 ( 72%)], Train Loss: 0.43936\n",
      "Epoch: 00 [20084/27778 ( 72%)], Train Loss: 0.43930\n",
      "Epoch: 00 [20124/27778 ( 72%)], Train Loss: 0.43927\n",
      "Epoch: 00 [20164/27778 ( 73%)], Train Loss: 0.43906\n",
      "Epoch: 00 [20204/27778 ( 73%)], Train Loss: 0.43870\n",
      "Epoch: 00 [20244/27778 ( 73%)], Train Loss: 0.43840\n",
      "Epoch: 00 [20284/27778 ( 73%)], Train Loss: 0.43830\n",
      "Epoch: 00 [20324/27778 ( 73%)], Train Loss: 0.43794\n",
      "Epoch: 00 [20364/27778 ( 73%)], Train Loss: 0.43756\n",
      "Epoch: 00 [20404/27778 ( 73%)], Train Loss: 0.43733\n",
      "Epoch: 00 [20444/27778 ( 74%)], Train Loss: 0.43693\n",
      "Epoch: 00 [20484/27778 ( 74%)], Train Loss: 0.43698\n",
      "Epoch: 00 [20524/27778 ( 74%)], Train Loss: 0.43679\n",
      "Epoch: 00 [20564/27778 ( 74%)], Train Loss: 0.43660\n",
      "Epoch: 00 [20604/27778 ( 74%)], Train Loss: 0.43621\n",
      "Epoch: 00 [20644/27778 ( 74%)], Train Loss: 0.43605\n",
      "Epoch: 00 [20684/27778 ( 74%)], Train Loss: 0.43569\n",
      "Epoch: 00 [20724/27778 ( 75%)], Train Loss: 0.43550\n",
      "Epoch: 00 [20764/27778 ( 75%)], Train Loss: 0.43507\n",
      "Epoch: 00 [20804/27778 ( 75%)], Train Loss: 0.43488\n",
      "Epoch: 00 [20844/27778 ( 75%)], Train Loss: 0.43477\n",
      "Epoch: 00 [20884/27778 ( 75%)], Train Loss: 0.43455\n",
      "Epoch: 00 [20924/27778 ( 75%)], Train Loss: 0.43474\n",
      "Epoch: 00 [20964/27778 ( 75%)], Train Loss: 0.43462\n",
      "Epoch: 00 [21004/27778 ( 76%)], Train Loss: 0.43460\n",
      "Epoch: 00 [21044/27778 ( 76%)], Train Loss: 0.43460\n",
      "Epoch: 00 [21084/27778 ( 76%)], Train Loss: 0.43430\n",
      "Epoch: 00 [21124/27778 ( 76%)], Train Loss: 0.43427\n",
      "Epoch: 00 [21164/27778 ( 76%)], Train Loss: 0.43411\n",
      "Epoch: 00 [21204/27778 ( 76%)], Train Loss: 0.43382\n",
      "Epoch: 00 [21244/27778 ( 76%)], Train Loss: 0.43348\n",
      "Epoch: 00 [21284/27778 ( 77%)], Train Loss: 0.43313\n",
      "Epoch: 00 [21324/27778 ( 77%)], Train Loss: 0.43279\n",
      "Epoch: 00 [21364/27778 ( 77%)], Train Loss: 0.43315\n",
      "Epoch: 00 [21404/27778 ( 77%)], Train Loss: 0.43281\n",
      "Epoch: 00 [21444/27778 ( 77%)], Train Loss: 0.43293\n",
      "Epoch: 00 [21484/27778 ( 77%)], Train Loss: 0.43289\n",
      "Epoch: 00 [21524/27778 ( 77%)], Train Loss: 0.43274\n",
      "Epoch: 00 [21564/27778 ( 78%)], Train Loss: 0.43260\n",
      "Epoch: 00 [21604/27778 ( 78%)], Train Loss: 0.43226\n",
      "Epoch: 00 [21644/27778 ( 78%)], Train Loss: 0.43181\n",
      "Epoch: 00 [21684/27778 ( 78%)], Train Loss: 0.43144\n",
      "Epoch: 00 [21724/27778 ( 78%)], Train Loss: 0.43101\n",
      "Epoch: 00 [21764/27778 ( 78%)], Train Loss: 0.43066\n",
      "Epoch: 00 [21804/27778 ( 78%)], Train Loss: 0.43049\n",
      "Epoch: 00 [21844/27778 ( 79%)], Train Loss: 0.43019\n",
      "Epoch: 00 [21884/27778 ( 79%)], Train Loss: 0.43022\n",
      "Epoch: 00 [21924/27778 ( 79%)], Train Loss: 0.43018\n",
      "Epoch: 00 [21964/27778 ( 79%)], Train Loss: 0.42989\n",
      "Epoch: 00 [22004/27778 ( 79%)], Train Loss: 0.42975\n",
      "Epoch: 00 [22044/27778 ( 79%)], Train Loss: 0.42964\n",
      "Epoch: 00 [22084/27778 ( 80%)], Train Loss: 0.42939\n",
      "Epoch: 00 [22124/27778 ( 80%)], Train Loss: 0.42909\n",
      "Epoch: 00 [22164/27778 ( 80%)], Train Loss: 0.42878\n",
      "Epoch: 00 [22204/27778 ( 80%)], Train Loss: 0.42852\n",
      "Epoch: 00 [22244/27778 ( 80%)], Train Loss: 0.42839\n",
      "Epoch: 00 [22284/27778 ( 80%)], Train Loss: 0.42816\n",
      "Epoch: 00 [22324/27778 ( 80%)], Train Loss: 0.42822\n",
      "Epoch: 00 [22364/27778 ( 81%)], Train Loss: 0.42808\n",
      "Epoch: 00 [22404/27778 ( 81%)], Train Loss: 0.42773\n",
      "Epoch: 00 [22444/27778 ( 81%)], Train Loss: 0.42770\n",
      "Epoch: 00 [22484/27778 ( 81%)], Train Loss: 0.42750\n",
      "Epoch: 00 [22524/27778 ( 81%)], Train Loss: 0.42735\n",
      "Epoch: 00 [22564/27778 ( 81%)], Train Loss: 0.42702\n",
      "Epoch: 00 [22604/27778 ( 81%)], Train Loss: 0.42663\n",
      "Epoch: 00 [22644/27778 ( 82%)], Train Loss: 0.42654\n",
      "Epoch: 00 [22684/27778 ( 82%)], Train Loss: 0.42641\n",
      "Epoch: 00 [22724/27778 ( 82%)], Train Loss: 0.42655\n",
      "Epoch: 00 [22764/27778 ( 82%)], Train Loss: 0.42662\n",
      "Epoch: 00 [22804/27778 ( 82%)], Train Loss: 0.42630\n",
      "Epoch: 00 [22844/27778 ( 82%)], Train Loss: 0.42625\n",
      "Epoch: 00 [22884/27778 ( 82%)], Train Loss: 0.42589\n",
      "Epoch: 00 [22924/27778 ( 83%)], Train Loss: 0.42559\n",
      "Epoch: 00 [22964/27778 ( 83%)], Train Loss: 0.42522\n",
      "Epoch: 00 [23004/27778 ( 83%)], Train Loss: 0.42493\n",
      "Epoch: 00 [23044/27778 ( 83%)], Train Loss: 0.42468\n",
      "Epoch: 00 [23084/27778 ( 83%)], Train Loss: 0.42488\n",
      "Epoch: 00 [23124/27778 ( 83%)], Train Loss: 0.42475\n",
      "Epoch: 00 [23164/27778 ( 83%)], Train Loss: 0.42434\n",
      "Epoch: 00 [23204/27778 ( 84%)], Train Loss: 0.42449\n",
      "Epoch: 00 [23244/27778 ( 84%)], Train Loss: 0.42442\n",
      "Epoch: 00 [23284/27778 ( 84%)], Train Loss: 0.42424\n",
      "Epoch: 00 [23324/27778 ( 84%)], Train Loss: 0.42414\n",
      "Epoch: 00 [23364/27778 ( 84%)], Train Loss: 0.42397\n",
      "Epoch: 00 [23404/27778 ( 84%)], Train Loss: 0.42371\n",
      "Epoch: 00 [23444/27778 ( 84%)], Train Loss: 0.42349\n",
      "Epoch: 00 [23484/27778 ( 85%)], Train Loss: 0.42343\n",
      "Epoch: 00 [23524/27778 ( 85%)], Train Loss: 0.42291\n",
      "Epoch: 00 [23564/27778 ( 85%)], Train Loss: 0.42278\n",
      "Epoch: 00 [23604/27778 ( 85%)], Train Loss: 0.42249\n",
      "Epoch: 00 [23644/27778 ( 85%)], Train Loss: 0.42213\n",
      "Epoch: 00 [23684/27778 ( 85%)], Train Loss: 0.42200\n",
      "Epoch: 00 [23724/27778 ( 85%)], Train Loss: 0.42178\n",
      "Epoch: 00 [23764/27778 ( 86%)], Train Loss: 0.42151\n",
      "Epoch: 00 [23804/27778 ( 86%)], Train Loss: 0.42125\n",
      "Epoch: 00 [23844/27778 ( 86%)], Train Loss: 0.42100\n",
      "Epoch: 00 [23884/27778 ( 86%)], Train Loss: 0.42077\n",
      "Epoch: 00 [23924/27778 ( 86%)], Train Loss: 0.42067\n",
      "Epoch: 00 [23964/27778 ( 86%)], Train Loss: 0.42051\n",
      "Epoch: 00 [24004/27778 ( 86%)], Train Loss: 0.42037\n",
      "Epoch: 00 [24044/27778 ( 87%)], Train Loss: 0.42001\n",
      "Epoch: 00 [24084/27778 ( 87%)], Train Loss: 0.41973\n",
      "Epoch: 00 [24124/27778 ( 87%)], Train Loss: 0.41939\n",
      "Epoch: 00 [24164/27778 ( 87%)], Train Loss: 0.41925\n",
      "Epoch: 00 [24204/27778 ( 87%)], Train Loss: 0.41903\n",
      "Epoch: 00 [24244/27778 ( 87%)], Train Loss: 0.41864\n",
      "Epoch: 00 [24284/27778 ( 87%)], Train Loss: 0.41851\n",
      "Epoch: 00 [24324/27778 ( 88%)], Train Loss: 0.41837\n",
      "Epoch: 00 [24364/27778 ( 88%)], Train Loss: 0.41812\n",
      "Epoch: 00 [24404/27778 ( 88%)], Train Loss: 0.41787\n",
      "Epoch: 00 [24444/27778 ( 88%)], Train Loss: 0.41765\n",
      "Epoch: 00 [24484/27778 ( 88%)], Train Loss: 0.41759\n",
      "Epoch: 00 [24524/27778 ( 88%)], Train Loss: 0.41751\n",
      "Epoch: 00 [24564/27778 ( 88%)], Train Loss: 0.41716\n",
      "Epoch: 00 [24604/27778 ( 89%)], Train Loss: 0.41722\n",
      "Epoch: 00 [24644/27778 ( 89%)], Train Loss: 0.41707\n",
      "Epoch: 00 [24684/27778 ( 89%)], Train Loss: 0.41690\n",
      "Epoch: 00 [24724/27778 ( 89%)], Train Loss: 0.41679\n",
      "Epoch: 00 [24764/27778 ( 89%)], Train Loss: 0.41680\n",
      "Epoch: 00 [24804/27778 ( 89%)], Train Loss: 0.41662\n",
      "Epoch: 00 [24844/27778 ( 89%)], Train Loss: 0.41640\n",
      "Epoch: 00 [24884/27778 ( 90%)], Train Loss: 0.41628\n",
      "Epoch: 00 [24924/27778 ( 90%)], Train Loss: 0.41623\n",
      "Epoch: 00 [24964/27778 ( 90%)], Train Loss: 0.41625\n",
      "Epoch: 00 [25004/27778 ( 90%)], Train Loss: 0.41607\n",
      "Epoch: 00 [25044/27778 ( 90%)], Train Loss: 0.41564\n",
      "Epoch: 00 [25084/27778 ( 90%)], Train Loss: 0.41539\n",
      "Epoch: 00 [25124/27778 ( 90%)], Train Loss: 0.41515\n",
      "Epoch: 00 [25164/27778 ( 91%)], Train Loss: 0.41503\n",
      "Epoch: 00 [25204/27778 ( 91%)], Train Loss: 0.41489\n",
      "Epoch: 00 [25244/27778 ( 91%)], Train Loss: 0.41467\n",
      "Epoch: 00 [25284/27778 ( 91%)], Train Loss: 0.41462\n",
      "Epoch: 00 [25324/27778 ( 91%)], Train Loss: 0.41447\n",
      "Epoch: 00 [25364/27778 ( 91%)], Train Loss: 0.41424\n",
      "Epoch: 00 [25404/27778 ( 91%)], Train Loss: 0.41415\n",
      "Epoch: 00 [25444/27778 ( 92%)], Train Loss: 0.41398\n",
      "Epoch: 00 [25484/27778 ( 92%)], Train Loss: 0.41377\n",
      "Epoch: 00 [25524/27778 ( 92%)], Train Loss: 0.41354\n",
      "Epoch: 00 [25564/27778 ( 92%)], Train Loss: 0.41321\n",
      "Epoch: 00 [25604/27778 ( 92%)], Train Loss: 0.41308\n",
      "Epoch: 00 [25644/27778 ( 92%)], Train Loss: 0.41277\n",
      "Epoch: 00 [25684/27778 ( 92%)], Train Loss: 0.41265\n",
      "Epoch: 00 [25724/27778 ( 93%)], Train Loss: 0.41249\n",
      "Epoch: 00 [25764/27778 ( 93%)], Train Loss: 0.41229\n",
      "Epoch: 00 [25804/27778 ( 93%)], Train Loss: 0.41201\n",
      "Epoch: 00 [25844/27778 ( 93%)], Train Loss: 0.41189\n",
      "Epoch: 00 [25884/27778 ( 93%)], Train Loss: 0.41172\n",
      "Epoch: 00 [25924/27778 ( 93%)], Train Loss: 0.41149\n",
      "Epoch: 00 [25964/27778 ( 93%)], Train Loss: 0.41159\n",
      "Epoch: 00 [26004/27778 ( 94%)], Train Loss: 0.41139\n",
      "Epoch: 00 [26044/27778 ( 94%)], Train Loss: 0.41121\n",
      "Epoch: 00 [26084/27778 ( 94%)], Train Loss: 0.41092\n",
      "Epoch: 00 [26124/27778 ( 94%)], Train Loss: 0.41112\n",
      "Epoch: 00 [26164/27778 ( 94%)], Train Loss: 0.41084\n",
      "Epoch: 00 [26204/27778 ( 94%)], Train Loss: 0.41053\n",
      "Epoch: 00 [26244/27778 ( 94%)], Train Loss: 0.41034\n",
      "Epoch: 00 [26284/27778 ( 95%)], Train Loss: 0.41017\n",
      "Epoch: 00 [26324/27778 ( 95%)], Train Loss: 0.41015\n",
      "Epoch: 00 [26364/27778 ( 95%)], Train Loss: 0.41014\n",
      "Epoch: 00 [26404/27778 ( 95%)], Train Loss: 0.41004\n",
      "Epoch: 00 [26444/27778 ( 95%)], Train Loss: 0.40977\n",
      "Epoch: 00 [26484/27778 ( 95%)], Train Loss: 0.40966\n",
      "Epoch: 00 [26524/27778 ( 95%)], Train Loss: 0.40939\n",
      "Epoch: 00 [26564/27778 ( 96%)], Train Loss: 0.40910\n",
      "Epoch: 00 [26604/27778 ( 96%)], Train Loss: 0.40918\n",
      "Epoch: 00 [26644/27778 ( 96%)], Train Loss: 0.40914\n",
      "Epoch: 00 [26684/27778 ( 96%)], Train Loss: 0.40898\n",
      "Epoch: 00 [26724/27778 ( 96%)], Train Loss: 0.40865\n",
      "Epoch: 00 [26764/27778 ( 96%)], Train Loss: 0.40860\n",
      "Epoch: 00 [26804/27778 ( 96%)], Train Loss: 0.40843\n",
      "Epoch: 00 [26844/27778 ( 97%)], Train Loss: 0.40823\n",
      "Epoch: 00 [26884/27778 ( 97%)], Train Loss: 0.40798\n",
      "Epoch: 00 [26924/27778 ( 97%)], Train Loss: 0.40775\n",
      "Epoch: 00 [26964/27778 ( 97%)], Train Loss: 0.40752\n",
      "Epoch: 00 [27004/27778 ( 97%)], Train Loss: 0.40740\n",
      "Epoch: 00 [27044/27778 ( 97%)], Train Loss: 0.40709\n",
      "Epoch: 00 [27084/27778 ( 98%)], Train Loss: 0.40712\n",
      "Epoch: 00 [27124/27778 ( 98%)], Train Loss: 0.40695\n",
      "Epoch: 00 [27164/27778 ( 98%)], Train Loss: 0.40695\n",
      "Epoch: 00 [27204/27778 ( 98%)], Train Loss: 0.40678\n",
      "Epoch: 00 [27244/27778 ( 98%)], Train Loss: 0.40656\n",
      "Epoch: 00 [27284/27778 ( 98%)], Train Loss: 0.40632\n",
      "Epoch: 00 [27324/27778 ( 98%)], Train Loss: 0.40619\n",
      "Epoch: 00 [27364/27778 ( 99%)], Train Loss: 0.40597\n",
      "Epoch: 00 [27404/27778 ( 99%)], Train Loss: 0.40581\n",
      "Epoch: 00 [27444/27778 ( 99%)], Train Loss: 0.40573\n",
      "Epoch: 00 [27484/27778 ( 99%)], Train Loss: 0.40564\n",
      "Epoch: 00 [27524/27778 ( 99%)], Train Loss: 0.40538\n",
      "Epoch: 00 [27564/27778 ( 99%)], Train Loss: 0.40520\n",
      "Epoch: 00 [27604/27778 ( 99%)], Train Loss: 0.40494\n",
      "Epoch: 00 [27644/27778 (100%)], Train Loss: 0.40459\n",
      "Epoch: 00 [27684/27778 (100%)], Train Loss: 0.40439\n",
      "Epoch: 00 [27724/27778 (100%)], Train Loss: 0.40432\n",
      "Epoch: 00 [27764/27778 (100%)], Train Loss: 0.40405\n",
      "Epoch: 00 [27778/27778 (100%)], Train Loss: 0.40396\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.58705\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.58705\n",
      "Saving model checkpoint to output/checkpoint-fold-5.\n",
      "\n",
      "Total Training Time: 3234.000950574875secs, Average Training Time per Epoch: 3234.000950574875secs.\n",
      "Total Validation Time: 141.4863977432251secs, Average Validation Time per Epoch: 141.4863977432251secs.\n",
      "--------------------------------------------------\n",
      "FOLD: 6\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27767, Num examples Valid=3955\n",
      "Total Training Steps: 3471, Total Warmup Steps: 347\n",
      "Epoch: 00 [    4/27767 (  0%)], Train Loss: 2.74330\n",
      "Epoch: 00 [   44/27767 (  0%)], Train Loss: 2.76654\n",
      "Epoch: 00 [   84/27767 (  0%)], Train Loss: 2.75421\n",
      "Epoch: 00 [  124/27767 (  0%)], Train Loss: 2.75438\n",
      "Epoch: 00 [  164/27767 (  1%)], Train Loss: 2.72641\n",
      "Epoch: 00 [  204/27767 (  1%)], Train Loss: 2.68662\n",
      "Epoch: 00 [  244/27767 (  1%)], Train Loss: 2.64362\n",
      "Epoch: 00 [  284/27767 (  1%)], Train Loss: 2.59175\n",
      "Epoch: 00 [  324/27767 (  1%)], Train Loss: 2.53781\n",
      "Epoch: 00 [  364/27767 (  1%)], Train Loss: 2.48603\n",
      "Epoch: 00 [  404/27767 (  1%)], Train Loss: 2.41310\n",
      "Epoch: 00 [  444/27767 (  2%)], Train Loss: 2.34221\n",
      "Epoch: 00 [  484/27767 (  2%)], Train Loss: 2.26530\n",
      "Epoch: 00 [  524/27767 (  2%)], Train Loss: 2.18156\n",
      "Epoch: 00 [  564/27767 (  2%)], Train Loss: 2.10263\n",
      "Epoch: 00 [  604/27767 (  2%)], Train Loss: 2.01763\n",
      "Epoch: 00 [  644/27767 (  2%)], Train Loss: 1.94441\n",
      "Epoch: 00 [  684/27767 (  2%)], Train Loss: 1.87354\n",
      "Epoch: 00 [  724/27767 (  3%)], Train Loss: 1.80473\n",
      "Epoch: 00 [  764/27767 (  3%)], Train Loss: 1.74737\n",
      "Epoch: 00 [  804/27767 (  3%)], Train Loss: 1.69188\n",
      "Epoch: 00 [  844/27767 (  3%)], Train Loss: 1.63580\n",
      "Epoch: 00 [  884/27767 (  3%)], Train Loss: 1.58805\n",
      "Epoch: 00 [  924/27767 (  3%)], Train Loss: 1.53344\n",
      "Epoch: 00 [  964/27767 (  3%)], Train Loss: 1.49116\n",
      "Epoch: 00 [ 1004/27767 (  4%)], Train Loss: 1.45088\n",
      "Epoch: 00 [ 1044/27767 (  4%)], Train Loss: 1.41363\n",
      "Epoch: 00 [ 1084/27767 (  4%)], Train Loss: 1.37985\n",
      "Epoch: 00 [ 1124/27767 (  4%)], Train Loss: 1.35313\n",
      "Epoch: 00 [ 1164/27767 (  4%)], Train Loss: 1.32786\n",
      "Epoch: 00 [ 1204/27767 (  4%)], Train Loss: 1.30130\n",
      "Epoch: 00 [ 1244/27767 (  4%)], Train Loss: 1.27364\n",
      "Epoch: 00 [ 1284/27767 (  5%)], Train Loss: 1.24849\n",
      "Epoch: 00 [ 1324/27767 (  5%)], Train Loss: 1.22213\n",
      "Epoch: 00 [ 1364/27767 (  5%)], Train Loss: 1.20561\n",
      "Epoch: 00 [ 1404/27767 (  5%)], Train Loss: 1.19006\n",
      "Epoch: 00 [ 1444/27767 (  5%)], Train Loss: 1.17096\n",
      "Epoch: 00 [ 1484/27767 (  5%)], Train Loss: 1.15370\n",
      "Epoch: 00 [ 1524/27767 (  5%)], Train Loss: 1.13453\n",
      "Epoch: 00 [ 1564/27767 (  6%)], Train Loss: 1.11715\n",
      "Epoch: 00 [ 1604/27767 (  6%)], Train Loss: 1.10014\n",
      "Epoch: 00 [ 1644/27767 (  6%)], Train Loss: 1.08683\n",
      "Epoch: 00 [ 1684/27767 (  6%)], Train Loss: 1.07492\n",
      "Epoch: 00 [ 1724/27767 (  6%)], Train Loss: 1.06146\n",
      "Epoch: 00 [ 1764/27767 (  6%)], Train Loss: 1.04687\n",
      "Epoch: 00 [ 1804/27767 (  6%)], Train Loss: 1.03695\n",
      "Epoch: 00 [ 1844/27767 (  7%)], Train Loss: 1.02981\n",
      "Epoch: 00 [ 1884/27767 (  7%)], Train Loss: 1.01848\n",
      "Epoch: 00 [ 1924/27767 (  7%)], Train Loss: 1.00688\n",
      "Epoch: 00 [ 1964/27767 (  7%)], Train Loss: 0.99551\n",
      "Epoch: 00 [ 2004/27767 (  7%)], Train Loss: 0.98550\n",
      "Epoch: 00 [ 2044/27767 (  7%)], Train Loss: 0.97416\n",
      "Epoch: 00 [ 2084/27767 (  8%)], Train Loss: 0.96665\n",
      "Epoch: 00 [ 2124/27767 (  8%)], Train Loss: 0.95890\n",
      "Epoch: 00 [ 2164/27767 (  8%)], Train Loss: 0.94983\n",
      "Epoch: 00 [ 2204/27767 (  8%)], Train Loss: 0.94081\n",
      "Epoch: 00 [ 2244/27767 (  8%)], Train Loss: 0.93434\n",
      "Epoch: 00 [ 2284/27767 (  8%)], Train Loss: 0.92585\n",
      "Epoch: 00 [ 2324/27767 (  8%)], Train Loss: 0.91755\n",
      "Epoch: 00 [ 2364/27767 (  9%)], Train Loss: 0.90751\n",
      "Epoch: 00 [ 2404/27767 (  9%)], Train Loss: 0.89993\n",
      "Epoch: 00 [ 2444/27767 (  9%)], Train Loss: 0.89389\n",
      "Epoch: 00 [ 2484/27767 (  9%)], Train Loss: 0.88841\n",
      "Epoch: 00 [ 2524/27767 (  9%)], Train Loss: 0.88357\n",
      "Epoch: 00 [ 2564/27767 (  9%)], Train Loss: 0.87535\n",
      "Epoch: 00 [ 2604/27767 (  9%)], Train Loss: 0.87055\n",
      "Epoch: 00 [ 2644/27767 ( 10%)], Train Loss: 0.86552\n",
      "Epoch: 00 [ 2684/27767 ( 10%)], Train Loss: 0.85849\n",
      "Epoch: 00 [ 2724/27767 ( 10%)], Train Loss: 0.85630\n",
      "Epoch: 00 [ 2764/27767 ( 10%)], Train Loss: 0.85327\n",
      "Epoch: 00 [ 2804/27767 ( 10%)], Train Loss: 0.84853\n",
      "Epoch: 00 [ 2844/27767 ( 10%)], Train Loss: 0.84116\n",
      "Epoch: 00 [ 2884/27767 ( 10%)], Train Loss: 0.83763\n",
      "Epoch: 00 [ 2924/27767 ( 11%)], Train Loss: 0.83274\n",
      "Epoch: 00 [ 2964/27767 ( 11%)], Train Loss: 0.83095\n",
      "Epoch: 00 [ 3004/27767 ( 11%)], Train Loss: 0.82578\n",
      "Epoch: 00 [ 3044/27767 ( 11%)], Train Loss: 0.81936\n",
      "Epoch: 00 [ 3084/27767 ( 11%)], Train Loss: 0.81479\n",
      "Epoch: 00 [ 3124/27767 ( 11%)], Train Loss: 0.81277\n",
      "Epoch: 00 [ 3164/27767 ( 11%)], Train Loss: 0.80663\n",
      "Epoch: 00 [ 3204/27767 ( 12%)], Train Loss: 0.80119\n",
      "Epoch: 00 [ 3244/27767 ( 12%)], Train Loss: 0.79809\n",
      "Epoch: 00 [ 3284/27767 ( 12%)], Train Loss: 0.79385\n",
      "Epoch: 00 [ 3324/27767 ( 12%)], Train Loss: 0.78806\n",
      "Epoch: 00 [ 3364/27767 ( 12%)], Train Loss: 0.78716\n",
      "Epoch: 00 [ 3404/27767 ( 12%)], Train Loss: 0.78449\n",
      "Epoch: 00 [ 3444/27767 ( 12%)], Train Loss: 0.78128\n",
      "Epoch: 00 [ 3484/27767 ( 13%)], Train Loss: 0.78035\n",
      "Epoch: 00 [ 3524/27767 ( 13%)], Train Loss: 0.77545\n",
      "Epoch: 00 [ 3564/27767 ( 13%)], Train Loss: 0.77091\n",
      "Epoch: 00 [ 3604/27767 ( 13%)], Train Loss: 0.76627\n",
      "Epoch: 00 [ 3644/27767 ( 13%)], Train Loss: 0.76324\n",
      "Epoch: 00 [ 3684/27767 ( 13%)], Train Loss: 0.76218\n",
      "Epoch: 00 [ 3724/27767 ( 13%)], Train Loss: 0.75787\n",
      "Epoch: 00 [ 3764/27767 ( 14%)], Train Loss: 0.75405\n",
      "Epoch: 00 [ 3804/27767 ( 14%)], Train Loss: 0.74953\n",
      "Epoch: 00 [ 3844/27767 ( 14%)], Train Loss: 0.74798\n",
      "Epoch: 00 [ 3884/27767 ( 14%)], Train Loss: 0.74462\n",
      "Epoch: 00 [ 3924/27767 ( 14%)], Train Loss: 0.74271\n",
      "Epoch: 00 [ 3964/27767 ( 14%)], Train Loss: 0.74047\n",
      "Epoch: 00 [ 4004/27767 ( 14%)], Train Loss: 0.73730\n",
      "Epoch: 00 [ 4044/27767 ( 15%)], Train Loss: 0.73534\n",
      "Epoch: 00 [ 4084/27767 ( 15%)], Train Loss: 0.73164\n",
      "Epoch: 00 [ 4124/27767 ( 15%)], Train Loss: 0.72637\n",
      "Epoch: 00 [ 4164/27767 ( 15%)], Train Loss: 0.72497\n",
      "Epoch: 00 [ 4204/27767 ( 15%)], Train Loss: 0.72384\n",
      "Epoch: 00 [ 4244/27767 ( 15%)], Train Loss: 0.72054\n",
      "Epoch: 00 [ 4284/27767 ( 15%)], Train Loss: 0.71830\n",
      "Epoch: 00 [ 4324/27767 ( 16%)], Train Loss: 0.71667\n",
      "Epoch: 00 [ 4364/27767 ( 16%)], Train Loss: 0.71462\n",
      "Epoch: 00 [ 4404/27767 ( 16%)], Train Loss: 0.71222\n",
      "Epoch: 00 [ 4444/27767 ( 16%)], Train Loss: 0.70999\n",
      "Epoch: 00 [ 4484/27767 ( 16%)], Train Loss: 0.70706\n",
      "Epoch: 00 [ 4524/27767 ( 16%)], Train Loss: 0.70378\n",
      "Epoch: 00 [ 4564/27767 ( 16%)], Train Loss: 0.70088\n",
      "Epoch: 00 [ 4604/27767 ( 17%)], Train Loss: 0.69869\n",
      "Epoch: 00 [ 4644/27767 ( 17%)], Train Loss: 0.69860\n",
      "Epoch: 00 [ 4684/27767 ( 17%)], Train Loss: 0.69618\n",
      "Epoch: 00 [ 4724/27767 ( 17%)], Train Loss: 0.69497\n",
      "Epoch: 00 [ 4764/27767 ( 17%)], Train Loss: 0.69278\n",
      "Epoch: 00 [ 4804/27767 ( 17%)], Train Loss: 0.69046\n",
      "Epoch: 00 [ 4844/27767 ( 17%)], Train Loss: 0.69011\n",
      "Epoch: 00 [ 4884/27767 ( 18%)], Train Loss: 0.68795\n",
      "Epoch: 00 [ 4924/27767 ( 18%)], Train Loss: 0.68669\n",
      "Epoch: 00 [ 4964/27767 ( 18%)], Train Loss: 0.68392\n",
      "Epoch: 00 [ 5004/27767 ( 18%)], Train Loss: 0.68132\n",
      "Epoch: 00 [ 5044/27767 ( 18%)], Train Loss: 0.67993\n",
      "Epoch: 00 [ 5084/27767 ( 18%)], Train Loss: 0.67869\n",
      "Epoch: 00 [ 5124/27767 ( 18%)], Train Loss: 0.67710\n",
      "Epoch: 00 [ 5164/27767 ( 19%)], Train Loss: 0.67438\n",
      "Epoch: 00 [ 5204/27767 ( 19%)], Train Loss: 0.67266\n",
      "Epoch: 00 [ 5244/27767 ( 19%)], Train Loss: 0.67083\n",
      "Epoch: 00 [ 5284/27767 ( 19%)], Train Loss: 0.66952\n",
      "Epoch: 00 [ 5324/27767 ( 19%)], Train Loss: 0.66687\n",
      "Epoch: 00 [ 5364/27767 ( 19%)], Train Loss: 0.66561\n",
      "Epoch: 00 [ 5404/27767 ( 19%)], Train Loss: 0.66446\n",
      "Epoch: 00 [ 5444/27767 ( 20%)], Train Loss: 0.66139\n",
      "Epoch: 00 [ 5484/27767 ( 20%)], Train Loss: 0.65980\n",
      "Epoch: 00 [ 5524/27767 ( 20%)], Train Loss: 0.65743\n",
      "Epoch: 00 [ 5564/27767 ( 20%)], Train Loss: 0.65555\n",
      "Epoch: 00 [ 5604/27767 ( 20%)], Train Loss: 0.65311\n",
      "Epoch: 00 [ 5644/27767 ( 20%)], Train Loss: 0.65101\n",
      "Epoch: 00 [ 5684/27767 ( 20%)], Train Loss: 0.64943\n",
      "Epoch: 00 [ 5724/27767 ( 21%)], Train Loss: 0.64713\n",
      "Epoch: 00 [ 5764/27767 ( 21%)], Train Loss: 0.64677\n",
      "Epoch: 00 [ 5804/27767 ( 21%)], Train Loss: 0.64453\n",
      "Epoch: 00 [ 5844/27767 ( 21%)], Train Loss: 0.64290\n",
      "Epoch: 00 [ 5884/27767 ( 21%)], Train Loss: 0.64167\n",
      "Epoch: 00 [ 5924/27767 ( 21%)], Train Loss: 0.64004\n",
      "Epoch: 00 [ 5964/27767 ( 21%)], Train Loss: 0.63957\n",
      "Epoch: 00 [ 6004/27767 ( 22%)], Train Loss: 0.63804\n",
      "Epoch: 00 [ 6044/27767 ( 22%)], Train Loss: 0.63736\n",
      "Epoch: 00 [ 6084/27767 ( 22%)], Train Loss: 0.63639\n",
      "Epoch: 00 [ 6124/27767 ( 22%)], Train Loss: 0.63520\n",
      "Epoch: 00 [ 6164/27767 ( 22%)], Train Loss: 0.63299\n",
      "Epoch: 00 [ 6204/27767 ( 22%)], Train Loss: 0.63058\n",
      "Epoch: 00 [ 6244/27767 ( 22%)], Train Loss: 0.62902\n",
      "Epoch: 00 [ 6284/27767 ( 23%)], Train Loss: 0.62795\n",
      "Epoch: 00 [ 6324/27767 ( 23%)], Train Loss: 0.62636\n",
      "Epoch: 00 [ 6364/27767 ( 23%)], Train Loss: 0.62426\n",
      "Epoch: 00 [ 6404/27767 ( 23%)], Train Loss: 0.62356\n",
      "Epoch: 00 [ 6444/27767 ( 23%)], Train Loss: 0.62248\n",
      "Epoch: 00 [ 6484/27767 ( 23%)], Train Loss: 0.62083\n",
      "Epoch: 00 [ 6524/27767 ( 23%)], Train Loss: 0.61979\n",
      "Epoch: 00 [ 6564/27767 ( 24%)], Train Loss: 0.61871\n",
      "Epoch: 00 [ 6604/27767 ( 24%)], Train Loss: 0.61729\n",
      "Epoch: 00 [ 6644/27767 ( 24%)], Train Loss: 0.61542\n",
      "Epoch: 00 [ 6684/27767 ( 24%)], Train Loss: 0.61426\n",
      "Epoch: 00 [ 6724/27767 ( 24%)], Train Loss: 0.61404\n",
      "Epoch: 00 [ 6764/27767 ( 24%)], Train Loss: 0.61314\n",
      "Epoch: 00 [ 6804/27767 ( 25%)], Train Loss: 0.61196\n",
      "Epoch: 00 [ 6844/27767 ( 25%)], Train Loss: 0.61072\n",
      "Epoch: 00 [ 6884/27767 ( 25%)], Train Loss: 0.60983\n",
      "Epoch: 00 [ 6924/27767 ( 25%)], Train Loss: 0.60817\n",
      "Epoch: 00 [ 6964/27767 ( 25%)], Train Loss: 0.60683\n",
      "Epoch: 00 [ 7004/27767 ( 25%)], Train Loss: 0.60518\n",
      "Epoch: 00 [ 7044/27767 ( 25%)], Train Loss: 0.60552\n",
      "Epoch: 00 [ 7084/27767 ( 26%)], Train Loss: 0.60470\n",
      "Epoch: 00 [ 7124/27767 ( 26%)], Train Loss: 0.60382\n",
      "Epoch: 00 [ 7164/27767 ( 26%)], Train Loss: 0.60292\n",
      "Epoch: 00 [ 7204/27767 ( 26%)], Train Loss: 0.60176\n",
      "Epoch: 00 [ 7244/27767 ( 26%)], Train Loss: 0.60113\n",
      "Epoch: 00 [ 7284/27767 ( 26%)], Train Loss: 0.60021\n",
      "Epoch: 00 [ 7324/27767 ( 26%)], Train Loss: 0.59921\n",
      "Epoch: 00 [ 7364/27767 ( 27%)], Train Loss: 0.59904\n",
      "Epoch: 00 [ 7404/27767 ( 27%)], Train Loss: 0.59799\n",
      "Epoch: 00 [ 7444/27767 ( 27%)], Train Loss: 0.59616\n",
      "Epoch: 00 [ 7484/27767 ( 27%)], Train Loss: 0.59530\n",
      "Epoch: 00 [ 7524/27767 ( 27%)], Train Loss: 0.59407\n",
      "Epoch: 00 [ 7564/27767 ( 27%)], Train Loss: 0.59283\n",
      "Epoch: 00 [ 7604/27767 ( 27%)], Train Loss: 0.59218\n",
      "Epoch: 00 [ 7644/27767 ( 28%)], Train Loss: 0.59219\n",
      "Epoch: 00 [ 7684/27767 ( 28%)], Train Loss: 0.59042\n",
      "Epoch: 00 [ 7724/27767 ( 28%)], Train Loss: 0.58854\n",
      "Epoch: 00 [ 7764/27767 ( 28%)], Train Loss: 0.58760\n",
      "Epoch: 00 [ 7804/27767 ( 28%)], Train Loss: 0.58622\n",
      "Epoch: 00 [ 7844/27767 ( 28%)], Train Loss: 0.58476\n",
      "Epoch: 00 [ 7884/27767 ( 28%)], Train Loss: 0.58328\n",
      "Epoch: 00 [ 7924/27767 ( 29%)], Train Loss: 0.58206\n",
      "Epoch: 00 [ 7964/27767 ( 29%)], Train Loss: 0.58037\n",
      "Epoch: 00 [ 8004/27767 ( 29%)], Train Loss: 0.57965\n",
      "Epoch: 00 [ 8044/27767 ( 29%)], Train Loss: 0.57995\n",
      "Epoch: 00 [ 8084/27767 ( 29%)], Train Loss: 0.57863\n",
      "Epoch: 00 [ 8124/27767 ( 29%)], Train Loss: 0.57772\n",
      "Epoch: 00 [ 8164/27767 ( 29%)], Train Loss: 0.57709\n",
      "Epoch: 00 [ 8204/27767 ( 30%)], Train Loss: 0.57595\n",
      "Epoch: 00 [ 8244/27767 ( 30%)], Train Loss: 0.57485\n",
      "Epoch: 00 [ 8284/27767 ( 30%)], Train Loss: 0.57397\n",
      "Epoch: 00 [ 8324/27767 ( 30%)], Train Loss: 0.57255\n",
      "Epoch: 00 [ 8364/27767 ( 30%)], Train Loss: 0.57148\n",
      "Epoch: 00 [ 8404/27767 ( 30%)], Train Loss: 0.57138\n",
      "Epoch: 00 [ 8444/27767 ( 30%)], Train Loss: 0.57051\n",
      "Epoch: 00 [ 8484/27767 ( 31%)], Train Loss: 0.57035\n",
      "Epoch: 00 [ 8524/27767 ( 31%)], Train Loss: 0.57058\n",
      "Epoch: 00 [ 8564/27767 ( 31%)], Train Loss: 0.57023\n",
      "Epoch: 00 [ 8604/27767 ( 31%)], Train Loss: 0.56998\n",
      "Epoch: 00 [ 8644/27767 ( 31%)], Train Loss: 0.56954\n",
      "Epoch: 00 [ 8684/27767 ( 31%)], Train Loss: 0.56896\n",
      "Epoch: 00 [ 8724/27767 ( 31%)], Train Loss: 0.56844\n",
      "Epoch: 00 [ 8764/27767 ( 32%)], Train Loss: 0.56755\n",
      "Epoch: 00 [ 8804/27767 ( 32%)], Train Loss: 0.56611\n",
      "Epoch: 00 [ 8844/27767 ( 32%)], Train Loss: 0.56535\n",
      "Epoch: 00 [ 8884/27767 ( 32%)], Train Loss: 0.56473\n",
      "Epoch: 00 [ 8924/27767 ( 32%)], Train Loss: 0.56453\n",
      "Epoch: 00 [ 8964/27767 ( 32%)], Train Loss: 0.56365\n",
      "Epoch: 00 [ 9004/27767 ( 32%)], Train Loss: 0.56299\n",
      "Epoch: 00 [ 9044/27767 ( 33%)], Train Loss: 0.56193\n",
      "Epoch: 00 [ 9084/27767 ( 33%)], Train Loss: 0.56168\n",
      "Epoch: 00 [ 9124/27767 ( 33%)], Train Loss: 0.56219\n",
      "Epoch: 00 [ 9164/27767 ( 33%)], Train Loss: 0.56153\n",
      "Epoch: 00 [ 9204/27767 ( 33%)], Train Loss: 0.56103\n",
      "Epoch: 00 [ 9244/27767 ( 33%)], Train Loss: 0.56011\n",
      "Epoch: 00 [ 9284/27767 ( 33%)], Train Loss: 0.55924\n",
      "Epoch: 00 [ 9324/27767 ( 34%)], Train Loss: 0.55806\n",
      "Epoch: 00 [ 9364/27767 ( 34%)], Train Loss: 0.55767\n",
      "Epoch: 00 [ 9404/27767 ( 34%)], Train Loss: 0.55654\n",
      "Epoch: 00 [ 9444/27767 ( 34%)], Train Loss: 0.55613\n",
      "Epoch: 00 [ 9484/27767 ( 34%)], Train Loss: 0.55633\n",
      "Epoch: 00 [ 9524/27767 ( 34%)], Train Loss: 0.55580\n",
      "Epoch: 00 [ 9564/27767 ( 34%)], Train Loss: 0.55543\n",
      "Epoch: 00 [ 9604/27767 ( 35%)], Train Loss: 0.55402\n",
      "Epoch: 00 [ 9644/27767 ( 35%)], Train Loss: 0.55285\n",
      "Epoch: 00 [ 9684/27767 ( 35%)], Train Loss: 0.55207\n",
      "Epoch: 00 [ 9724/27767 ( 35%)], Train Loss: 0.55142\n",
      "Epoch: 00 [ 9764/27767 ( 35%)], Train Loss: 0.55029\n",
      "Epoch: 00 [ 9804/27767 ( 35%)], Train Loss: 0.54969\n",
      "Epoch: 00 [ 9844/27767 ( 35%)], Train Loss: 0.54927\n",
      "Epoch: 00 [ 9884/27767 ( 36%)], Train Loss: 0.54838\n",
      "Epoch: 00 [ 9924/27767 ( 36%)], Train Loss: 0.54757\n",
      "Epoch: 00 [ 9964/27767 ( 36%)], Train Loss: 0.54620\n",
      "Epoch: 00 [10004/27767 ( 36%)], Train Loss: 0.54542\n",
      "Epoch: 00 [10044/27767 ( 36%)], Train Loss: 0.54571\n",
      "Epoch: 00 [10084/27767 ( 36%)], Train Loss: 0.54459\n",
      "Epoch: 00 [10124/27767 ( 36%)], Train Loss: 0.54404\n",
      "Epoch: 00 [10164/27767 ( 37%)], Train Loss: 0.54327\n",
      "Epoch: 00 [10204/27767 ( 37%)], Train Loss: 0.54225\n",
      "Epoch: 00 [10244/27767 ( 37%)], Train Loss: 0.54151\n",
      "Epoch: 00 [10284/27767 ( 37%)], Train Loss: 0.54096\n",
      "Epoch: 00 [10324/27767 ( 37%)], Train Loss: 0.54033\n",
      "Epoch: 00 [10364/27767 ( 37%)], Train Loss: 0.53916\n",
      "Epoch: 00 [10404/27767 ( 37%)], Train Loss: 0.53787\n",
      "Epoch: 00 [10444/27767 ( 38%)], Train Loss: 0.53700\n",
      "Epoch: 00 [10484/27767 ( 38%)], Train Loss: 0.53596\n",
      "Epoch: 00 [10524/27767 ( 38%)], Train Loss: 0.53479\n",
      "Epoch: 00 [10564/27767 ( 38%)], Train Loss: 0.53383\n",
      "Epoch: 00 [10604/27767 ( 38%)], Train Loss: 0.53373\n",
      "Epoch: 00 [10644/27767 ( 38%)], Train Loss: 0.53289\n",
      "Epoch: 00 [10684/27767 ( 38%)], Train Loss: 0.53247\n",
      "Epoch: 00 [10724/27767 ( 39%)], Train Loss: 0.53256\n",
      "Epoch: 00 [10764/27767 ( 39%)], Train Loss: 0.53196\n",
      "Epoch: 00 [10804/27767 ( 39%)], Train Loss: 0.53152\n",
      "Epoch: 00 [10844/27767 ( 39%)], Train Loss: 0.53114\n",
      "Epoch: 00 [10884/27767 ( 39%)], Train Loss: 0.53054\n",
      "Epoch: 00 [10924/27767 ( 39%)], Train Loss: 0.53035\n",
      "Epoch: 00 [10964/27767 ( 39%)], Train Loss: 0.52983\n",
      "Epoch: 00 [11004/27767 ( 40%)], Train Loss: 0.52924\n",
      "Epoch: 00 [11044/27767 ( 40%)], Train Loss: 0.52906\n",
      "Epoch: 00 [11084/27767 ( 40%)], Train Loss: 0.52860\n",
      "Epoch: 00 [11124/27767 ( 40%)], Train Loss: 0.52784\n",
      "Epoch: 00 [11164/27767 ( 40%)], Train Loss: 0.52691\n",
      "Epoch: 00 [11204/27767 ( 40%)], Train Loss: 0.52669\n",
      "Epoch: 00 [11244/27767 ( 40%)], Train Loss: 0.52671\n",
      "Epoch: 00 [11284/27767 ( 41%)], Train Loss: 0.52647\n",
      "Epoch: 00 [11324/27767 ( 41%)], Train Loss: 0.52631\n",
      "Epoch: 00 [11364/27767 ( 41%)], Train Loss: 0.52603\n",
      "Epoch: 00 [11404/27767 ( 41%)], Train Loss: 0.52523\n",
      "Epoch: 00 [11444/27767 ( 41%)], Train Loss: 0.52459\n",
      "Epoch: 00 [11484/27767 ( 41%)], Train Loss: 0.52375\n",
      "Epoch: 00 [11524/27767 ( 42%)], Train Loss: 0.52352\n",
      "Epoch: 00 [11564/27767 ( 42%)], Train Loss: 0.52311\n",
      "Epoch: 00 [11604/27767 ( 42%)], Train Loss: 0.52213\n",
      "Epoch: 00 [11644/27767 ( 42%)], Train Loss: 0.52108\n",
      "Epoch: 00 [11684/27767 ( 42%)], Train Loss: 0.52044\n",
      "Epoch: 00 [11724/27767 ( 42%)], Train Loss: 0.51982\n",
      "Epoch: 00 [11764/27767 ( 42%)], Train Loss: 0.51961\n",
      "Epoch: 00 [11804/27767 ( 43%)], Train Loss: 0.52005\n",
      "Epoch: 00 [11844/27767 ( 43%)], Train Loss: 0.51918\n",
      "Epoch: 00 [11884/27767 ( 43%)], Train Loss: 0.51929\n",
      "Epoch: 00 [11924/27767 ( 43%)], Train Loss: 0.51917\n",
      "Epoch: 00 [11964/27767 ( 43%)], Train Loss: 0.51824\n",
      "Epoch: 00 [12004/27767 ( 43%)], Train Loss: 0.51744\n",
      "Epoch: 00 [12044/27767 ( 43%)], Train Loss: 0.51720\n",
      "Epoch: 00 [12084/27767 ( 44%)], Train Loss: 0.51700\n",
      "Epoch: 00 [12124/27767 ( 44%)], Train Loss: 0.51631\n",
      "Epoch: 00 [12164/27767 ( 44%)], Train Loss: 0.51583\n",
      "Epoch: 00 [12204/27767 ( 44%)], Train Loss: 0.51545\n",
      "Epoch: 00 [12244/27767 ( 44%)], Train Loss: 0.51478\n",
      "Epoch: 00 [12284/27767 ( 44%)], Train Loss: 0.51405\n",
      "Epoch: 00 [12324/27767 ( 44%)], Train Loss: 0.51347\n",
      "Epoch: 00 [12364/27767 ( 45%)], Train Loss: 0.51238\n",
      "Epoch: 00 [12404/27767 ( 45%)], Train Loss: 0.51177\n",
      "Epoch: 00 [12444/27767 ( 45%)], Train Loss: 0.51134\n",
      "Epoch: 00 [12484/27767 ( 45%)], Train Loss: 0.51058\n",
      "Epoch: 00 [12524/27767 ( 45%)], Train Loss: 0.51024\n",
      "Epoch: 00 [12564/27767 ( 45%)], Train Loss: 0.50954\n",
      "Epoch: 00 [12604/27767 ( 45%)], Train Loss: 0.50880\n",
      "Epoch: 00 [12644/27767 ( 46%)], Train Loss: 0.50864\n",
      "Epoch: 00 [12684/27767 ( 46%)], Train Loss: 0.50829\n",
      "Epoch: 00 [12724/27767 ( 46%)], Train Loss: 0.50774\n",
      "Epoch: 00 [12764/27767 ( 46%)], Train Loss: 0.50768\n",
      "Epoch: 00 [12804/27767 ( 46%)], Train Loss: 0.50718\n",
      "Epoch: 00 [12844/27767 ( 46%)], Train Loss: 0.50642\n",
      "Epoch: 00 [12884/27767 ( 46%)], Train Loss: 0.50677\n",
      "Epoch: 00 [12924/27767 ( 47%)], Train Loss: 0.50614\n",
      "Epoch: 00 [12964/27767 ( 47%)], Train Loss: 0.50597\n",
      "Epoch: 00 [13004/27767 ( 47%)], Train Loss: 0.50560\n",
      "Epoch: 00 [13044/27767 ( 47%)], Train Loss: 0.50513\n",
      "Epoch: 00 [13084/27767 ( 47%)], Train Loss: 0.50496\n",
      "Epoch: 00 [13124/27767 ( 47%)], Train Loss: 0.50427\n",
      "Epoch: 00 [13164/27767 ( 47%)], Train Loss: 0.50384\n",
      "Epoch: 00 [13204/27767 ( 48%)], Train Loss: 0.50388\n",
      "Epoch: 00 [13244/27767 ( 48%)], Train Loss: 0.50363\n",
      "Epoch: 00 [13284/27767 ( 48%)], Train Loss: 0.50347\n",
      "Epoch: 00 [13324/27767 ( 48%)], Train Loss: 0.50352\n",
      "Epoch: 00 [13364/27767 ( 48%)], Train Loss: 0.50341\n",
      "Epoch: 00 [13404/27767 ( 48%)], Train Loss: 0.50292\n",
      "Epoch: 00 [13444/27767 ( 48%)], Train Loss: 0.50262\n",
      "Epoch: 00 [13484/27767 ( 49%)], Train Loss: 0.50218\n",
      "Epoch: 00 [13524/27767 ( 49%)], Train Loss: 0.50150\n",
      "Epoch: 00 [13564/27767 ( 49%)], Train Loss: 0.50115\n",
      "Epoch: 00 [13604/27767 ( 49%)], Train Loss: 0.50094\n",
      "Epoch: 00 [13644/27767 ( 49%)], Train Loss: 0.50019\n",
      "Epoch: 00 [13684/27767 ( 49%)], Train Loss: 0.50012\n",
      "Epoch: 00 [13724/27767 ( 49%)], Train Loss: 0.49988\n",
      "Epoch: 00 [13764/27767 ( 50%)], Train Loss: 0.49894\n",
      "Epoch: 00 [13804/27767 ( 50%)], Train Loss: 0.49822\n",
      "Epoch: 00 [13844/27767 ( 50%)], Train Loss: 0.49783\n",
      "Epoch: 00 [13884/27767 ( 50%)], Train Loss: 0.49744\n",
      "Epoch: 00 [13924/27767 ( 50%)], Train Loss: 0.49719\n",
      "Epoch: 00 [13964/27767 ( 50%)], Train Loss: 0.49640\n",
      "Epoch: 00 [14004/27767 ( 50%)], Train Loss: 0.49607\n",
      "Epoch: 00 [14044/27767 ( 51%)], Train Loss: 0.49577\n",
      "Epoch: 00 [14084/27767 ( 51%)], Train Loss: 0.49543\n",
      "Epoch: 00 [14124/27767 ( 51%)], Train Loss: 0.49479\n",
      "Epoch: 00 [14164/27767 ( 51%)], Train Loss: 0.49458\n",
      "Epoch: 00 [14204/27767 ( 51%)], Train Loss: 0.49424\n",
      "Epoch: 00 [14244/27767 ( 51%)], Train Loss: 0.49382\n",
      "Epoch: 00 [14284/27767 ( 51%)], Train Loss: 0.49342\n",
      "Epoch: 00 [14324/27767 ( 52%)], Train Loss: 0.49266\n",
      "Epoch: 00 [14364/27767 ( 52%)], Train Loss: 0.49212\n",
      "Epoch: 00 [14404/27767 ( 52%)], Train Loss: 0.49164\n",
      "Epoch: 00 [14444/27767 ( 52%)], Train Loss: 0.49161\n",
      "Epoch: 00 [14484/27767 ( 52%)], Train Loss: 0.49101\n",
      "Epoch: 00 [14524/27767 ( 52%)], Train Loss: 0.49044\n",
      "Epoch: 00 [14564/27767 ( 52%)], Train Loss: 0.49024\n",
      "Epoch: 00 [14604/27767 ( 53%)], Train Loss: 0.48991\n",
      "Epoch: 00 [14644/27767 ( 53%)], Train Loss: 0.48994\n",
      "Epoch: 00 [14684/27767 ( 53%)], Train Loss: 0.48940\n",
      "Epoch: 00 [14724/27767 ( 53%)], Train Loss: 0.48879\n",
      "Epoch: 00 [14764/27767 ( 53%)], Train Loss: 0.48856\n",
      "Epoch: 00 [14804/27767 ( 53%)], Train Loss: 0.48849\n",
      "Epoch: 00 [14844/27767 ( 53%)], Train Loss: 0.48812\n",
      "Epoch: 00 [14884/27767 ( 54%)], Train Loss: 0.48781\n",
      "Epoch: 00 [14924/27767 ( 54%)], Train Loss: 0.48757\n",
      "Epoch: 00 [14964/27767 ( 54%)], Train Loss: 0.48676\n",
      "Epoch: 00 [15004/27767 ( 54%)], Train Loss: 0.48611\n",
      "Epoch: 00 [15044/27767 ( 54%)], Train Loss: 0.48542\n",
      "Epoch: 00 [15084/27767 ( 54%)], Train Loss: 0.48501\n",
      "Epoch: 00 [15124/27767 ( 54%)], Train Loss: 0.48450\n",
      "Epoch: 00 [15164/27767 ( 55%)], Train Loss: 0.48404\n",
      "Epoch: 00 [15204/27767 ( 55%)], Train Loss: 0.48349\n",
      "Epoch: 00 [15244/27767 ( 55%)], Train Loss: 0.48296\n",
      "Epoch: 00 [15284/27767 ( 55%)], Train Loss: 0.48237\n",
      "Epoch: 00 [15324/27767 ( 55%)], Train Loss: 0.48172\n",
      "Epoch: 00 [15364/27767 ( 55%)], Train Loss: 0.48150\n",
      "Epoch: 00 [15404/27767 ( 55%)], Train Loss: 0.48206\n",
      "Epoch: 00 [15444/27767 ( 56%)], Train Loss: 0.48159\n",
      "Epoch: 00 [15484/27767 ( 56%)], Train Loss: 0.48140\n",
      "Epoch: 00 [15524/27767 ( 56%)], Train Loss: 0.48063\n",
      "Epoch: 00 [15564/27767 ( 56%)], Train Loss: 0.48016\n",
      "Epoch: 00 [15604/27767 ( 56%)], Train Loss: 0.47961\n",
      "Epoch: 00 [15644/27767 ( 56%)], Train Loss: 0.47931\n",
      "Epoch: 00 [15684/27767 ( 56%)], Train Loss: 0.47893\n",
      "Epoch: 00 [15724/27767 ( 57%)], Train Loss: 0.47856\n",
      "Epoch: 00 [15764/27767 ( 57%)], Train Loss: 0.47791\n",
      "Epoch: 00 [15804/27767 ( 57%)], Train Loss: 0.47773\n",
      "Epoch: 00 [15844/27767 ( 57%)], Train Loss: 0.47698\n",
      "Epoch: 00 [15884/27767 ( 57%)], Train Loss: 0.47662\n",
      "Epoch: 00 [15924/27767 ( 57%)], Train Loss: 0.47593\n",
      "Epoch: 00 [15964/27767 ( 57%)], Train Loss: 0.47575\n",
      "Epoch: 00 [16004/27767 ( 58%)], Train Loss: 0.47574\n",
      "Epoch: 00 [16044/27767 ( 58%)], Train Loss: 0.47551\n",
      "Epoch: 00 [16084/27767 ( 58%)], Train Loss: 0.47522\n",
      "Epoch: 00 [16124/27767 ( 58%)], Train Loss: 0.47475\n",
      "Epoch: 00 [16164/27767 ( 58%)], Train Loss: 0.47424\n",
      "Epoch: 00 [16204/27767 ( 58%)], Train Loss: 0.47364\n",
      "Epoch: 00 [16244/27767 ( 59%)], Train Loss: 0.47321\n",
      "Epoch: 00 [16284/27767 ( 59%)], Train Loss: 0.47255\n",
      "Epoch: 00 [16324/27767 ( 59%)], Train Loss: 0.47196\n",
      "Epoch: 00 [16364/27767 ( 59%)], Train Loss: 0.47158\n",
      "Epoch: 00 [16404/27767 ( 59%)], Train Loss: 0.47135\n",
      "Epoch: 00 [16444/27767 ( 59%)], Train Loss: 0.47097\n",
      "Epoch: 00 [16484/27767 ( 59%)], Train Loss: 0.47045\n",
      "Epoch: 00 [16524/27767 ( 60%)], Train Loss: 0.47036\n",
      "Epoch: 00 [16564/27767 ( 60%)], Train Loss: 0.47018\n",
      "Epoch: 00 [16604/27767 ( 60%)], Train Loss: 0.46989\n",
      "Epoch: 00 [16644/27767 ( 60%)], Train Loss: 0.46994\n",
      "Epoch: 00 [16684/27767 ( 60%)], Train Loss: 0.46933\n",
      "Epoch: 00 [16724/27767 ( 60%)], Train Loss: 0.46890\n",
      "Epoch: 00 [16764/27767 ( 60%)], Train Loss: 0.46848\n",
      "Epoch: 00 [16804/27767 ( 61%)], Train Loss: 0.46797\n",
      "Epoch: 00 [16844/27767 ( 61%)], Train Loss: 0.46759\n",
      "Epoch: 00 [16884/27767 ( 61%)], Train Loss: 0.46713\n",
      "Epoch: 00 [16924/27767 ( 61%)], Train Loss: 0.46669\n",
      "Epoch: 00 [16964/27767 ( 61%)], Train Loss: 0.46629\n",
      "Epoch: 00 [17004/27767 ( 61%)], Train Loss: 0.46586\n",
      "Epoch: 00 [17044/27767 ( 61%)], Train Loss: 0.46515\n",
      "Epoch: 00 [17084/27767 ( 62%)], Train Loss: 0.46456\n",
      "Epoch: 00 [17124/27767 ( 62%)], Train Loss: 0.46429\n",
      "Epoch: 00 [17164/27767 ( 62%)], Train Loss: 0.46405\n",
      "Epoch: 00 [17204/27767 ( 62%)], Train Loss: 0.46372\n",
      "Epoch: 00 [17244/27767 ( 62%)], Train Loss: 0.46331\n",
      "Epoch: 00 [17284/27767 ( 62%)], Train Loss: 0.46273\n",
      "Epoch: 00 [17324/27767 ( 62%)], Train Loss: 0.46255\n",
      "Epoch: 00 [17364/27767 ( 63%)], Train Loss: 0.46270\n",
      "Epoch: 00 [17404/27767 ( 63%)], Train Loss: 0.46245\n",
      "Epoch: 00 [17444/27767 ( 63%)], Train Loss: 0.46194\n",
      "Epoch: 00 [17484/27767 ( 63%)], Train Loss: 0.46164\n",
      "Epoch: 00 [17524/27767 ( 63%)], Train Loss: 0.46145\n",
      "Epoch: 00 [17564/27767 ( 63%)], Train Loss: 0.46180\n",
      "Epoch: 00 [17604/27767 ( 63%)], Train Loss: 0.46149\n",
      "Epoch: 00 [17644/27767 ( 64%)], Train Loss: 0.46072\n",
      "Epoch: 00 [17684/27767 ( 64%)], Train Loss: 0.46019\n",
      "Epoch: 00 [17724/27767 ( 64%)], Train Loss: 0.45951\n",
      "Epoch: 00 [17764/27767 ( 64%)], Train Loss: 0.45924\n",
      "Epoch: 00 [17804/27767 ( 64%)], Train Loss: 0.45918\n",
      "Epoch: 00 [17844/27767 ( 64%)], Train Loss: 0.45928\n",
      "Epoch: 00 [17884/27767 ( 64%)], Train Loss: 0.45887\n",
      "Epoch: 00 [17924/27767 ( 65%)], Train Loss: 0.45841\n",
      "Epoch: 00 [17964/27767 ( 65%)], Train Loss: 0.45795\n",
      "Epoch: 00 [18004/27767 ( 65%)], Train Loss: 0.45754\n",
      "Epoch: 00 [18044/27767 ( 65%)], Train Loss: 0.45731\n",
      "Epoch: 00 [18084/27767 ( 65%)], Train Loss: 0.45704\n",
      "Epoch: 00 [18124/27767 ( 65%)], Train Loss: 0.45690\n",
      "Epoch: 00 [18164/27767 ( 65%)], Train Loss: 0.45679\n",
      "Epoch: 00 [18204/27767 ( 66%)], Train Loss: 0.45650\n",
      "Epoch: 00 [18244/27767 ( 66%)], Train Loss: 0.45617\n",
      "Epoch: 00 [18284/27767 ( 66%)], Train Loss: 0.45624\n",
      "Epoch: 00 [18324/27767 ( 66%)], Train Loss: 0.45594\n",
      "Epoch: 00 [18364/27767 ( 66%)], Train Loss: 0.45554\n",
      "Epoch: 00 [18404/27767 ( 66%)], Train Loss: 0.45541\n",
      "Epoch: 00 [18444/27767 ( 66%)], Train Loss: 0.45515\n",
      "Epoch: 00 [18484/27767 ( 67%)], Train Loss: 0.45502\n",
      "Epoch: 00 [18524/27767 ( 67%)], Train Loss: 0.45476\n",
      "Epoch: 00 [18564/27767 ( 67%)], Train Loss: 0.45470\n",
      "Epoch: 00 [18604/27767 ( 67%)], Train Loss: 0.45517\n",
      "Epoch: 00 [18644/27767 ( 67%)], Train Loss: 0.45485\n",
      "Epoch: 00 [18684/27767 ( 67%)], Train Loss: 0.45428\n",
      "Epoch: 00 [18724/27767 ( 67%)], Train Loss: 0.45382\n",
      "Epoch: 00 [18764/27767 ( 68%)], Train Loss: 0.45345\n",
      "Epoch: 00 [18804/27767 ( 68%)], Train Loss: 0.45307\n",
      "Epoch: 00 [18844/27767 ( 68%)], Train Loss: 0.45273\n",
      "Epoch: 00 [18884/27767 ( 68%)], Train Loss: 0.45297\n",
      "Epoch: 00 [18924/27767 ( 68%)], Train Loss: 0.45313\n",
      "Epoch: 00 [18964/27767 ( 68%)], Train Loss: 0.45291\n",
      "Epoch: 00 [19004/27767 ( 68%)], Train Loss: 0.45275\n",
      "Epoch: 00 [19044/27767 ( 69%)], Train Loss: 0.45211\n",
      "Epoch: 00 [19084/27767 ( 69%)], Train Loss: 0.45187\n",
      "Epoch: 00 [19124/27767 ( 69%)], Train Loss: 0.45138\n",
      "Epoch: 00 [19164/27767 ( 69%)], Train Loss: 0.45135\n",
      "Epoch: 00 [19204/27767 ( 69%)], Train Loss: 0.45090\n",
      "Epoch: 00 [19244/27767 ( 69%)], Train Loss: 0.45067\n",
      "Epoch: 00 [19284/27767 ( 69%)], Train Loss: 0.45039\n",
      "Epoch: 00 [19324/27767 ( 70%)], Train Loss: 0.45008\n",
      "Epoch: 00 [19364/27767 ( 70%)], Train Loss: 0.45030\n",
      "Epoch: 00 [19404/27767 ( 70%)], Train Loss: 0.44976\n",
      "Epoch: 00 [19444/27767 ( 70%)], Train Loss: 0.44929\n",
      "Epoch: 00 [19484/27767 ( 70%)], Train Loss: 0.44922\n",
      "Epoch: 00 [19524/27767 ( 70%)], Train Loss: 0.44893\n",
      "Epoch: 00 [19564/27767 ( 70%)], Train Loss: 0.44862\n",
      "Epoch: 00 [19604/27767 ( 71%)], Train Loss: 0.44807\n",
      "Epoch: 00 [19644/27767 ( 71%)], Train Loss: 0.44773\n",
      "Epoch: 00 [19684/27767 ( 71%)], Train Loss: 0.44731\n",
      "Epoch: 00 [19724/27767 ( 71%)], Train Loss: 0.44704\n",
      "Epoch: 00 [19764/27767 ( 71%)], Train Loss: 0.44704\n",
      "Epoch: 00 [19804/27767 ( 71%)], Train Loss: 0.44668\n",
      "Epoch: 00 [19844/27767 ( 71%)], Train Loss: 0.44622\n",
      "Epoch: 00 [19884/27767 ( 72%)], Train Loss: 0.44615\n",
      "Epoch: 00 [19924/27767 ( 72%)], Train Loss: 0.44585\n",
      "Epoch: 00 [19964/27767 ( 72%)], Train Loss: 0.44582\n",
      "Epoch: 00 [20004/27767 ( 72%)], Train Loss: 0.44572\n",
      "Epoch: 00 [20044/27767 ( 72%)], Train Loss: 0.44621\n",
      "Epoch: 00 [20084/27767 ( 72%)], Train Loss: 0.44628\n",
      "Epoch: 00 [20124/27767 ( 72%)], Train Loss: 0.44609\n",
      "Epoch: 00 [20164/27767 ( 73%)], Train Loss: 0.44575\n",
      "Epoch: 00 [20204/27767 ( 73%)], Train Loss: 0.44551\n",
      "Epoch: 00 [20244/27767 ( 73%)], Train Loss: 0.44489\n",
      "Epoch: 00 [20284/27767 ( 73%)], Train Loss: 0.44471\n",
      "Epoch: 00 [20324/27767 ( 73%)], Train Loss: 0.44457\n",
      "Epoch: 00 [20364/27767 ( 73%)], Train Loss: 0.44404\n",
      "Epoch: 00 [20404/27767 ( 73%)], Train Loss: 0.44369\n",
      "Epoch: 00 [20444/27767 ( 74%)], Train Loss: 0.44328\n",
      "Epoch: 00 [20484/27767 ( 74%)], Train Loss: 0.44305\n",
      "Epoch: 00 [20524/27767 ( 74%)], Train Loss: 0.44279\n",
      "Epoch: 00 [20564/27767 ( 74%)], Train Loss: 0.44255\n",
      "Epoch: 00 [20604/27767 ( 74%)], Train Loss: 0.44213\n",
      "Epoch: 00 [20644/27767 ( 74%)], Train Loss: 0.44214\n",
      "Epoch: 00 [20684/27767 ( 74%)], Train Loss: 0.44197\n",
      "Epoch: 00 [20724/27767 ( 75%)], Train Loss: 0.44187\n",
      "Epoch: 00 [20764/27767 ( 75%)], Train Loss: 0.44142\n",
      "Epoch: 00 [20804/27767 ( 75%)], Train Loss: 0.44098\n",
      "Epoch: 00 [20844/27767 ( 75%)], Train Loss: 0.44068\n",
      "Epoch: 00 [20884/27767 ( 75%)], Train Loss: 0.44046\n",
      "Epoch: 00 [20924/27767 ( 75%)], Train Loss: 0.44018\n",
      "Epoch: 00 [20964/27767 ( 75%)], Train Loss: 0.43979\n",
      "Epoch: 00 [21004/27767 ( 76%)], Train Loss: 0.43950\n",
      "Epoch: 00 [21044/27767 ( 76%)], Train Loss: 0.43916\n",
      "Epoch: 00 [21084/27767 ( 76%)], Train Loss: 0.43905\n",
      "Epoch: 00 [21124/27767 ( 76%)], Train Loss: 0.43885\n",
      "Epoch: 00 [21164/27767 ( 76%)], Train Loss: 0.43875\n",
      "Epoch: 00 [21204/27767 ( 76%)], Train Loss: 0.43832\n",
      "Epoch: 00 [21244/27767 ( 77%)], Train Loss: 0.43819\n",
      "Epoch: 00 [21284/27767 ( 77%)], Train Loss: 0.43802\n",
      "Epoch: 00 [21324/27767 ( 77%)], Train Loss: 0.43801\n",
      "Epoch: 00 [21364/27767 ( 77%)], Train Loss: 0.43762\n",
      "Epoch: 00 [21404/27767 ( 77%)], Train Loss: 0.43721\n",
      "Epoch: 00 [21444/27767 ( 77%)], Train Loss: 0.43742\n",
      "Epoch: 00 [21484/27767 ( 77%)], Train Loss: 0.43721\n",
      "Epoch: 00 [21524/27767 ( 78%)], Train Loss: 0.43707\n",
      "Epoch: 00 [21564/27767 ( 78%)], Train Loss: 0.43665\n",
      "Epoch: 00 [21604/27767 ( 78%)], Train Loss: 0.43645\n",
      "Epoch: 00 [21644/27767 ( 78%)], Train Loss: 0.43642\n",
      "Epoch: 00 [21684/27767 ( 78%)], Train Loss: 0.43622\n",
      "Epoch: 00 [21724/27767 ( 78%)], Train Loss: 0.43593\n",
      "Epoch: 00 [21764/27767 ( 78%)], Train Loss: 0.43559\n",
      "Epoch: 00 [21804/27767 ( 79%)], Train Loss: 0.43528\n",
      "Epoch: 00 [21844/27767 ( 79%)], Train Loss: 0.43487\n",
      "Epoch: 00 [21884/27767 ( 79%)], Train Loss: 0.43455\n",
      "Epoch: 00 [21924/27767 ( 79%)], Train Loss: 0.43426\n",
      "Epoch: 00 [21964/27767 ( 79%)], Train Loss: 0.43380\n",
      "Epoch: 00 [22004/27767 ( 79%)], Train Loss: 0.43363\n",
      "Epoch: 00 [22044/27767 ( 79%)], Train Loss: 0.43330\n",
      "Epoch: 00 [22084/27767 ( 80%)], Train Loss: 0.43314\n",
      "Epoch: 00 [22124/27767 ( 80%)], Train Loss: 0.43283\n",
      "Epoch: 00 [22164/27767 ( 80%)], Train Loss: 0.43242\n",
      "Epoch: 00 [22204/27767 ( 80%)], Train Loss: 0.43214\n",
      "Epoch: 00 [22244/27767 ( 80%)], Train Loss: 0.43196\n",
      "Epoch: 00 [22284/27767 ( 80%)], Train Loss: 0.43175\n",
      "Epoch: 00 [22324/27767 ( 80%)], Train Loss: 0.43176\n",
      "Epoch: 00 [22364/27767 ( 81%)], Train Loss: 0.43161\n",
      "Epoch: 00 [22404/27767 ( 81%)], Train Loss: 0.43137\n",
      "Epoch: 00 [22444/27767 ( 81%)], Train Loss: 0.43098\n",
      "Epoch: 00 [22484/27767 ( 81%)], Train Loss: 0.43078\n",
      "Epoch: 00 [22524/27767 ( 81%)], Train Loss: 0.43040\n",
      "Epoch: 00 [22564/27767 ( 81%)], Train Loss: 0.43037\n",
      "Epoch: 00 [22604/27767 ( 81%)], Train Loss: 0.42990\n",
      "Epoch: 00 [22644/27767 ( 82%)], Train Loss: 0.42975\n",
      "Epoch: 00 [22684/27767 ( 82%)], Train Loss: 0.42933\n",
      "Epoch: 00 [22724/27767 ( 82%)], Train Loss: 0.42903\n",
      "Epoch: 00 [22764/27767 ( 82%)], Train Loss: 0.42871\n",
      "Epoch: 00 [22804/27767 ( 82%)], Train Loss: 0.42850\n",
      "Epoch: 00 [22844/27767 ( 82%)], Train Loss: 0.42826\n",
      "Epoch: 00 [22884/27767 ( 82%)], Train Loss: 0.42825\n",
      "Epoch: 00 [22924/27767 ( 83%)], Train Loss: 0.42791\n",
      "Epoch: 00 [22964/27767 ( 83%)], Train Loss: 0.42778\n",
      "Epoch: 00 [23004/27767 ( 83%)], Train Loss: 0.42757\n",
      "Epoch: 00 [23044/27767 ( 83%)], Train Loss: 0.42742\n",
      "Epoch: 00 [23084/27767 ( 83%)], Train Loss: 0.42723\n",
      "Epoch: 00 [23124/27767 ( 83%)], Train Loss: 0.42691\n",
      "Epoch: 00 [23164/27767 ( 83%)], Train Loss: 0.42675\n",
      "Epoch: 00 [23204/27767 ( 84%)], Train Loss: 0.42650\n",
      "Epoch: 00 [23244/27767 ( 84%)], Train Loss: 0.42612\n",
      "Epoch: 00 [23284/27767 ( 84%)], Train Loss: 0.42588\n",
      "Epoch: 00 [23324/27767 ( 84%)], Train Loss: 0.42569\n",
      "Epoch: 00 [23364/27767 ( 84%)], Train Loss: 0.42530\n",
      "Epoch: 00 [23404/27767 ( 84%)], Train Loss: 0.42528\n",
      "Epoch: 00 [23444/27767 ( 84%)], Train Loss: 0.42492\n",
      "Epoch: 00 [23484/27767 ( 85%)], Train Loss: 0.42480\n",
      "Epoch: 00 [23524/27767 ( 85%)], Train Loss: 0.42438\n",
      "Epoch: 00 [23564/27767 ( 85%)], Train Loss: 0.42420\n",
      "Epoch: 00 [23604/27767 ( 85%)], Train Loss: 0.42392\n",
      "Epoch: 00 [23644/27767 ( 85%)], Train Loss: 0.42356\n",
      "Epoch: 00 [23684/27767 ( 85%)], Train Loss: 0.42334\n",
      "Epoch: 00 [23724/27767 ( 85%)], Train Loss: 0.42307\n",
      "Epoch: 00 [23764/27767 ( 86%)], Train Loss: 0.42265\n",
      "Epoch: 00 [23804/27767 ( 86%)], Train Loss: 0.42257\n",
      "Epoch: 00 [23844/27767 ( 86%)], Train Loss: 0.42237\n",
      "Epoch: 00 [23884/27767 ( 86%)], Train Loss: 0.42194\n",
      "Epoch: 00 [23924/27767 ( 86%)], Train Loss: 0.42162\n",
      "Epoch: 00 [23964/27767 ( 86%)], Train Loss: 0.42138\n",
      "Epoch: 00 [24004/27767 ( 86%)], Train Loss: 0.42117\n",
      "Epoch: 00 [24044/27767 ( 87%)], Train Loss: 0.42099\n",
      "Epoch: 00 [24084/27767 ( 87%)], Train Loss: 0.42080\n",
      "Epoch: 00 [24124/27767 ( 87%)], Train Loss: 0.42060\n",
      "Epoch: 00 [24164/27767 ( 87%)], Train Loss: 0.42044\n",
      "Epoch: 00 [24204/27767 ( 87%)], Train Loss: 0.42009\n",
      "Epoch: 00 [24244/27767 ( 87%)], Train Loss: 0.41997\n",
      "Epoch: 00 [24284/27767 ( 87%)], Train Loss: 0.41988\n",
      "Epoch: 00 [24324/27767 ( 88%)], Train Loss: 0.42002\n",
      "Epoch: 00 [24364/27767 ( 88%)], Train Loss: 0.41984\n",
      "Epoch: 00 [24404/27767 ( 88%)], Train Loss: 0.41963\n",
      "Epoch: 00 [24444/27767 ( 88%)], Train Loss: 0.41959\n",
      "Epoch: 00 [24484/27767 ( 88%)], Train Loss: 0.41930\n",
      "Epoch: 00 [24524/27767 ( 88%)], Train Loss: 0.41897\n",
      "Epoch: 00 [24564/27767 ( 88%)], Train Loss: 0.41877\n",
      "Epoch: 00 [24604/27767 ( 89%)], Train Loss: 0.41879\n",
      "Epoch: 00 [24644/27767 ( 89%)], Train Loss: 0.41840\n",
      "Epoch: 00 [24684/27767 ( 89%)], Train Loss: 0.41810\n",
      "Epoch: 00 [24724/27767 ( 89%)], Train Loss: 0.41817\n",
      "Epoch: 00 [24764/27767 ( 89%)], Train Loss: 0.41820\n",
      "Epoch: 00 [24804/27767 ( 89%)], Train Loss: 0.41813\n",
      "Epoch: 00 [24844/27767 ( 89%)], Train Loss: 0.41798\n",
      "Epoch: 00 [24884/27767 ( 90%)], Train Loss: 0.41768\n",
      "Epoch: 00 [24924/27767 ( 90%)], Train Loss: 0.41739\n",
      "Epoch: 00 [24964/27767 ( 90%)], Train Loss: 0.41715\n",
      "Epoch: 00 [25004/27767 ( 90%)], Train Loss: 0.41704\n",
      "Epoch: 00 [25044/27767 ( 90%)], Train Loss: 0.41678\n",
      "Epoch: 00 [25084/27767 ( 90%)], Train Loss: 0.41649\n",
      "Epoch: 00 [25124/27767 ( 90%)], Train Loss: 0.41617\n",
      "Epoch: 00 [25164/27767 ( 91%)], Train Loss: 0.41612\n",
      "Epoch: 00 [25204/27767 ( 91%)], Train Loss: 0.41584\n",
      "Epoch: 00 [25244/27767 ( 91%)], Train Loss: 0.41558\n",
      "Epoch: 00 [25284/27767 ( 91%)], Train Loss: 0.41520\n",
      "Epoch: 00 [25324/27767 ( 91%)], Train Loss: 0.41503\n",
      "Epoch: 00 [25364/27767 ( 91%)], Train Loss: 0.41479\n",
      "Epoch: 00 [25404/27767 ( 91%)], Train Loss: 0.41484\n",
      "Epoch: 00 [25444/27767 ( 92%)], Train Loss: 0.41471\n",
      "Epoch: 00 [25484/27767 ( 92%)], Train Loss: 0.41446\n",
      "Epoch: 00 [25524/27767 ( 92%)], Train Loss: 0.41458\n",
      "Epoch: 00 [25564/27767 ( 92%)], Train Loss: 0.41430\n",
      "Epoch: 00 [25604/27767 ( 92%)], Train Loss: 0.41409\n",
      "Epoch: 00 [25644/27767 ( 92%)], Train Loss: 0.41391\n",
      "Epoch: 00 [25684/27767 ( 92%)], Train Loss: 0.41358\n",
      "Epoch: 00 [25724/27767 ( 93%)], Train Loss: 0.41363\n",
      "Epoch: 00 [25764/27767 ( 93%)], Train Loss: 0.41364\n",
      "Epoch: 00 [25804/27767 ( 93%)], Train Loss: 0.41356\n",
      "Epoch: 00 [25844/27767 ( 93%)], Train Loss: 0.41347\n",
      "Epoch: 00 [25884/27767 ( 93%)], Train Loss: 0.41320\n",
      "Epoch: 00 [25924/27767 ( 93%)], Train Loss: 0.41297\n",
      "Epoch: 00 [25964/27767 ( 94%)], Train Loss: 0.41277\n",
      "Epoch: 00 [26004/27767 ( 94%)], Train Loss: 0.41256\n",
      "Epoch: 00 [26044/27767 ( 94%)], Train Loss: 0.41229\n",
      "Epoch: 00 [26084/27767 ( 94%)], Train Loss: 0.41200\n",
      "Epoch: 00 [26124/27767 ( 94%)], Train Loss: 0.41172\n",
      "Epoch: 00 [26164/27767 ( 94%)], Train Loss: 0.41141\n",
      "Epoch: 00 [26204/27767 ( 94%)], Train Loss: 0.41129\n",
      "Epoch: 00 [26244/27767 ( 95%)], Train Loss: 0.41102\n",
      "Epoch: 00 [26284/27767 ( 95%)], Train Loss: 0.41072\n",
      "Epoch: 00 [26324/27767 ( 95%)], Train Loss: 0.41050\n",
      "Epoch: 00 [26364/27767 ( 95%)], Train Loss: 0.41030\n",
      "Epoch: 00 [26404/27767 ( 95%)], Train Loss: 0.40999\n",
      "Epoch: 00 [26444/27767 ( 95%)], Train Loss: 0.40981\n",
      "Epoch: 00 [26484/27767 ( 95%)], Train Loss: 0.40973\n",
      "Epoch: 00 [26524/27767 ( 96%)], Train Loss: 0.40945\n",
      "Epoch: 00 [26564/27767 ( 96%)], Train Loss: 0.40914\n",
      "Epoch: 00 [26604/27767 ( 96%)], Train Loss: 0.40899\n",
      "Epoch: 00 [26644/27767 ( 96%)], Train Loss: 0.40875\n",
      "Epoch: 00 [26684/27767 ( 96%)], Train Loss: 0.40850\n",
      "Epoch: 00 [26724/27767 ( 96%)], Train Loss: 0.40838\n",
      "Epoch: 00 [26764/27767 ( 96%)], Train Loss: 0.40810\n",
      "Epoch: 00 [26804/27767 ( 97%)], Train Loss: 0.40801\n",
      "Epoch: 00 [26844/27767 ( 97%)], Train Loss: 0.40797\n",
      "Epoch: 00 [26884/27767 ( 97%)], Train Loss: 0.40765\n",
      "Epoch: 00 [26924/27767 ( 97%)], Train Loss: 0.40728\n",
      "Epoch: 00 [26964/27767 ( 97%)], Train Loss: 0.40719\n",
      "Epoch: 00 [27004/27767 ( 97%)], Train Loss: 0.40695\n",
      "Epoch: 00 [27044/27767 ( 97%)], Train Loss: 0.40675\n",
      "Epoch: 00 [27084/27767 ( 98%)], Train Loss: 0.40645\n",
      "Epoch: 00 [27124/27767 ( 98%)], Train Loss: 0.40623\n",
      "Epoch: 00 [27164/27767 ( 98%)], Train Loss: 0.40612\n",
      "Epoch: 00 [27204/27767 ( 98%)], Train Loss: 0.40578\n",
      "Epoch: 00 [27244/27767 ( 98%)], Train Loss: 0.40559\n",
      "Epoch: 00 [27284/27767 ( 98%)], Train Loss: 0.40541\n",
      "Epoch: 00 [27324/27767 ( 98%)], Train Loss: 0.40517\n",
      "Epoch: 00 [27364/27767 ( 99%)], Train Loss: 0.40509\n",
      "Epoch: 00 [27404/27767 ( 99%)], Train Loss: 0.40491\n",
      "Epoch: 00 [27444/27767 ( 99%)], Train Loss: 0.40456\n",
      "Epoch: 00 [27484/27767 ( 99%)], Train Loss: 0.40436\n",
      "Epoch: 00 [27524/27767 ( 99%)], Train Loss: 0.40408\n",
      "Epoch: 00 [27564/27767 ( 99%)], Train Loss: 0.40387\n",
      "Epoch: 00 [27604/27767 ( 99%)], Train Loss: 0.40373\n",
      "Epoch: 00 [27644/27767 (100%)], Train Loss: 0.40353\n",
      "Epoch: 00 [27684/27767 (100%)], Train Loss: 0.40355\n",
      "Epoch: 00 [27724/27767 (100%)], Train Loss: 0.40342\n",
      "Epoch: 00 [27764/27767 (100%)], Train Loss: 0.40315\n",
      "Epoch: 00 [27767/27767 (100%)], Train Loss: 0.40314\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.54844\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.54844\n",
      "Saving model checkpoint to output/checkpoint-fold-6.\n",
      "\n",
      "Total Training Time: 3231.574151277542secs, Average Training Time per Epoch: 3231.574151277542secs.\n",
      "Total Validation Time: 141.42319703102112secs, Average Validation Time per Epoch: 141.42319703102112secs.\n",
      "--------------------------------------------------\n",
      "FOLD: 7\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 27767, Num examples Valid=3955\n",
      "Total Training Steps: 3471, Total Warmup Steps: 347\n",
      "Epoch: 00 [    4/27767 (  0%)], Train Loss: 2.80358\n",
      "Epoch: 00 [   44/27767 (  0%)], Train Loss: 2.78321\n",
      "Epoch: 00 [   84/27767 (  0%)], Train Loss: 2.76198\n",
      "Epoch: 00 [  124/27767 (  0%)], Train Loss: 2.75336\n",
      "Epoch: 00 [  164/27767 (  1%)], Train Loss: 2.72723\n",
      "Epoch: 00 [  204/27767 (  1%)], Train Loss: 2.68722\n",
      "Epoch: 00 [  244/27767 (  1%)], Train Loss: 2.64209\n",
      "Epoch: 00 [  284/27767 (  1%)], Train Loss: 2.59230\n",
      "Epoch: 00 [  324/27767 (  1%)], Train Loss: 2.53651\n",
      "Epoch: 00 [  364/27767 (  1%)], Train Loss: 2.47019\n",
      "Epoch: 00 [  404/27767 (  1%)], Train Loss: 2.39030\n",
      "Epoch: 00 [  444/27767 (  2%)], Train Loss: 2.31545\n",
      "Epoch: 00 [  484/27767 (  2%)], Train Loss: 2.23006\n",
      "Epoch: 00 [  524/27767 (  2%)], Train Loss: 2.14246\n",
      "Epoch: 00 [  564/27767 (  2%)], Train Loss: 2.06072\n",
      "Epoch: 00 [  604/27767 (  2%)], Train Loss: 1.98342\n",
      "Epoch: 00 [  644/27767 (  2%)], Train Loss: 1.91672\n",
      "Epoch: 00 [  684/27767 (  2%)], Train Loss: 1.84082\n",
      "Epoch: 00 [  724/27767 (  3%)], Train Loss: 1.77909\n",
      "Epoch: 00 [  764/27767 (  3%)], Train Loss: 1.71471\n",
      "Epoch: 00 [  804/27767 (  3%)], Train Loss: 1.65562\n",
      "Epoch: 00 [  844/27767 (  3%)], Train Loss: 1.59833\n",
      "Epoch: 00 [  884/27767 (  3%)], Train Loss: 1.55655\n",
      "Epoch: 00 [  924/27767 (  3%)], Train Loss: 1.51942\n",
      "Epoch: 00 [  964/27767 (  3%)], Train Loss: 1.47508\n",
      "Epoch: 00 [ 1004/27767 (  4%)], Train Loss: 1.43739\n",
      "Epoch: 00 [ 1044/27767 (  4%)], Train Loss: 1.39814\n",
      "Epoch: 00 [ 1084/27767 (  4%)], Train Loss: 1.36660\n",
      "Epoch: 00 [ 1124/27767 (  4%)], Train Loss: 1.33646\n",
      "Epoch: 00 [ 1164/27767 (  4%)], Train Loss: 1.31270\n",
      "Epoch: 00 [ 1204/27767 (  4%)], Train Loss: 1.28335\n",
      "Epoch: 00 [ 1244/27767 (  4%)], Train Loss: 1.25783\n",
      "Epoch: 00 [ 1284/27767 (  5%)], Train Loss: 1.24614\n",
      "Epoch: 00 [ 1324/27767 (  5%)], Train Loss: 1.22156\n",
      "Epoch: 00 [ 1364/27767 (  5%)], Train Loss: 1.20041\n",
      "Epoch: 00 [ 1404/27767 (  5%)], Train Loss: 1.18171\n",
      "Epoch: 00 [ 1444/27767 (  5%)], Train Loss: 1.16057\n",
      "Epoch: 00 [ 1484/27767 (  5%)], Train Loss: 1.14230\n",
      "Epoch: 00 [ 1524/27767 (  5%)], Train Loss: 1.12593\n",
      "Epoch: 00 [ 1564/27767 (  6%)], Train Loss: 1.10728\n",
      "Epoch: 00 [ 1604/27767 (  6%)], Train Loss: 1.08971\n",
      "Epoch: 00 [ 1644/27767 (  6%)], Train Loss: 1.07574\n",
      "Epoch: 00 [ 1684/27767 (  6%)], Train Loss: 1.05943\n",
      "Epoch: 00 [ 1724/27767 (  6%)], Train Loss: 1.04648\n",
      "Epoch: 00 [ 1764/27767 (  6%)], Train Loss: 1.03561\n",
      "Epoch: 00 [ 1804/27767 (  6%)], Train Loss: 1.02302\n",
      "Epoch: 00 [ 1844/27767 (  7%)], Train Loss: 1.01604\n",
      "Epoch: 00 [ 1884/27767 (  7%)], Train Loss: 1.00210\n",
      "Epoch: 00 [ 1924/27767 (  7%)], Train Loss: 0.99250\n",
      "Epoch: 00 [ 1964/27767 (  7%)], Train Loss: 0.97912\n",
      "Epoch: 00 [ 2004/27767 (  7%)], Train Loss: 0.96717\n",
      "Epoch: 00 [ 2044/27767 (  7%)], Train Loss: 0.95812\n",
      "Epoch: 00 [ 2084/27767 (  8%)], Train Loss: 0.95003\n",
      "Epoch: 00 [ 2124/27767 (  8%)], Train Loss: 0.94224\n",
      "Epoch: 00 [ 2164/27767 (  8%)], Train Loss: 0.93245\n",
      "Epoch: 00 [ 2204/27767 (  8%)], Train Loss: 0.92523\n",
      "Epoch: 00 [ 2244/27767 (  8%)], Train Loss: 0.92315\n",
      "Epoch: 00 [ 2284/27767 (  8%)], Train Loss: 0.91418\n",
      "Epoch: 00 [ 2324/27767 (  8%)], Train Loss: 0.90553\n",
      "Epoch: 00 [ 2364/27767 (  9%)], Train Loss: 0.89731\n",
      "Epoch: 00 [ 2404/27767 (  9%)], Train Loss: 0.89048\n",
      "Epoch: 00 [ 2444/27767 (  9%)], Train Loss: 0.88468\n",
      "Epoch: 00 [ 2484/27767 (  9%)], Train Loss: 0.87919\n",
      "Epoch: 00 [ 2524/27767 (  9%)], Train Loss: 0.87053\n",
      "Epoch: 00 [ 2564/27767 (  9%)], Train Loss: 0.86231\n",
      "Epoch: 00 [ 2604/27767 (  9%)], Train Loss: 0.85533\n",
      "Epoch: 00 [ 2644/27767 ( 10%)], Train Loss: 0.84940\n",
      "Epoch: 00 [ 2684/27767 ( 10%)], Train Loss: 0.84216\n",
      "Epoch: 00 [ 2724/27767 ( 10%)], Train Loss: 0.83694\n",
      "Epoch: 00 [ 2764/27767 ( 10%)], Train Loss: 0.82887\n",
      "Epoch: 00 [ 2804/27767 ( 10%)], Train Loss: 0.82460\n",
      "Epoch: 00 [ 2844/27767 ( 10%)], Train Loss: 0.81983\n",
      "Epoch: 00 [ 2884/27767 ( 10%)], Train Loss: 0.81603\n",
      "Epoch: 00 [ 2924/27767 ( 11%)], Train Loss: 0.81254\n",
      "Epoch: 00 [ 2964/27767 ( 11%)], Train Loss: 0.80683\n",
      "Epoch: 00 [ 3004/27767 ( 11%)], Train Loss: 0.80266\n",
      "Epoch: 00 [ 3044/27767 ( 11%)], Train Loss: 0.79880\n",
      "Epoch: 00 [ 3084/27767 ( 11%)], Train Loss: 0.79680\n",
      "Epoch: 00 [ 3124/27767 ( 11%)], Train Loss: 0.79326\n",
      "Epoch: 00 [ 3164/27767 ( 11%)], Train Loss: 0.79361\n",
      "Epoch: 00 [ 3204/27767 ( 12%)], Train Loss: 0.79180\n",
      "Epoch: 00 [ 3244/27767 ( 12%)], Train Loss: 0.78941\n",
      "Epoch: 00 [ 3284/27767 ( 12%)], Train Loss: 0.78377\n",
      "Epoch: 00 [ 3324/27767 ( 12%)], Train Loss: 0.77790\n",
      "Epoch: 00 [ 3364/27767 ( 12%)], Train Loss: 0.77521\n",
      "Epoch: 00 [ 3404/27767 ( 12%)], Train Loss: 0.77124\n",
      "Epoch: 00 [ 3444/27767 ( 12%)], Train Loss: 0.76746\n",
      "Epoch: 00 [ 3484/27767 ( 13%)], Train Loss: 0.76315\n",
      "Epoch: 00 [ 3524/27767 ( 13%)], Train Loss: 0.76080\n",
      "Epoch: 00 [ 3564/27767 ( 13%)], Train Loss: 0.75811\n",
      "Epoch: 00 [ 3604/27767 ( 13%)], Train Loss: 0.75482\n",
      "Epoch: 00 [ 3644/27767 ( 13%)], Train Loss: 0.75181\n",
      "Epoch: 00 [ 3684/27767 ( 13%)], Train Loss: 0.74830\n",
      "Epoch: 00 [ 3724/27767 ( 13%)], Train Loss: 0.74421\n",
      "Epoch: 00 [ 3764/27767 ( 14%)], Train Loss: 0.74185\n",
      "Epoch: 00 [ 3804/27767 ( 14%)], Train Loss: 0.73900\n",
      "Epoch: 00 [ 3844/27767 ( 14%)], Train Loss: 0.73675\n",
      "Epoch: 00 [ 3884/27767 ( 14%)], Train Loss: 0.73305\n",
      "Epoch: 00 [ 3924/27767 ( 14%)], Train Loss: 0.72994\n",
      "Epoch: 00 [ 3964/27767 ( 14%)], Train Loss: 0.72792\n",
      "Epoch: 00 [ 4004/27767 ( 14%)], Train Loss: 0.72407\n",
      "Epoch: 00 [ 4044/27767 ( 15%)], Train Loss: 0.72491\n",
      "Epoch: 00 [ 4084/27767 ( 15%)], Train Loss: 0.72253\n",
      "Epoch: 00 [ 4124/27767 ( 15%)], Train Loss: 0.72026\n",
      "Epoch: 00 [ 4164/27767 ( 15%)], Train Loss: 0.71715\n",
      "Epoch: 00 [ 4204/27767 ( 15%)], Train Loss: 0.71451\n",
      "Epoch: 00 [ 4244/27767 ( 15%)], Train Loss: 0.71114\n",
      "Epoch: 00 [ 4284/27767 ( 15%)], Train Loss: 0.70839\n",
      "Epoch: 00 [ 4324/27767 ( 16%)], Train Loss: 0.70521\n",
      "Epoch: 00 [ 4364/27767 ( 16%)], Train Loss: 0.70275\n",
      "Epoch: 00 [ 4404/27767 ( 16%)], Train Loss: 0.70061\n",
      "Epoch: 00 [ 4444/27767 ( 16%)], Train Loss: 0.69892\n",
      "Epoch: 00 [ 4484/27767 ( 16%)], Train Loss: 0.69615\n",
      "Epoch: 00 [ 4524/27767 ( 16%)], Train Loss: 0.69334\n",
      "Epoch: 00 [ 4564/27767 ( 16%)], Train Loss: 0.69183\n",
      "Epoch: 00 [ 4604/27767 ( 17%)], Train Loss: 0.69176\n",
      "Epoch: 00 [ 4644/27767 ( 17%)], Train Loss: 0.68996\n",
      "Epoch: 00 [ 4684/27767 ( 17%)], Train Loss: 0.68774\n",
      "Epoch: 00 [ 4724/27767 ( 17%)], Train Loss: 0.68517\n",
      "Epoch: 00 [ 4764/27767 ( 17%)], Train Loss: 0.68333\n",
      "Epoch: 00 [ 4804/27767 ( 17%)], Train Loss: 0.68245\n",
      "Epoch: 00 [ 4844/27767 ( 17%)], Train Loss: 0.67894\n",
      "Epoch: 00 [ 4884/27767 ( 18%)], Train Loss: 0.67638\n",
      "Epoch: 00 [ 4924/27767 ( 18%)], Train Loss: 0.67335\n",
      "Epoch: 00 [ 4964/27767 ( 18%)], Train Loss: 0.67016\n",
      "Epoch: 00 [ 5004/27767 ( 18%)], Train Loss: 0.66784\n",
      "Epoch: 00 [ 5044/27767 ( 18%)], Train Loss: 0.66488\n",
      "Epoch: 00 [ 5084/27767 ( 18%)], Train Loss: 0.66186\n",
      "Epoch: 00 [ 5124/27767 ( 18%)], Train Loss: 0.66123\n",
      "Epoch: 00 [ 5164/27767 ( 19%)], Train Loss: 0.66027\n",
      "Epoch: 00 [ 5204/27767 ( 19%)], Train Loss: 0.65864\n",
      "Epoch: 00 [ 5244/27767 ( 19%)], Train Loss: 0.65597\n",
      "Epoch: 00 [ 5284/27767 ( 19%)], Train Loss: 0.65381\n",
      "Epoch: 00 [ 5324/27767 ( 19%)], Train Loss: 0.65089\n",
      "Epoch: 00 [ 5364/27767 ( 19%)], Train Loss: 0.65010\n",
      "Epoch: 00 [ 5404/27767 ( 19%)], Train Loss: 0.64769\n",
      "Epoch: 00 [ 5444/27767 ( 20%)], Train Loss: 0.64538\n",
      "Epoch: 00 [ 5484/27767 ( 20%)], Train Loss: 0.64354\n",
      "Epoch: 00 [ 5524/27767 ( 20%)], Train Loss: 0.64098\n",
      "Epoch: 00 [ 5564/27767 ( 20%)], Train Loss: 0.63880\n",
      "Epoch: 00 [ 5604/27767 ( 20%)], Train Loss: 0.63606\n",
      "Epoch: 00 [ 5644/27767 ( 20%)], Train Loss: 0.63463\n",
      "Epoch: 00 [ 5684/27767 ( 20%)], Train Loss: 0.63274\n",
      "Epoch: 00 [ 5724/27767 ( 21%)], Train Loss: 0.63145\n",
      "Epoch: 00 [ 5764/27767 ( 21%)], Train Loss: 0.62999\n",
      "Epoch: 00 [ 5804/27767 ( 21%)], Train Loss: 0.62838\n",
      "Epoch: 00 [ 5844/27767 ( 21%)], Train Loss: 0.62725\n",
      "Epoch: 00 [ 5884/27767 ( 21%)], Train Loss: 0.62501\n",
      "Epoch: 00 [ 5924/27767 ( 21%)], Train Loss: 0.62341\n",
      "Epoch: 00 [ 5964/27767 ( 21%)], Train Loss: 0.62171\n",
      "Epoch: 00 [ 6004/27767 ( 22%)], Train Loss: 0.62008\n",
      "Epoch: 00 [ 6044/27767 ( 22%)], Train Loss: 0.61854\n",
      "Epoch: 00 [ 6084/27767 ( 22%)], Train Loss: 0.61793\n",
      "Epoch: 00 [ 6124/27767 ( 22%)], Train Loss: 0.61737\n",
      "Epoch: 00 [ 6164/27767 ( 22%)], Train Loss: 0.61644\n",
      "Epoch: 00 [ 6204/27767 ( 22%)], Train Loss: 0.61459\n",
      "Epoch: 00 [ 6244/27767 ( 22%)], Train Loss: 0.61378\n",
      "Epoch: 00 [ 6284/27767 ( 23%)], Train Loss: 0.61299\n",
      "Epoch: 00 [ 6324/27767 ( 23%)], Train Loss: 0.61185\n",
      "Epoch: 00 [ 6364/27767 ( 23%)], Train Loss: 0.61083\n",
      "Epoch: 00 [ 6404/27767 ( 23%)], Train Loss: 0.61029\n",
      "Epoch: 00 [ 6444/27767 ( 23%)], Train Loss: 0.60878\n",
      "Epoch: 00 [ 6484/27767 ( 23%)], Train Loss: 0.60721\n",
      "Epoch: 00 [ 6524/27767 ( 23%)], Train Loss: 0.60574\n",
      "Epoch: 00 [ 6564/27767 ( 24%)], Train Loss: 0.60372\n",
      "Epoch: 00 [ 6604/27767 ( 24%)], Train Loss: 0.60264\n",
      "Epoch: 00 [ 6644/27767 ( 24%)], Train Loss: 0.60170\n",
      "Epoch: 00 [ 6684/27767 ( 24%)], Train Loss: 0.60067\n",
      "Epoch: 00 [ 6724/27767 ( 24%)], Train Loss: 0.59883\n",
      "Epoch: 00 [ 6764/27767 ( 24%)], Train Loss: 0.59766\n",
      "Epoch: 00 [ 6804/27767 ( 25%)], Train Loss: 0.59647\n",
      "Epoch: 00 [ 6844/27767 ( 25%)], Train Loss: 0.59561\n",
      "Epoch: 00 [ 6884/27767 ( 25%)], Train Loss: 0.59420\n",
      "Epoch: 00 [ 6924/27767 ( 25%)], Train Loss: 0.59320\n",
      "Epoch: 00 [ 6964/27767 ( 25%)], Train Loss: 0.59164\n",
      "Epoch: 00 [ 7004/27767 ( 25%)], Train Loss: 0.59136\n",
      "Epoch: 00 [ 7044/27767 ( 25%)], Train Loss: 0.59054\n",
      "Epoch: 00 [ 7084/27767 ( 26%)], Train Loss: 0.58947\n",
      "Epoch: 00 [ 7124/27767 ( 26%)], Train Loss: 0.58911\n",
      "Epoch: 00 [ 7164/27767 ( 26%)], Train Loss: 0.58838\n",
      "Epoch: 00 [ 7204/27767 ( 26%)], Train Loss: 0.58745\n",
      "Epoch: 00 [ 7244/27767 ( 26%)], Train Loss: 0.58648\n",
      "Epoch: 00 [ 7284/27767 ( 26%)], Train Loss: 0.58426\n",
      "Epoch: 00 [ 7324/27767 ( 26%)], Train Loss: 0.58302\n",
      "Epoch: 00 [ 7364/27767 ( 27%)], Train Loss: 0.58220\n",
      "Epoch: 00 [ 7404/27767 ( 27%)], Train Loss: 0.58202\n",
      "Epoch: 00 [ 7444/27767 ( 27%)], Train Loss: 0.58103\n",
      "Epoch: 00 [ 7484/27767 ( 27%)], Train Loss: 0.58080\n",
      "Epoch: 00 [ 7524/27767 ( 27%)], Train Loss: 0.58002\n",
      "Epoch: 00 [ 7564/27767 ( 27%)], Train Loss: 0.57870\n",
      "Epoch: 00 [ 7604/27767 ( 27%)], Train Loss: 0.57827\n",
      "Epoch: 00 [ 7644/27767 ( 28%)], Train Loss: 0.57721\n",
      "Epoch: 00 [ 7684/27767 ( 28%)], Train Loss: 0.57572\n",
      "Epoch: 00 [ 7724/27767 ( 28%)], Train Loss: 0.57488\n",
      "Epoch: 00 [ 7764/27767 ( 28%)], Train Loss: 0.57444\n",
      "Epoch: 00 [ 7804/27767 ( 28%)], Train Loss: 0.57391\n",
      "Epoch: 00 [ 7844/27767 ( 28%)], Train Loss: 0.57222\n",
      "Epoch: 00 [ 7884/27767 ( 28%)], Train Loss: 0.57119\n",
      "Epoch: 00 [ 7924/27767 ( 29%)], Train Loss: 0.57009\n",
      "Epoch: 00 [ 7964/27767 ( 29%)], Train Loss: 0.56912\n",
      "Epoch: 00 [ 8004/27767 ( 29%)], Train Loss: 0.56844\n",
      "Epoch: 00 [ 8044/27767 ( 29%)], Train Loss: 0.56879\n",
      "Epoch: 00 [ 8084/27767 ( 29%)], Train Loss: 0.56789\n",
      "Epoch: 00 [ 8124/27767 ( 29%)], Train Loss: 0.56740\n",
      "Epoch: 00 [ 8164/27767 ( 29%)], Train Loss: 0.56714\n",
      "Epoch: 00 [ 8204/27767 ( 30%)], Train Loss: 0.56594\n",
      "Epoch: 00 [ 8244/27767 ( 30%)], Train Loss: 0.56554\n",
      "Epoch: 00 [ 8284/27767 ( 30%)], Train Loss: 0.56442\n",
      "Epoch: 00 [ 8324/27767 ( 30%)], Train Loss: 0.56348\n",
      "Epoch: 00 [ 8364/27767 ( 30%)], Train Loss: 0.56211\n",
      "Epoch: 00 [ 8404/27767 ( 30%)], Train Loss: 0.56170\n",
      "Epoch: 00 [ 8444/27767 ( 30%)], Train Loss: 0.56130\n",
      "Epoch: 00 [ 8484/27767 ( 31%)], Train Loss: 0.56067\n",
      "Epoch: 00 [ 8524/27767 ( 31%)], Train Loss: 0.56014\n",
      "Epoch: 00 [ 8564/27767 ( 31%)], Train Loss: 0.55914\n",
      "Epoch: 00 [ 8604/27767 ( 31%)], Train Loss: 0.55804\n",
      "Epoch: 00 [ 8644/27767 ( 31%)], Train Loss: 0.55708\n",
      "Epoch: 00 [ 8684/27767 ( 31%)], Train Loss: 0.55589\n",
      "Epoch: 00 [ 8724/27767 ( 31%)], Train Loss: 0.55474\n",
      "Epoch: 00 [ 8764/27767 ( 32%)], Train Loss: 0.55428\n",
      "Epoch: 00 [ 8804/27767 ( 32%)], Train Loss: 0.55354\n",
      "Epoch: 00 [ 8844/27767 ( 32%)], Train Loss: 0.55239\n",
      "Epoch: 00 [ 8884/27767 ( 32%)], Train Loss: 0.55192\n",
      "Epoch: 00 [ 8924/27767 ( 32%)], Train Loss: 0.55235\n",
      "Epoch: 00 [ 8964/27767 ( 32%)], Train Loss: 0.55123\n",
      "Epoch: 00 [ 9004/27767 ( 32%)], Train Loss: 0.55094\n",
      "Epoch: 00 [ 9044/27767 ( 33%)], Train Loss: 0.54987\n",
      "Epoch: 00 [ 9084/27767 ( 33%)], Train Loss: 0.55003\n",
      "Epoch: 00 [ 9124/27767 ( 33%)], Train Loss: 0.54875\n",
      "Epoch: 00 [ 9164/27767 ( 33%)], Train Loss: 0.54846\n",
      "Epoch: 00 [ 9204/27767 ( 33%)], Train Loss: 0.54731\n",
      "Epoch: 00 [ 9244/27767 ( 33%)], Train Loss: 0.54633\n",
      "Epoch: 00 [ 9284/27767 ( 33%)], Train Loss: 0.54575\n",
      "Epoch: 00 [ 9324/27767 ( 34%)], Train Loss: 0.54529\n",
      "Epoch: 00 [ 9364/27767 ( 34%)], Train Loss: 0.54531\n",
      "Epoch: 00 [ 9404/27767 ( 34%)], Train Loss: 0.54560\n",
      "Epoch: 00 [ 9444/27767 ( 34%)], Train Loss: 0.54455\n",
      "Epoch: 00 [ 9484/27767 ( 34%)], Train Loss: 0.54346\n",
      "Epoch: 00 [ 9524/27767 ( 34%)], Train Loss: 0.54286\n",
      "Epoch: 00 [ 9564/27767 ( 34%)], Train Loss: 0.54205\n",
      "Epoch: 00 [ 9604/27767 ( 35%)], Train Loss: 0.54101\n",
      "Epoch: 00 [ 9644/27767 ( 35%)], Train Loss: 0.53982\n",
      "Epoch: 00 [ 9684/27767 ( 35%)], Train Loss: 0.53891\n",
      "Epoch: 00 [ 9724/27767 ( 35%)], Train Loss: 0.53807\n",
      "Epoch: 00 [ 9764/27767 ( 35%)], Train Loss: 0.53746\n",
      "Epoch: 00 [ 9804/27767 ( 35%)], Train Loss: 0.53800\n",
      "Epoch: 00 [ 9844/27767 ( 35%)], Train Loss: 0.53721\n",
      "Epoch: 00 [ 9884/27767 ( 36%)], Train Loss: 0.53657\n",
      "Epoch: 00 [ 9924/27767 ( 36%)], Train Loss: 0.53568\n",
      "Epoch: 00 [ 9964/27767 ( 36%)], Train Loss: 0.53537\n",
      "Epoch: 00 [10004/27767 ( 36%)], Train Loss: 0.53463\n",
      "Epoch: 00 [10044/27767 ( 36%)], Train Loss: 0.53416\n",
      "Epoch: 00 [10084/27767 ( 36%)], Train Loss: 0.53317\n",
      "Epoch: 00 [10124/27767 ( 36%)], Train Loss: 0.53263\n",
      "Epoch: 00 [10164/27767 ( 37%)], Train Loss: 0.53223\n",
      "Epoch: 00 [10204/27767 ( 37%)], Train Loss: 0.53196\n",
      "Epoch: 00 [10244/27767 ( 37%)], Train Loss: 0.53167\n",
      "Epoch: 00 [10284/27767 ( 37%)], Train Loss: 0.53119\n",
      "Epoch: 00 [10324/27767 ( 37%)], Train Loss: 0.53004\n",
      "Epoch: 00 [10364/27767 ( 37%)], Train Loss: 0.52938\n",
      "Epoch: 00 [10404/27767 ( 37%)], Train Loss: 0.52818\n",
      "Epoch: 00 [10444/27767 ( 38%)], Train Loss: 0.52710\n",
      "Epoch: 00 [10484/27767 ( 38%)], Train Loss: 0.52617\n",
      "Epoch: 00 [10524/27767 ( 38%)], Train Loss: 0.52554\n",
      "Epoch: 00 [10564/27767 ( 38%)], Train Loss: 0.52498\n",
      "Epoch: 00 [10604/27767 ( 38%)], Train Loss: 0.52436\n",
      "Epoch: 00 [10644/27767 ( 38%)], Train Loss: 0.52448\n",
      "Epoch: 00 [10684/27767 ( 38%)], Train Loss: 0.52394\n",
      "Epoch: 00 [10724/27767 ( 39%)], Train Loss: 0.52390\n",
      "Epoch: 00 [10764/27767 ( 39%)], Train Loss: 0.52365\n",
      "Epoch: 00 [10804/27767 ( 39%)], Train Loss: 0.52319\n",
      "Epoch: 00 [10844/27767 ( 39%)], Train Loss: 0.52234\n",
      "Epoch: 00 [10884/27767 ( 39%)], Train Loss: 0.52210\n",
      "Epoch: 00 [10924/27767 ( 39%)], Train Loss: 0.52133\n",
      "Epoch: 00 [10964/27767 ( 39%)], Train Loss: 0.52048\n",
      "Epoch: 00 [11004/27767 ( 40%)], Train Loss: 0.51974\n",
      "Epoch: 00 [11044/27767 ( 40%)], Train Loss: 0.51916\n",
      "Epoch: 00 [11084/27767 ( 40%)], Train Loss: 0.51835\n",
      "Epoch: 00 [11124/27767 ( 40%)], Train Loss: 0.51771\n",
      "Epoch: 00 [11164/27767 ( 40%)], Train Loss: 0.51684\n",
      "Epoch: 00 [11204/27767 ( 40%)], Train Loss: 0.51648\n",
      "Epoch: 00 [11244/27767 ( 40%)], Train Loss: 0.51587\n",
      "Epoch: 00 [11284/27767 ( 41%)], Train Loss: 0.51561\n",
      "Epoch: 00 [11324/27767 ( 41%)], Train Loss: 0.51489\n",
      "Epoch: 00 [11364/27767 ( 41%)], Train Loss: 0.51425\n",
      "Epoch: 00 [11404/27767 ( 41%)], Train Loss: 0.51367\n",
      "Epoch: 00 [11444/27767 ( 41%)], Train Loss: 0.51285\n",
      "Epoch: 00 [11484/27767 ( 41%)], Train Loss: 0.51267\n",
      "Epoch: 00 [11524/27767 ( 42%)], Train Loss: 0.51221\n",
      "Epoch: 00 [11564/27767 ( 42%)], Train Loss: 0.51194\n",
      "Epoch: 00 [11604/27767 ( 42%)], Train Loss: 0.51091\n",
      "Epoch: 00 [11644/27767 ( 42%)], Train Loss: 0.51054\n",
      "Epoch: 00 [11684/27767 ( 42%)], Train Loss: 0.50960\n",
      "Epoch: 00 [11724/27767 ( 42%)], Train Loss: 0.50906\n",
      "Epoch: 00 [11764/27767 ( 42%)], Train Loss: 0.50923\n",
      "Epoch: 00 [11804/27767 ( 43%)], Train Loss: 0.50823\n",
      "Epoch: 00 [11844/27767 ( 43%)], Train Loss: 0.50771\n",
      "Epoch: 00 [11884/27767 ( 43%)], Train Loss: 0.50702\n",
      "Epoch: 00 [11924/27767 ( 43%)], Train Loss: 0.50693\n",
      "Epoch: 00 [11964/27767 ( 43%)], Train Loss: 0.50677\n",
      "Epoch: 00 [12004/27767 ( 43%)], Train Loss: 0.50626\n",
      "Epoch: 00 [12044/27767 ( 43%)], Train Loss: 0.50596\n",
      "Epoch: 00 [12084/27767 ( 44%)], Train Loss: 0.50547\n",
      "Epoch: 00 [12124/27767 ( 44%)], Train Loss: 0.50519\n",
      "Epoch: 00 [12164/27767 ( 44%)], Train Loss: 0.50460\n",
      "Epoch: 00 [12204/27767 ( 44%)], Train Loss: 0.50355\n",
      "Epoch: 00 [12244/27767 ( 44%)], Train Loss: 0.50348\n",
      "Epoch: 00 [12284/27767 ( 44%)], Train Loss: 0.50337\n",
      "Epoch: 00 [12324/27767 ( 44%)], Train Loss: 0.50287\n",
      "Epoch: 00 [12364/27767 ( 45%)], Train Loss: 0.50245\n",
      "Epoch: 00 [12404/27767 ( 45%)], Train Loss: 0.50188\n",
      "Epoch: 00 [12444/27767 ( 45%)], Train Loss: 0.50116\n",
      "Epoch: 00 [12484/27767 ( 45%)], Train Loss: 0.50040\n",
      "Epoch: 00 [12524/27767 ( 45%)], Train Loss: 0.49956\n",
      "Epoch: 00 [12564/27767 ( 45%)], Train Loss: 0.49934\n",
      "Epoch: 00 [12604/27767 ( 45%)], Train Loss: 0.49838\n",
      "Epoch: 00 [12644/27767 ( 46%)], Train Loss: 0.49812\n",
      "Epoch: 00 [12684/27767 ( 46%)], Train Loss: 0.49842\n",
      "Epoch: 00 [12724/27767 ( 46%)], Train Loss: 0.49838\n",
      "Epoch: 00 [12764/27767 ( 46%)], Train Loss: 0.49786\n",
      "Epoch: 00 [12804/27767 ( 46%)], Train Loss: 0.49739\n",
      "Epoch: 00 [12844/27767 ( 46%)], Train Loss: 0.49688\n",
      "Epoch: 00 [12884/27767 ( 46%)], Train Loss: 0.49659\n",
      "Epoch: 00 [12924/27767 ( 47%)], Train Loss: 0.49600\n",
      "Epoch: 00 [12964/27767 ( 47%)], Train Loss: 0.49544\n",
      "Epoch: 00 [13004/27767 ( 47%)], Train Loss: 0.49495\n",
      "Epoch: 00 [13044/27767 ( 47%)], Train Loss: 0.49423\n",
      "Epoch: 00 [13084/27767 ( 47%)], Train Loss: 0.49346\n",
      "Epoch: 00 [13124/27767 ( 47%)], Train Loss: 0.49314\n",
      "Epoch: 00 [13164/27767 ( 47%)], Train Loss: 0.49303\n",
      "Epoch: 00 [13204/27767 ( 48%)], Train Loss: 0.49250\n",
      "Epoch: 00 [13244/27767 ( 48%)], Train Loss: 0.49224\n",
      "Epoch: 00 [13284/27767 ( 48%)], Train Loss: 0.49154\n",
      "Epoch: 00 [13324/27767 ( 48%)], Train Loss: 0.49101\n",
      "Epoch: 00 [13364/27767 ( 48%)], Train Loss: 0.49121\n",
      "Epoch: 00 [13404/27767 ( 48%)], Train Loss: 0.49118\n",
      "Epoch: 00 [13444/27767 ( 48%)], Train Loss: 0.49085\n",
      "Epoch: 00 [13484/27767 ( 49%)], Train Loss: 0.49054\n",
      "Epoch: 00 [13524/27767 ( 49%)], Train Loss: 0.49010\n",
      "Epoch: 00 [13564/27767 ( 49%)], Train Loss: 0.48986\n",
      "Epoch: 00 [13604/27767 ( 49%)], Train Loss: 0.48947\n",
      "Epoch: 00 [13644/27767 ( 49%)], Train Loss: 0.48889\n",
      "Epoch: 00 [13684/27767 ( 49%)], Train Loss: 0.48863\n",
      "Epoch: 00 [13724/27767 ( 49%)], Train Loss: 0.48847\n",
      "Epoch: 00 [13764/27767 ( 50%)], Train Loss: 0.48808\n",
      "Epoch: 00 [13804/27767 ( 50%)], Train Loss: 0.48780\n",
      "Epoch: 00 [13844/27767 ( 50%)], Train Loss: 0.48705\n",
      "Epoch: 00 [13884/27767 ( 50%)], Train Loss: 0.48654\n",
      "Epoch: 00 [13924/27767 ( 50%)], Train Loss: 0.48630\n",
      "Epoch: 00 [13964/27767 ( 50%)], Train Loss: 0.48570\n",
      "Epoch: 00 [14004/27767 ( 50%)], Train Loss: 0.48514\n",
      "Epoch: 00 [14044/27767 ( 51%)], Train Loss: 0.48480\n",
      "Epoch: 00 [14084/27767 ( 51%)], Train Loss: 0.48432\n",
      "Epoch: 00 [14124/27767 ( 51%)], Train Loss: 0.48382\n",
      "Epoch: 00 [14164/27767 ( 51%)], Train Loss: 0.48322\n",
      "Epoch: 00 [14204/27767 ( 51%)], Train Loss: 0.48310\n",
      "Epoch: 00 [14244/27767 ( 51%)], Train Loss: 0.48264\n",
      "Epoch: 00 [14284/27767 ( 51%)], Train Loss: 0.48279\n",
      "Epoch: 00 [14324/27767 ( 52%)], Train Loss: 0.48196\n",
      "Epoch: 00 [14364/27767 ( 52%)], Train Loss: 0.48156\n",
      "Epoch: 00 [14404/27767 ( 52%)], Train Loss: 0.48137\n",
      "Epoch: 00 [14444/27767 ( 52%)], Train Loss: 0.48116\n",
      "Epoch: 00 [14484/27767 ( 52%)], Train Loss: 0.48072\n",
      "Epoch: 00 [14524/27767 ( 52%)], Train Loss: 0.48039\n",
      "Epoch: 00 [14564/27767 ( 52%)], Train Loss: 0.48040\n",
      "Epoch: 00 [14604/27767 ( 53%)], Train Loss: 0.48000\n",
      "Epoch: 00 [14644/27767 ( 53%)], Train Loss: 0.47946\n",
      "Epoch: 00 [14684/27767 ( 53%)], Train Loss: 0.47887\n",
      "Epoch: 00 [14724/27767 ( 53%)], Train Loss: 0.47873\n",
      "Epoch: 00 [14764/27767 ( 53%)], Train Loss: 0.47819\n",
      "Epoch: 00 [14804/27767 ( 53%)], Train Loss: 0.47798\n",
      "Epoch: 00 [14844/27767 ( 53%)], Train Loss: 0.47789\n",
      "Epoch: 00 [14884/27767 ( 54%)], Train Loss: 0.47765\n",
      "Epoch: 00 [14924/27767 ( 54%)], Train Loss: 0.47780\n",
      "Epoch: 00 [14964/27767 ( 54%)], Train Loss: 0.47728\n",
      "Epoch: 00 [15004/27767 ( 54%)], Train Loss: 0.47688\n",
      "Epoch: 00 [15044/27767 ( 54%)], Train Loss: 0.47667\n",
      "Epoch: 00 [15084/27767 ( 54%)], Train Loss: 0.47677\n",
      "Epoch: 00 [15124/27767 ( 54%)], Train Loss: 0.47633\n",
      "Epoch: 00 [15164/27767 ( 55%)], Train Loss: 0.47593\n",
      "Epoch: 00 [15204/27767 ( 55%)], Train Loss: 0.47548\n",
      "Epoch: 00 [15244/27767 ( 55%)], Train Loss: 0.47506\n",
      "Epoch: 00 [15284/27767 ( 55%)], Train Loss: 0.47487\n",
      "Epoch: 00 [15324/27767 ( 55%)], Train Loss: 0.47427\n",
      "Epoch: 00 [15364/27767 ( 55%)], Train Loss: 0.47396\n",
      "Epoch: 00 [15404/27767 ( 55%)], Train Loss: 0.47361\n",
      "Epoch: 00 [15444/27767 ( 56%)], Train Loss: 0.47321\n",
      "Epoch: 00 [15484/27767 ( 56%)], Train Loss: 0.47297\n",
      "Epoch: 00 [15524/27767 ( 56%)], Train Loss: 0.47238\n",
      "Epoch: 00 [15564/27767 ( 56%)], Train Loss: 0.47178\n",
      "Epoch: 00 [15604/27767 ( 56%)], Train Loss: 0.47186\n",
      "Epoch: 00 [15644/27767 ( 56%)], Train Loss: 0.47142\n",
      "Epoch: 00 [15684/27767 ( 56%)], Train Loss: 0.47121\n",
      "Epoch: 00 [15724/27767 ( 57%)], Train Loss: 0.47068\n",
      "Epoch: 00 [15764/27767 ( 57%)], Train Loss: 0.47021\n",
      "Epoch: 00 [15804/27767 ( 57%)], Train Loss: 0.47000\n",
      "Epoch: 00 [15844/27767 ( 57%)], Train Loss: 0.46971\n",
      "Epoch: 00 [15884/27767 ( 57%)], Train Loss: 0.46936\n",
      "Epoch: 00 [15924/27767 ( 57%)], Train Loss: 0.46892\n",
      "Epoch: 00 [15964/27767 ( 57%)], Train Loss: 0.46845\n",
      "Epoch: 00 [16004/27767 ( 58%)], Train Loss: 0.46801\n",
      "Epoch: 00 [16044/27767 ( 58%)], Train Loss: 0.46754\n",
      "Epoch: 00 [16084/27767 ( 58%)], Train Loss: 0.46716\n",
      "Epoch: 00 [16124/27767 ( 58%)], Train Loss: 0.46646\n",
      "Epoch: 00 [16164/27767 ( 58%)], Train Loss: 0.46618\n",
      "Epoch: 00 [16204/27767 ( 58%)], Train Loss: 0.46609\n",
      "Epoch: 00 [16244/27767 ( 59%)], Train Loss: 0.46552\n",
      "Epoch: 00 [16284/27767 ( 59%)], Train Loss: 0.46548\n",
      "Epoch: 00 [16324/27767 ( 59%)], Train Loss: 0.46514\n",
      "Epoch: 00 [16364/27767 ( 59%)], Train Loss: 0.46488\n",
      "Epoch: 00 [16404/27767 ( 59%)], Train Loss: 0.46436\n",
      "Epoch: 00 [16444/27767 ( 59%)], Train Loss: 0.46405\n",
      "Epoch: 00 [16484/27767 ( 59%)], Train Loss: 0.46377\n",
      "Epoch: 00 [16524/27767 ( 60%)], Train Loss: 0.46341\n",
      "Epoch: 00 [16564/27767 ( 60%)], Train Loss: 0.46348\n",
      "Epoch: 00 [16604/27767 ( 60%)], Train Loss: 0.46328\n",
      "Epoch: 00 [16644/27767 ( 60%)], Train Loss: 0.46304\n",
      "Epoch: 00 [16684/27767 ( 60%)], Train Loss: 0.46227\n",
      "Epoch: 00 [16724/27767 ( 60%)], Train Loss: 0.46262\n",
      "Epoch: 00 [16764/27767 ( 60%)], Train Loss: 0.46210\n",
      "Epoch: 00 [16804/27767 ( 61%)], Train Loss: 0.46206\n",
      "Epoch: 00 [16844/27767 ( 61%)], Train Loss: 0.46166\n",
      "Epoch: 00 [16884/27767 ( 61%)], Train Loss: 0.46098\n",
      "Epoch: 00 [16924/27767 ( 61%)], Train Loss: 0.46033\n",
      "Epoch: 00 [16964/27767 ( 61%)], Train Loss: 0.46013\n",
      "Epoch: 00 [17004/27767 ( 61%)], Train Loss: 0.45990\n",
      "Epoch: 00 [17044/27767 ( 61%)], Train Loss: 0.45990\n",
      "Epoch: 00 [17084/27767 ( 62%)], Train Loss: 0.45956\n",
      "Epoch: 00 [17124/27767 ( 62%)], Train Loss: 0.45928\n",
      "Epoch: 00 [17164/27767 ( 62%)], Train Loss: 0.45933\n",
      "Epoch: 00 [17204/27767 ( 62%)], Train Loss: 0.45920\n",
      "Epoch: 00 [17244/27767 ( 62%)], Train Loss: 0.45884\n",
      "Epoch: 00 [17284/27767 ( 62%)], Train Loss: 0.45824\n",
      "Epoch: 00 [17324/27767 ( 62%)], Train Loss: 0.45848\n",
      "Epoch: 00 [17364/27767 ( 63%)], Train Loss: 0.45840\n",
      "Epoch: 00 [17404/27767 ( 63%)], Train Loss: 0.45788\n",
      "Epoch: 00 [17444/27767 ( 63%)], Train Loss: 0.45736\n",
      "Epoch: 00 [17484/27767 ( 63%)], Train Loss: 0.45680\n",
      "Epoch: 00 [17524/27767 ( 63%)], Train Loss: 0.45658\n",
      "Epoch: 00 [17564/27767 ( 63%)], Train Loss: 0.45608\n",
      "Epoch: 00 [17604/27767 ( 63%)], Train Loss: 0.45607\n",
      "Epoch: 00 [17644/27767 ( 64%)], Train Loss: 0.45597\n",
      "Epoch: 00 [17684/27767 ( 64%)], Train Loss: 0.45565\n",
      "Epoch: 00 [17724/27767 ( 64%)], Train Loss: 0.45496\n",
      "Epoch: 00 [17764/27767 ( 64%)], Train Loss: 0.45498\n",
      "Epoch: 00 [17804/27767 ( 64%)], Train Loss: 0.45472\n",
      "Epoch: 00 [17844/27767 ( 64%)], Train Loss: 0.45447\n",
      "Epoch: 00 [17884/27767 ( 64%)], Train Loss: 0.45420\n",
      "Epoch: 00 [17924/27767 ( 65%)], Train Loss: 0.45412\n",
      "Epoch: 00 [17964/27767 ( 65%)], Train Loss: 0.45422\n",
      "Epoch: 00 [18004/27767 ( 65%)], Train Loss: 0.45407\n",
      "Epoch: 00 [18044/27767 ( 65%)], Train Loss: 0.45398\n",
      "Epoch: 00 [18084/27767 ( 65%)], Train Loss: 0.45354\n",
      "Epoch: 00 [18124/27767 ( 65%)], Train Loss: 0.45352\n",
      "Epoch: 00 [18164/27767 ( 65%)], Train Loss: 0.45306\n",
      "Epoch: 00 [18204/27767 ( 66%)], Train Loss: 0.45277\n",
      "Epoch: 00 [18244/27767 ( 66%)], Train Loss: 0.45247\n",
      "Epoch: 00 [18284/27767 ( 66%)], Train Loss: 0.45187\n",
      "Epoch: 00 [18324/27767 ( 66%)], Train Loss: 0.45142\n",
      "Epoch: 00 [18364/27767 ( 66%)], Train Loss: 0.45097\n",
      "Epoch: 00 [18404/27767 ( 66%)], Train Loss: 0.45080\n",
      "Epoch: 00 [18444/27767 ( 66%)], Train Loss: 0.45046\n",
      "Epoch: 00 [18484/27767 ( 67%)], Train Loss: 0.45047\n",
      "Epoch: 00 [18524/27767 ( 67%)], Train Loss: 0.45023\n",
      "Epoch: 00 [18564/27767 ( 67%)], Train Loss: 0.44978\n",
      "Epoch: 00 [18604/27767 ( 67%)], Train Loss: 0.44934\n",
      "Epoch: 00 [18644/27767 ( 67%)], Train Loss: 0.44877\n",
      "Epoch: 00 [18684/27767 ( 67%)], Train Loss: 0.44834\n",
      "Epoch: 00 [18724/27767 ( 67%)], Train Loss: 0.44811\n",
      "Epoch: 00 [18764/27767 ( 68%)], Train Loss: 0.44771\n",
      "Epoch: 00 [18804/27767 ( 68%)], Train Loss: 0.44733\n",
      "Epoch: 00 [18844/27767 ( 68%)], Train Loss: 0.44708\n",
      "Epoch: 00 [18884/27767 ( 68%)], Train Loss: 0.44673\n",
      "Epoch: 00 [18924/27767 ( 68%)], Train Loss: 0.44659\n",
      "Epoch: 00 [18964/27767 ( 68%)], Train Loss: 0.44621\n",
      "Epoch: 00 [19004/27767 ( 68%)], Train Loss: 0.44597\n",
      "Epoch: 00 [19044/27767 ( 69%)], Train Loss: 0.44570\n",
      "Epoch: 00 [19084/27767 ( 69%)], Train Loss: 0.44551\n",
      "Epoch: 00 [19124/27767 ( 69%)], Train Loss: 0.44519\n",
      "Epoch: 00 [19164/27767 ( 69%)], Train Loss: 0.44468\n",
      "Epoch: 00 [19204/27767 ( 69%)], Train Loss: 0.44433\n",
      "Epoch: 00 [19244/27767 ( 69%)], Train Loss: 0.44395\n",
      "Epoch: 00 [19284/27767 ( 69%)], Train Loss: 0.44382\n",
      "Epoch: 00 [19324/27767 ( 70%)], Train Loss: 0.44372\n",
      "Epoch: 00 [19364/27767 ( 70%)], Train Loss: 0.44341\n",
      "Epoch: 00 [19404/27767 ( 70%)], Train Loss: 0.44308\n",
      "Epoch: 00 [19444/27767 ( 70%)], Train Loss: 0.44276\n",
      "Epoch: 00 [19484/27767 ( 70%)], Train Loss: 0.44268\n",
      "Epoch: 00 [19524/27767 ( 70%)], Train Loss: 0.44219\n",
      "Epoch: 00 [19564/27767 ( 70%)], Train Loss: 0.44228\n",
      "Epoch: 00 [19604/27767 ( 71%)], Train Loss: 0.44192\n",
      "Epoch: 00 [19644/27767 ( 71%)], Train Loss: 0.44166\n",
      "Epoch: 00 [19684/27767 ( 71%)], Train Loss: 0.44149\n",
      "Epoch: 00 [19724/27767 ( 71%)], Train Loss: 0.44122\n",
      "Epoch: 00 [19764/27767 ( 71%)], Train Loss: 0.44112\n",
      "Epoch: 00 [19804/27767 ( 71%)], Train Loss: 0.44084\n",
      "Epoch: 00 [19844/27767 ( 71%)], Train Loss: 0.44053\n",
      "Epoch: 00 [19884/27767 ( 72%)], Train Loss: 0.44065\n",
      "Epoch: 00 [19924/27767 ( 72%)], Train Loss: 0.44019\n",
      "Epoch: 00 [19964/27767 ( 72%)], Train Loss: 0.44000\n",
      "Epoch: 00 [20004/27767 ( 72%)], Train Loss: 0.43964\n",
      "Epoch: 00 [20044/27767 ( 72%)], Train Loss: 0.43926\n",
      "Epoch: 00 [20084/27767 ( 72%)], Train Loss: 0.43916\n",
      "Epoch: 00 [20124/27767 ( 72%)], Train Loss: 0.43928\n",
      "Epoch: 00 [20164/27767 ( 73%)], Train Loss: 0.43900\n",
      "Epoch: 00 [20204/27767 ( 73%)], Train Loss: 0.43871\n",
      "Epoch: 00 [20244/27767 ( 73%)], Train Loss: 0.43825\n",
      "Epoch: 00 [20284/27767 ( 73%)], Train Loss: 0.43787\n",
      "Epoch: 00 [20324/27767 ( 73%)], Train Loss: 0.43779\n",
      "Epoch: 00 [20364/27767 ( 73%)], Train Loss: 0.43775\n",
      "Epoch: 00 [20404/27767 ( 73%)], Train Loss: 0.43752\n",
      "Epoch: 00 [20444/27767 ( 74%)], Train Loss: 0.43749\n",
      "Epoch: 00 [20484/27767 ( 74%)], Train Loss: 0.43732\n",
      "Epoch: 00 [20524/27767 ( 74%)], Train Loss: 0.43713\n",
      "Epoch: 00 [20564/27767 ( 74%)], Train Loss: 0.43685\n",
      "Epoch: 00 [20604/27767 ( 74%)], Train Loss: 0.43650\n",
      "Epoch: 00 [20644/27767 ( 74%)], Train Loss: 0.43623\n",
      "Epoch: 00 [20684/27767 ( 74%)], Train Loss: 0.43588\n",
      "Epoch: 00 [20724/27767 ( 75%)], Train Loss: 0.43582\n",
      "Epoch: 00 [20764/27767 ( 75%)], Train Loss: 0.43562\n",
      "Epoch: 00 [20804/27767 ( 75%)], Train Loss: 0.43543\n",
      "Epoch: 00 [20844/27767 ( 75%)], Train Loss: 0.43508\n",
      "Epoch: 00 [20884/27767 ( 75%)], Train Loss: 0.43477\n",
      "Epoch: 00 [20924/27767 ( 75%)], Train Loss: 0.43473\n",
      "Epoch: 00 [20964/27767 ( 75%)], Train Loss: 0.43448\n",
      "Epoch: 00 [21004/27767 ( 76%)], Train Loss: 0.43424\n",
      "Epoch: 00 [21044/27767 ( 76%)], Train Loss: 0.43415\n",
      "Epoch: 00 [21084/27767 ( 76%)], Train Loss: 0.43379\n",
      "Epoch: 00 [21124/27767 ( 76%)], Train Loss: 0.43374\n",
      "Epoch: 00 [21164/27767 ( 76%)], Train Loss: 0.43345\n",
      "Epoch: 00 [21204/27767 ( 76%)], Train Loss: 0.43329\n",
      "Epoch: 00 [21244/27767 ( 77%)], Train Loss: 0.43295\n",
      "Epoch: 00 [21284/27767 ( 77%)], Train Loss: 0.43266\n",
      "Epoch: 00 [21324/27767 ( 77%)], Train Loss: 0.43235\n",
      "Epoch: 00 [21364/27767 ( 77%)], Train Loss: 0.43234\n",
      "Epoch: 00 [21404/27767 ( 77%)], Train Loss: 0.43191\n",
      "Epoch: 00 [21444/27767 ( 77%)], Train Loss: 0.43161\n",
      "Epoch: 00 [21484/27767 ( 77%)], Train Loss: 0.43130\n",
      "Epoch: 00 [21524/27767 ( 78%)], Train Loss: 0.43125\n",
      "Epoch: 00 [21564/27767 ( 78%)], Train Loss: 0.43118\n",
      "Epoch: 00 [21604/27767 ( 78%)], Train Loss: 0.43106\n",
      "Epoch: 00 [21644/27767 ( 78%)], Train Loss: 0.43090\n",
      "Epoch: 00 [21684/27767 ( 78%)], Train Loss: 0.43088\n",
      "Epoch: 00 [21724/27767 ( 78%)], Train Loss: 0.43075\n",
      "Epoch: 00 [21764/27767 ( 78%)], Train Loss: 0.43050\n",
      "Epoch: 00 [21804/27767 ( 79%)], Train Loss: 0.43017\n",
      "Epoch: 00 [21844/27767 ( 79%)], Train Loss: 0.43005\n",
      "Epoch: 00 [21884/27767 ( 79%)], Train Loss: 0.42976\n",
      "Epoch: 00 [21924/27767 ( 79%)], Train Loss: 0.42961\n",
      "Epoch: 00 [21964/27767 ( 79%)], Train Loss: 0.42920\n",
      "Epoch: 00 [22004/27767 ( 79%)], Train Loss: 0.42914\n",
      "Epoch: 00 [22044/27767 ( 79%)], Train Loss: 0.42884\n",
      "Epoch: 00 [22084/27767 ( 80%)], Train Loss: 0.42867\n",
      "Epoch: 00 [22124/27767 ( 80%)], Train Loss: 0.42828\n",
      "Epoch: 00 [22164/27767 ( 80%)], Train Loss: 0.42832\n",
      "Epoch: 00 [22204/27767 ( 80%)], Train Loss: 0.42781\n",
      "Epoch: 00 [22244/27767 ( 80%)], Train Loss: 0.42756\n",
      "Epoch: 00 [22284/27767 ( 80%)], Train Loss: 0.42712\n",
      "Epoch: 00 [22324/27767 ( 80%)], Train Loss: 0.42690\n",
      "Epoch: 00 [22364/27767 ( 81%)], Train Loss: 0.42657\n",
      "Epoch: 00 [22404/27767 ( 81%)], Train Loss: 0.42628\n",
      "Epoch: 00 [22444/27767 ( 81%)], Train Loss: 0.42594\n",
      "Epoch: 00 [22484/27767 ( 81%)], Train Loss: 0.42539\n",
      "Epoch: 00 [22524/27767 ( 81%)], Train Loss: 0.42512\n",
      "Epoch: 00 [22564/27767 ( 81%)], Train Loss: 0.42464\n",
      "Epoch: 00 [22604/27767 ( 81%)], Train Loss: 0.42466\n",
      "Epoch: 00 [22644/27767 ( 82%)], Train Loss: 0.42443\n",
      "Epoch: 00 [22684/27767 ( 82%)], Train Loss: 0.42411\n",
      "Epoch: 00 [22724/27767 ( 82%)], Train Loss: 0.42394\n",
      "Epoch: 00 [22764/27767 ( 82%)], Train Loss: 0.42343\n",
      "Epoch: 00 [22804/27767 ( 82%)], Train Loss: 0.42313\n",
      "Epoch: 00 [22844/27767 ( 82%)], Train Loss: 0.42291\n",
      "Epoch: 00 [22884/27767 ( 82%)], Train Loss: 0.42263\n",
      "Epoch: 00 [22924/27767 ( 83%)], Train Loss: 0.42226\n",
      "Epoch: 00 [22964/27767 ( 83%)], Train Loss: 0.42208\n",
      "Epoch: 00 [23004/27767 ( 83%)], Train Loss: 0.42203\n",
      "Epoch: 00 [23044/27767 ( 83%)], Train Loss: 0.42202\n",
      "Epoch: 00 [23084/27767 ( 83%)], Train Loss: 0.42162\n",
      "Epoch: 00 [23124/27767 ( 83%)], Train Loss: 0.42128\n",
      "Epoch: 00 [23164/27767 ( 83%)], Train Loss: 0.42104\n",
      "Epoch: 00 [23204/27767 ( 84%)], Train Loss: 0.42081\n",
      "Epoch: 00 [23244/27767 ( 84%)], Train Loss: 0.42075\n",
      "Epoch: 00 [23284/27767 ( 84%)], Train Loss: 0.42049\n",
      "Epoch: 00 [23324/27767 ( 84%)], Train Loss: 0.42017\n",
      "Epoch: 00 [23364/27767 ( 84%)], Train Loss: 0.42007\n",
      "Epoch: 00 [23404/27767 ( 84%)], Train Loss: 0.42000\n",
      "Epoch: 00 [23444/27767 ( 84%)], Train Loss: 0.41986\n",
      "Epoch: 00 [23484/27767 ( 85%)], Train Loss: 0.41972\n",
      "Epoch: 00 [23524/27767 ( 85%)], Train Loss: 0.41943\n",
      "Epoch: 00 [23564/27767 ( 85%)], Train Loss: 0.41926\n",
      "Epoch: 00 [23604/27767 ( 85%)], Train Loss: 0.41935\n",
      "Epoch: 00 [23644/27767 ( 85%)], Train Loss: 0.41939\n",
      "Epoch: 00 [23684/27767 ( 85%)], Train Loss: 0.41926\n",
      "Epoch: 00 [23724/27767 ( 85%)], Train Loss: 0.41910\n",
      "Epoch: 00 [23764/27767 ( 86%)], Train Loss: 0.41912\n",
      "Epoch: 00 [23804/27767 ( 86%)], Train Loss: 0.41884\n",
      "Epoch: 00 [23844/27767 ( 86%)], Train Loss: 0.41875\n",
      "Epoch: 00 [23884/27767 ( 86%)], Train Loss: 0.41866\n",
      "Epoch: 00 [23924/27767 ( 86%)], Train Loss: 0.41872\n",
      "Epoch: 00 [23964/27767 ( 86%)], Train Loss: 0.41842\n",
      "Epoch: 00 [24004/27767 ( 86%)], Train Loss: 0.41811\n",
      "Epoch: 00 [24044/27767 ( 87%)], Train Loss: 0.41793\n",
      "Epoch: 00 [24084/27767 ( 87%)], Train Loss: 0.41763\n",
      "Epoch: 00 [24124/27767 ( 87%)], Train Loss: 0.41741\n",
      "Epoch: 00 [24164/27767 ( 87%)], Train Loss: 0.41711\n",
      "Epoch: 00 [24204/27767 ( 87%)], Train Loss: 0.41671\n",
      "Epoch: 00 [24244/27767 ( 87%)], Train Loss: 0.41658\n",
      "Epoch: 00 [24284/27767 ( 87%)], Train Loss: 0.41641\n",
      "Epoch: 00 [24324/27767 ( 88%)], Train Loss: 0.41650\n",
      "Epoch: 00 [24364/27767 ( 88%)], Train Loss: 0.41621\n",
      "Epoch: 00 [24404/27767 ( 88%)], Train Loss: 0.41594\n",
      "Epoch: 00 [24444/27767 ( 88%)], Train Loss: 0.41557\n",
      "Epoch: 00 [24484/27767 ( 88%)], Train Loss: 0.41538\n",
      "Epoch: 00 [24524/27767 ( 88%)], Train Loss: 0.41512\n",
      "Epoch: 00 [24564/27767 ( 88%)], Train Loss: 0.41515\n",
      "Epoch: 00 [24604/27767 ( 89%)], Train Loss: 0.41521\n",
      "Epoch: 00 [24644/27767 ( 89%)], Train Loss: 0.41487\n",
      "Epoch: 00 [24684/27767 ( 89%)], Train Loss: 0.41454\n",
      "Epoch: 00 [24724/27767 ( 89%)], Train Loss: 0.41433\n",
      "Epoch: 00 [24764/27767 ( 89%)], Train Loss: 0.41418\n",
      "Epoch: 00 [24804/27767 ( 89%)], Train Loss: 0.41402\n",
      "Epoch: 00 [24844/27767 ( 89%)], Train Loss: 0.41372\n",
      "Epoch: 00 [24884/27767 ( 90%)], Train Loss: 0.41374\n",
      "Epoch: 00 [24924/27767 ( 90%)], Train Loss: 0.41363\n",
      "Epoch: 00 [24964/27767 ( 90%)], Train Loss: 0.41353\n",
      "Epoch: 00 [25004/27767 ( 90%)], Train Loss: 0.41382\n",
      "Epoch: 00 [25044/27767 ( 90%)], Train Loss: 0.41343\n",
      "Epoch: 00 [25084/27767 ( 90%)], Train Loss: 0.41324\n",
      "Epoch: 00 [25124/27767 ( 90%)], Train Loss: 0.41296\n",
      "Epoch: 00 [25164/27767 ( 91%)], Train Loss: 0.41276\n",
      "Epoch: 00 [25204/27767 ( 91%)], Train Loss: 0.41240\n",
      "Epoch: 00 [25244/27767 ( 91%)], Train Loss: 0.41213\n",
      "Epoch: 00 [25284/27767 ( 91%)], Train Loss: 0.41192\n",
      "Epoch: 00 [25324/27767 ( 91%)], Train Loss: 0.41163\n",
      "Epoch: 00 [25364/27767 ( 91%)], Train Loss: 0.41147\n",
      "Epoch: 00 [25404/27767 ( 91%)], Train Loss: 0.41127\n",
      "Epoch: 00 [25444/27767 ( 92%)], Train Loss: 0.41148\n",
      "Epoch: 00 [25484/27767 ( 92%)], Train Loss: 0.41128\n",
      "Epoch: 00 [25524/27767 ( 92%)], Train Loss: 0.41117\n",
      "Epoch: 00 [25564/27767 ( 92%)], Train Loss: 0.41105\n",
      "Epoch: 00 [25604/27767 ( 92%)], Train Loss: 0.41074\n",
      "Epoch: 00 [25644/27767 ( 92%)], Train Loss: 0.41048\n",
      "Epoch: 00 [25684/27767 ( 92%)], Train Loss: 0.41015\n",
      "Epoch: 00 [25724/27767 ( 93%)], Train Loss: 0.41022\n",
      "Epoch: 00 [25764/27767 ( 93%)], Train Loss: 0.41022\n",
      "Epoch: 00 [25804/27767 ( 93%)], Train Loss: 0.40995\n",
      "Epoch: 00 [25844/27767 ( 93%)], Train Loss: 0.40969\n",
      "Epoch: 00 [25884/27767 ( 93%)], Train Loss: 0.40951\n",
      "Epoch: 00 [25924/27767 ( 93%)], Train Loss: 0.40925\n",
      "Epoch: 00 [25964/27767 ( 94%)], Train Loss: 0.40916\n",
      "Epoch: 00 [26004/27767 ( 94%)], Train Loss: 0.40885\n",
      "Epoch: 00 [26044/27767 ( 94%)], Train Loss: 0.40864\n",
      "Epoch: 00 [26084/27767 ( 94%)], Train Loss: 0.40842\n",
      "Epoch: 00 [26124/27767 ( 94%)], Train Loss: 0.40804\n",
      "Epoch: 00 [26164/27767 ( 94%)], Train Loss: 0.40776\n",
      "Epoch: 00 [26204/27767 ( 94%)], Train Loss: 0.40752\n",
      "Epoch: 00 [26244/27767 ( 95%)], Train Loss: 0.40734\n",
      "Epoch: 00 [26284/27767 ( 95%)], Train Loss: 0.40700\n",
      "Epoch: 00 [26324/27767 ( 95%)], Train Loss: 0.40671\n",
      "Epoch: 00 [26364/27767 ( 95%)], Train Loss: 0.40651\n",
      "Epoch: 00 [26404/27767 ( 95%)], Train Loss: 0.40611\n",
      "Epoch: 00 [26444/27767 ( 95%)], Train Loss: 0.40597\n",
      "Epoch: 00 [26484/27767 ( 95%)], Train Loss: 0.40591\n",
      "Epoch: 00 [26524/27767 ( 96%)], Train Loss: 0.40581\n",
      "Epoch: 00 [26564/27767 ( 96%)], Train Loss: 0.40563\n",
      "Epoch: 00 [26604/27767 ( 96%)], Train Loss: 0.40531\n",
      "Epoch: 00 [26644/27767 ( 96%)], Train Loss: 0.40517\n",
      "Epoch: 00 [26684/27767 ( 96%)], Train Loss: 0.40514\n",
      "Epoch: 00 [26724/27767 ( 96%)], Train Loss: 0.40485\n",
      "Epoch: 00 [26764/27767 ( 96%)], Train Loss: 0.40470\n",
      "Epoch: 00 [26804/27767 ( 97%)], Train Loss: 0.40485\n",
      "Epoch: 00 [26844/27767 ( 97%)], Train Loss: 0.40471\n",
      "Epoch: 00 [26884/27767 ( 97%)], Train Loss: 0.40452\n",
      "Epoch: 00 [26924/27767 ( 97%)], Train Loss: 0.40426\n",
      "Epoch: 00 [26964/27767 ( 97%)], Train Loss: 0.40436\n",
      "Epoch: 00 [27004/27767 ( 97%)], Train Loss: 0.40424\n",
      "Epoch: 00 [27044/27767 ( 97%)], Train Loss: 0.40419\n",
      "Epoch: 00 [27084/27767 ( 98%)], Train Loss: 0.40412\n",
      "Epoch: 00 [27124/27767 ( 98%)], Train Loss: 0.40401\n",
      "Epoch: 00 [27164/27767 ( 98%)], Train Loss: 0.40379\n",
      "Epoch: 00 [27204/27767 ( 98%)], Train Loss: 0.40379\n",
      "Epoch: 00 [27244/27767 ( 98%)], Train Loss: 0.40367\n",
      "Epoch: 00 [27284/27767 ( 98%)], Train Loss: 0.40335\n",
      "Epoch: 00 [27324/27767 ( 98%)], Train Loss: 0.40315\n",
      "Epoch: 00 [27364/27767 ( 99%)], Train Loss: 0.40316\n",
      "Epoch: 00 [27404/27767 ( 99%)], Train Loss: 0.40298\n",
      "Epoch: 00 [27444/27767 ( 99%)], Train Loss: 0.40288\n",
      "Epoch: 00 [27484/27767 ( 99%)], Train Loss: 0.40254\n",
      "Epoch: 00 [27524/27767 ( 99%)], Train Loss: 0.40223\n",
      "Epoch: 00 [27564/27767 ( 99%)], Train Loss: 0.40196\n",
      "Epoch: 00 [27604/27767 ( 99%)], Train Loss: 0.40172\n",
      "Epoch: 00 [27644/27767 (100%)], Train Loss: 0.40155\n",
      "Epoch: 00 [27684/27767 (100%)], Train Loss: 0.40131\n",
      "Epoch: 00 [27724/27767 (100%)], Train Loss: 0.40111\n",
      "Epoch: 00 [27764/27767 (100%)], Train Loss: 0.40096\n",
      "Epoch: 00 [27767/27767 (100%)], Train Loss: 0.40092\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.60167\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.60167\n",
      "Saving model checkpoint to output/checkpoint-fold-7.\n",
      "\n",
      "Total Training Time: 3230.972322702408secs, Average Training Time per Epoch: 3230.972322702408secs.\n",
      "Total Validation Time: 141.41412210464478secs, Average Validation Time per Epoch: 141.41412210464478secs.\n"
     ]
    }
   ],
   "source": [
    "for fold in range(8):\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    torch.cuda.empty_cache()\n",
    "    run(train, fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27556.495593,
   "end_time": "2022-06-29T04:30:39.400424",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-28T20:51:22.904831",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0553a2dc525d41b7927eef8d866657d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b03911748a7047a0a1797b9565dd217b",
       "placeholder": "​",
       "style": "IPY_MODEL_dbc954e3cb9744ea9391b6b22c3f90e8",
       "value": " 4.83M/4.83M [00:00&lt;00:00, 8.26MB/s]"
      }
     },
     "08032f54b99846c4b35aa6ea53b0c1e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c2dab43e49694bdea59d76d663d1d802",
       "max": 179,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e204993433934ebca4e48cb6850a5de7",
       "value": 179
      }
     },
     "0a349412b305484db1a996b2c4c6d12e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a9b93ba73f4b4a31b33c3f11e6b21d3b",
        "IPY_MODEL_08032f54b99846c4b35aa6ea53b0c1e5",
        "IPY_MODEL_efbd60d0f337499daa8c779f38aa1cde"
       ],
       "layout": "IPY_MODEL_c3434e99208944d6a36dcdac6ab55631"
      }
     },
     "116206cec1214cbfa3f337786ee953e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "11a5e0e0c6fb4c6b99a69b2ec56b2542": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13e5361089d14dda8b43795e6801720f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13e8baa72ef046e6b3a277a189c2dffe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5969be81422c440b98dd50e2f5604d88",
       "placeholder": "​",
       "style": "IPY_MODEL_116206cec1214cbfa3f337786ee953e2",
       "value": "Downloading: 100%"
      }
     },
     "1418ceead93c4d1790cf2fedd581a800": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2a46c11c01bf4ed4a6f601b070e6eff3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2bb944e5ef6b4be5a18118e08fcbd6ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "31d63f2ca3c4485fa31379a952abd61b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "38a7d761a5184fd0bec5b53948042e2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43ff4e512659456992123351b802030e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ac16667492e4c548b794ae7b94c58db",
        "IPY_MODEL_af2eeef8b61e4033815a71f9e9346367",
        "IPY_MODEL_0553a2dc525d41b7927eef8d866657d0"
       ],
       "layout": "IPY_MODEL_d548598c421e47d58acaaac358277e0c"
      }
     },
     "54ea6c4061c942ef953d7b4fd2f8096b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c3f3bbbcd65b419f9bf1bf174615368d",
       "max": 606,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_56795101db22437890e7752bc567031f",
       "value": 606
      }
     },
     "56795101db22437890e7752bc567031f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "583fbb0b40b54efca636fc41166214e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_11a5e0e0c6fb4c6b99a69b2ec56b2542",
       "placeholder": "​",
       "style": "IPY_MODEL_f6f6f844f64f4d1b9221a5d018a85332",
       "value": " 2.09G/2.09G [00:50&lt;00:00, 49.6MB/s]"
      }
     },
     "5969be81422c440b98dd50e2f5604d88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b8ba1f2f5544b1ca3d55016ed03f37e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_31d63f2ca3c4485fa31379a952abd61b",
       "max": 2239666418,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cf6ceff3c7c64841802fa0f0b5da478d",
       "value": 2239666418
      }
     },
     "5ef95d52ab7e4492a1d1601efd8753ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "622e9a1b10a94ba1967c19d9a34f9c19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65402aa43b8e443f9d4c6a36ac7ef4ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "67225f6f1fe7469c9e9a40954288b4ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7897920a69fd4fe6bbd6097ac4dd93cf",
        "IPY_MODEL_9e921869db9d45e58b9f69e8b5b42968",
        "IPY_MODEL_bb570bda56c54eb5be34273bb19bf8a6"
       ],
       "layout": "IPY_MODEL_5ef95d52ab7e4492a1d1601efd8753ac"
      }
     },
     "7897920a69fd4fe6bbd6097ac4dd93cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_caa09cc4317f42edbcc356103ac53ecf",
       "placeholder": "​",
       "style": "IPY_MODEL_1418ceead93c4d1790cf2fedd581a800",
       "value": "Downloading: 100%"
      }
     },
     "78ba15ea9fd14ce781972dbad0a4e746": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ac16667492e4c548b794ae7b94c58db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_13e5361089d14dda8b43795e6801720f",
       "placeholder": "​",
       "style": "IPY_MODEL_65402aa43b8e443f9d4c6a36ac7ef4ee",
       "value": "Downloading: 100%"
      }
     },
     "8347f4834cd34601a1afefd1cfdb723c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "90b1c299acaa4f70a03aaa6fb09d74df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bba43974142c47d2a553435e9122b8ee",
       "placeholder": "​",
       "style": "IPY_MODEL_c666ef5fed144630abf98fac9e9dbd0b",
       "value": " 606/606 [00:00&lt;00:00, 24.4kB/s]"
      }
     },
     "9e921869db9d45e58b9f69e8b5b42968": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa0cbb494a574c76b6650f49d1502405",
       "max": 150,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2bb944e5ef6b4be5a18118e08fcbd6ba",
       "value": 150
      }
     },
     "a57125623e254b5b8b32b5bd20e2e3d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a9b93ba73f4b4a31b33c3f11e6b21d3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_78ba15ea9fd14ce781972dbad0a4e746",
       "placeholder": "​",
       "style": "IPY_MODEL_d9a775a16c894e24987681e4db1c686a",
       "value": "Downloading: 100%"
      }
     },
     "af2eeef8b61e4033815a71f9e9346367": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_622e9a1b10a94ba1967c19d9a34f9c19",
       "max": 5069051,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b1210dd9afc840af81173c971b441c98",
       "value": 5069051
      }
     },
     "b03911748a7047a0a1797b9565dd217b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1210dd9afc840af81173c971b441c98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bb570bda56c54eb5be34273bb19bf8a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cf25ebc709ba4c18bcfd9cf225cd926a",
       "placeholder": "​",
       "style": "IPY_MODEL_a57125623e254b5b8b32b5bd20e2e3d2",
       "value": " 150/150 [00:00&lt;00:00, 6.17kB/s]"
      }
     },
     "bba43974142c47d2a553435e9122b8ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2dab43e49694bdea59d76d663d1d802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3434e99208944d6a36dcdac6ab55631": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3f3bbbcd65b419f9bf1bf174615368d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c60b6d8fd8d844ea8f0df6746ef9d0dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c666ef5fed144630abf98fac9e9dbd0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c7f67cd67ac8442c8531754b48137df1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "caa09cc4317f42edbcc356103ac53ecf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf25ebc709ba4c18bcfd9cf225cd926a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf6ceff3c7c64841802fa0f0b5da478d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d276a19012a74f7baa9d274f4fae03ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d78dbc6ac8d4401d9871c3e8a795a72c",
        "IPY_MODEL_5b8ba1f2f5544b1ca3d55016ed03f37e",
        "IPY_MODEL_583fbb0b40b54efca636fc41166214e8"
       ],
       "layout": "IPY_MODEL_f6b44198a7ef4d1a9f3ae9d1e322897a"
      }
     },
     "d548598c421e47d58acaaac358277e0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d78dbc6ac8d4401d9871c3e8a795a72c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_38a7d761a5184fd0bec5b53948042e2e",
       "placeholder": "​",
       "style": "IPY_MODEL_8347f4834cd34601a1afefd1cfdb723c",
       "value": "Downloading: 100%"
      }
     },
     "d9a775a16c894e24987681e4db1c686a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dbc954e3cb9744ea9391b6b22c3f90e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e204993433934ebca4e48cb6850a5de7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e91323decdd44207aeccadc9811ad863": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_13e8baa72ef046e6b3a277a189c2dffe",
        "IPY_MODEL_54ea6c4061c942ef953d7b4fd2f8096b",
        "IPY_MODEL_90b1c299acaa4f70a03aaa6fb09d74df"
       ],
       "layout": "IPY_MODEL_2a46c11c01bf4ed4a6f601b070e6eff3"
      }
     },
     "efbd60d0f337499daa8c779f38aa1cde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c60b6d8fd8d844ea8f0df6746ef9d0dc",
       "placeholder": "​",
       "style": "IPY_MODEL_c7f67cd67ac8442c8531754b48137df1",
       "value": " 179/179 [00:00&lt;00:00, 6.68kB/s]"
      }
     },
     "f6b44198a7ef4d1a9f3ae9d1e322897a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6f6f844f64f4d1b9221a5d018a85332": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fa0cbb494a574c76b6650f49d1502405": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
